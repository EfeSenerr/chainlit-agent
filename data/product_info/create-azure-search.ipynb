{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Format Document Search Index Pipeline\n",
    "\n",
    "This notebook creates a comprehensive document search index from multiple file types, **always starting fresh** for testing and development. Every run will delete and recreate the index to ensure the schema is always up to date.\n",
    "\n",
    "## Supported File Types\n",
    "\n",
    "- **📝 Markdown files (.md)** - With smart title extraction from headings\n",
    "- **📄 Text files (.txt)** - Plain text processing  \n",
    "- **📊 CSV files (.csv)** - Intelligent table parsing with special handling for statistics files\n",
    "- **📑 PDF files (.pdf)** - Text extraction from all pages\n",
    "- **📈 Excel files (.xlsx/.xls)** - Multi-sheet processing with column structure\n",
    "- **⚙️ JSON files (.json)** - Structured data formatting\n",
    "\n",
    "## Pipeline Flow\n",
    "\n",
    "1. **🗑️ Delete existing index** - Ensures clean state\n",
    "2. **🏗️ Create fresh index** - With latest schema including metadata fields\n",
    "3. **📁 Process documents** - Smart content extraction and chunking\n",
    "4. **📤 Upload to index** - Batch processing with error handling\n",
    "5. **📊 Show statistics** - Summary of indexed content\n",
    "\n",
    "## Requirements\n",
    "\n",
    "You'll need credentials for the following Azure services:\n",
    "\n",
    "- **Azure Search Service** - For the vector search index\n",
    "- **Azure OpenAI Service** - For text embeddings\n",
    "\n",
    "Find the names and keys in the Azure Portal and add them to a `.env` file in the root of this repository. The `.env` file is not checked in to source control. You can use the [`.env.sample`](../../.env.sample) file as a template.\n",
    "\n",
    "## Fresh Index Benefits\n",
    "\n",
    "This \"always fresh\" approach is perfect for:\n",
    "- **Testing new features** - No schema conflicts\n",
    "- **Iterative development** - Clean state every run  \n",
    "- **Content updates** - All changes are captured\n",
    "- **Schema evolution** - New fields work immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SemanticSearch,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters\n",
    ")\n",
    "from typing import List, Dict\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import mimetypes\n",
    "\n",
    "# Additional imports for document processing\n",
    "try:\n",
    "    import PyPDF2\n",
    "    PDF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PDF_AVAILABLE = False\n",
    "    print(\"Warning: PyPDF2 not available. Install with: pip install PyPDF2\")\n",
    "\n",
    "try:\n",
    "    import openpyxl\n",
    "    EXCEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    EXCEL_AVAILABLE = False\n",
    "    print(\"Warning: openpyxl not available. Install with: pip install openpyxl\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index(search_index_client: SearchIndexClient, search_index: str):\n",
    "    print(f\"deleting index {search_index}\")\n",
    "    search_index_client.delete_index(search_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document processing functions for different file types with proper encoding handling\n",
    "\n",
    "def detect_encoding(file_path: Path) -> str:\n",
    "    \"\"\"Detect the encoding of a text file, with preference for UTF-8 and German-specific encodings\"\"\"\n",
    "    try:\n",
    "        # Try common encodings for German text\n",
    "        encodings_to_try = ['utf-8', 'utf-8-sig', 'iso-8859-1', 'cp1252', 'latin1']\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    # Read a sample to test encoding\n",
    "                    sample = f.read(1024)\n",
    "                    # Check if it contains common German characters properly\n",
    "                    if any(char in sample for char in ['ä', 'ö', 'ü', 'ß', 'Ä', 'Ö', 'Ü']):\n",
    "                        return encoding\n",
    "                    elif encoding == 'utf-8' and not any(char in sample for char in ['Ã']):\n",
    "                        # UTF-8 without encoding artifacts\n",
    "                        return encoding\n",
    "            except (UnicodeDecodeError, UnicodeError):\n",
    "                continue\n",
    "        \n",
    "        # Default to utf-8 if nothing works\n",
    "        return 'utf-8'\n",
    "    except Exception:\n",
    "        return 'utf-8'\n",
    "\n",
    "def clean_german_text(text: str) -> str:\n",
    "    \"\"\"Clean up common German encoding issues\"\"\"\n",
    "    # Fix common UTF-8 encoding artifacts for German umlauts\n",
    "    replacements = {\n",
    "        'Ã¤': 'ä',\n",
    "        'Ã¶': 'ö', \n",
    "        'Ã¼': 'ü',\n",
    "        'Ã': 'ß',\n",
    "        'Ã„': 'Ä',\n",
    "        'Ã–': 'Ö',\n",
    "        'Ãœ': 'Ü',\n",
    "        'â€œ': '\"',\n",
    "        'â€': '\"',\n",
    "        'â€™': \"'\",\n",
    "        'â€\"': '–',\n",
    "        'â€\"': '—'\n",
    "    }\n",
    "    \n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_csv_file(file_path: Path) -> str:\n",
    "    \"\"\"Process CSV files with intelligent content extraction and proper encoding, with special handling for German statistical data\"\"\"\n",
    "    try:\n",
    "        # Detect encoding for better German text handling\n",
    "        encoding = detect_encoding(file_path)\n",
    "        \n",
    "        # Special handling for German BKA statistics files\n",
    "        if \"statistic\" in file_path.name.lower() or \"bka\" in file_path.name.lower():\n",
    "            return process_german_statistics_csv_integrated(file_path, encoding)\n",
    "        \n",
    "        # Try to read with pandas using detected encoding\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        \n",
    "        # Check if this looks like a statistics table (like your example)\n",
    "        if df.columns[0].startswith('Unnamed'):\n",
    "            # This looks like a messy CSV with metadata, try to clean it\n",
    "            df = pd.read_csv(file_path, encoding=encoding, header=None)\n",
    "            \n",
    "            # Extract meaningful content\n",
    "            content_parts = []\n",
    "            \n",
    "            # Look for title or description rows\n",
    "            for idx, row in df.iterrows():\n",
    "                row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])\n",
    "                if len(row_text.strip()) > 10:  # Skip empty or very short rows\n",
    "                    content_parts.append(row_text.strip())\n",
    "            \n",
    "            # Join the meaningful parts\n",
    "            content = '\\n'.join(content_parts)\n",
    "            \n",
    "        else:\n",
    "            # Regular CSV - convert to readable format\n",
    "            content_parts = [f\"CSV Data from {file_path.name}\"]\n",
    "            content_parts.append(f\"Columns: {', '.join(df.columns)}\")\n",
    "            content_parts.append(\"\")\n",
    "            \n",
    "            # Add first few rows as examples\n",
    "            for idx, row in df.head(10).iterrows():\n",
    "                row_data = []\n",
    "                for col in df.columns:\n",
    "                    if pd.notna(row[col]):\n",
    "                        row_data.append(f\"{col}: {row[col]}\")\n",
    "                if row_data:\n",
    "                    content_parts.append(\" | \".join(row_data))\n",
    "            \n",
    "            if len(df) > 10:\n",
    "                content_parts.append(f\"... and {len(df) - 10} more rows\")\n",
    "            \n",
    "            content = '\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up any encoding artifacts\n",
    "        content = clean_german_text(content)\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing CSV file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_german_statistics_csv_integrated(file_path: Path, encoding: str = 'utf-8') -> str:\n",
    "    \"\"\"\n",
    "    Specialized processor for German statistics CSV files like BKA crime statistics\n",
    "    Integrated into the normal multi-file processing workflow\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   📊 Using specialized German statistics processing\")\n",
    "        \n",
    "        # Read the raw file with proper encoding\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        content_parts = []\n",
    "        content_parts.append(f\"German Statistical Data from {file_path.name}\")\n",
    "        content_parts.append(\"\")\n",
    "        \n",
    "        # Extract meaningful rows with German text and numbers\n",
    "        meaningful_rows = []\n",
    "        data_rows = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines and pure comma lines\n",
    "            if not line or line.replace(',', '').strip() == '':\n",
    "                continue\n",
    "                \n",
    "            # Parse CSV row\n",
    "            try:\n",
    "                row_parts = [part.strip().strip('\"') for part in line.split(',')]\n",
    "                row_text = ' '.join([part for part in row_parts if part and part != 'nan'])\n",
    "                \n",
    "                # Check if this row contains meaningful German text (more than just column headers)\n",
    "                if len(row_text) > 20:\n",
    "                    # Key statistical content - look for German keywords and numbers\n",
    "                    if any(keyword in row_text.lower() for keyword in [\n",
    "                        'straftat', 'verdächtig', 'kriminal', 'delikt', 'deutschland', \n",
    "                        'polizei', 'ermittelt', 'ausländer', 'rohheit', 'diebstahl'\n",
    "                    ]):\n",
    "                        meaningful_rows.append(f\"Zeile {i+1}: {row_text}\")\n",
    "                        \n",
    "                        # If it contains large numbers, it's likely key statistical data\n",
    "                        if any(num in row_text for num in ['000', '.000', ',000']):\n",
    "                            content_parts.append(f\"📊 WICHTIGE STATISTIK: {row_text}\")\n",
    "                    \n",
    "                    # Data rows with specific crime categories and numbers\n",
    "                    elif any(crime_type in row_text for crime_type in [\n",
    "                        'Strafrechtliche Nebengesetze', 'Rohheitsdelikte', 'Diebstahlsdelikte',\n",
    "                        'Vermögens- und Fälschungsdelikte', 'Gewaltkriminalität', 'Rauschgiftkriminalität'\n",
    "                    ]):\n",
    "                        data_rows.append(row_text)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                # If CSV parsing fails, just treat as text\n",
    "                if len(line) > 20 and any(keyword in line.lower() for keyword in [\n",
    "                    'straftat', 'verdächtig', 'deutschland', 'polizei', 'ausländer'\n",
    "                ]):\n",
    "                    meaningful_rows.append(f\"Text Zeile {i+1}: {line}\")\n",
    "        \n",
    "        # Add meaningful content to the result\n",
    "        if meaningful_rows:\n",
    "            content_parts.append(\"BESCHREIBUNG UND KONTEXT:\")\n",
    "            content_parts.extend(meaningful_rows[:10])  # Limit to avoid too much metadata\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        if data_rows:\n",
    "            content_parts.append(\"STATISTISCHE DATEN:\")\n",
    "            content_parts.extend(data_rows)\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        # Add source information\n",
    "        content_parts.append(\"QUELLE: Bundeskriminalamt (BKA)\")\n",
    "        content_parts.append(\"JAHR: 2024\")\n",
    "        content_parts.append(\"REGION: Deutschland\")\n",
    "        \n",
    "        result = '\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up any encoding issues\n",
    "        result = clean_german_text(result)\n",
    "        \n",
    "        print(f\"   ✅ German statistics: {len(meaningful_rows)} description rows, {len(data_rows)} data rows\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error in German statistics processing: {e}\")\n",
    "        return f\"Error processing German statistics file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_pdf_file(file_path: Path) -> str:\n",
    "    \"\"\"Process PDF files to extract text content with proper encoding\"\"\"\n",
    "    if not PDF_AVAILABLE:\n",
    "        return f\"PDF processing not available for {file_path.name}. Install PyPDF2.\"\n",
    "    \n",
    "    try:\n",
    "        content_parts = []\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                text = page.extract_text()\n",
    "                if text.strip():\n",
    "                    # Clean German encoding issues\n",
    "                    text = clean_german_text(text)\n",
    "                    content_parts.append(f\"Page {page_num}:\\n{text.strip()}\")\n",
    "        \n",
    "        return '\\n\\n'.join(content_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing PDF file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_excel_file(file_path: Path) -> str:\n",
    "    \"\"\"Process Excel files with multiple sheets and proper encoding\"\"\"\n",
    "    if not EXCEL_AVAILABLE:\n",
    "        return f\"Excel processing not available for {file_path.name}. Install openpyxl.\"\n",
    "    \n",
    "    try:\n",
    "        content_parts = [f\"Excel Data from {file_path.name}\"]\n",
    "        \n",
    "        # Read all sheets\n",
    "        xl_file = pd.ExcelFile(file_path)\n",
    "        \n",
    "        for sheet_name in xl_file.sheet_names:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            \n",
    "            content_parts.append(f\"\\nSheet: {sheet_name}\")\n",
    "            content_parts.append(f\"Columns: {', '.join(df.columns)}\")\n",
    "            \n",
    "            # Add first few rows\n",
    "            for idx, row in df.head(5).iterrows():\n",
    "                row_data = []\n",
    "                for col in df.columns:\n",
    "                    if pd.notna(row[col]):\n",
    "                        row_data.append(f\"{col}: {row[col]}\")\n",
    "                if row_data:\n",
    "                    content_parts.append(\" | \".join(row_data))\n",
    "            \n",
    "            if len(df) > 5:\n",
    "                content_parts.append(f\"... and {len(df) - 5} more rows\")\n",
    "        \n",
    "        content = '\\n'.join(content_parts)\n",
    "        # Clean German encoding issues\n",
    "        content = clean_german_text(content)\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing Excel file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_text_file(file_path: Path) -> str:\n",
    "    \"\"\"Process plain text files with proper encoding detection\"\"\"\n",
    "    try:\n",
    "        # Detect best encoding for the file\n",
    "        encoding = detect_encoding(file_path)\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Clean up any encoding artifacts\n",
    "        content = clean_german_text(content)\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback encodings\n",
    "        encodings_to_try = ['utf-8', 'iso-8859-1', 'cp1252', 'latin1']\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    content = f.read()\n",
    "                content = clean_german_text(content)\n",
    "                return content\n",
    "            except (UnicodeDecodeError, UnicodeError):\n",
    "                continue\n",
    "        \n",
    "        return f\"Error reading text file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_json_file(file_path: Path) -> str:\n",
    "    \"\"\"Process JSON files with proper encoding\"\"\"\n",
    "    try:\n",
    "        # Detect encoding first\n",
    "        encoding = detect_encoding(file_path)\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert JSON to readable format\n",
    "        content_parts = [f\"JSON Data from {file_path.name}\"]\n",
    "        json_content = json.dumps(data, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Clean up any encoding artifacts\n",
    "        json_content = clean_german_text(json_content)\n",
    "        content_parts.append(json_content)\n",
    "        \n",
    "        return '\\n'.join(content_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing JSON file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def get_file_processor(file_path: Path):\n",
    "    \"\"\"Get the appropriate processor function for a file type\"\"\"\n",
    "    suffix = file_path.suffix.lower()\n",
    "    \n",
    "    processors = {\n",
    "        '.md': process_text_file,\n",
    "        '.txt': process_text_file,\n",
    "        '.csv': process_csv_file,\n",
    "        '.pdf': process_pdf_file,\n",
    "        '.xlsx': process_excel_file,\n",
    "        '.xls': process_excel_file,\n",
    "        '.json': process_json_file,\n",
    "    }\n",
    "    \n",
    "    return processors.get(suffix, process_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_definition(name: str) -> SearchIndex:\n",
    "    \"\"\"\n",
    "    Returns an Azure Cognitive Search index with the given name.\n",
    "    \"\"\"\n",
    "    # The fields we want to index. The \"embedding\" field is a vector field that will\n",
    "    # be used for vector search.\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"filepath\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
    "        # Additional metadata fields for better referencing\n",
    "        SimpleField(name=\"originalFilename\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"originalTitle\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"documentStem\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"documentType\", type=SearchFieldDataType.String),  # New: file type\n",
    "        SimpleField(name=\"fileExtension\", type=SearchFieldDataType.String),  # New: file extension\n",
    "        SimpleField(name=\"chunkIndex\", type=SearchFieldDataType.Int32),\n",
    "        SimpleField(name=\"totalChunks\", type=SearchFieldDataType.Int32),\n",
    "        SimpleField(name=\"isChunked\", type=SearchFieldDataType.Boolean),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # Size of the vector created by the text-embedding-3-large model.\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"titleVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # Size of the vector created by the text-embedding-3-large model.\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "        ),    \n",
    "    ]\n",
    "\n",
    "    # The \"content\" field should be prioritized for semantic ranking.\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"default\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            keywords_fields=[],\n",
    "            content_fields=[SemanticField(field_name=\"content\")],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # For vector search, we want to use the HNSW (Hierarchical Navigable Small World)\n",
    "    # algorithm (a type of approximate nearest neighbor search algorithm) with cosine\n",
    "    # distance.\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            ),\n",
    "            ExhaustiveKnnAlgorithmConfiguration(\n",
    "                name=\"myExhaustiveKnn\",\n",
    "                kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "                parameters=ExhaustiveKnnParameters(\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "                vectorizer_name=\"myvectorizer\"\n",
    "            ),\n",
    "            VectorSearchProfile(\n",
    "                name=\"myExhaustiveKnnProfile\",\n",
    "                algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "            ),\n",
    "        ],\n",
    "        vectorizers=[  \n",
    "            AzureOpenAIVectorizer(  \n",
    "                vectorizer_name=\"myvectorizer\",  \n",
    "                kind=\"azureOpenAI\",  \n",
    "                parameters=AzureOpenAIVectorizerParameters(  \n",
    "                    resource_url=os.environ[\"AZURE_OPENAI_ENDPOINT\"],  \n",
    "                    deployment_name=os.environ[\"AZURE_EMBEDDING_NAME\"],\n",
    "                    model_name=os.environ[\"AZURE_EMBEDDING_NAME\"],\n",
    "                    # Todo: there is some issue with managed identity for AI search\n",
    "                    # authIdentity=\"/subscriptions/db4948d5-b90c-47d4-91b5-d8c4c43493d2/resourcegroups/rg-chainlit-agent/providers/Microsoft.ManagedIdentity/userAssignedIdentities/id-7r5g6is3dx73u\"\n",
    "                ),\n",
    "            ),  \n",
    "        ],  \n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index.\n",
    "    index = SearchIndex(\n",
    "        name=name,\n",
    "        fields=fields,\n",
    "        semantic_search=semantic_search,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_tokens: int = 6500) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks that fit within the token limit.\n",
    "    Using a more conservative estimate of ~3 characters per token.\n",
    "    \"\"\"\n",
    "    # More conservative estimate: 1 token ≈ 3 characters (down from 4)\n",
    "    # And using 6000 tokens max instead of 7000 for extra safety\n",
    "    max_chars = max_tokens * 3\n",
    "    \n",
    "    # Split by paragraphs first (double newlines)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If adding this paragraph would exceed the limit, start a new chunk\n",
    "        if len(current_chunk) + len(paragraph) + 2 > max_chars and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + paragraph\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    # Handle edge case where a single paragraph is too long\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) <= max_chars:\n",
    "            final_chunks.append(chunk)\n",
    "        else:\n",
    "            # Split by sentences if paragraph is too long\n",
    "            sentences = chunk.split('. ')\n",
    "            temp_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                if len(temp_chunk) + len(sentence) + 2 > max_chars and temp_chunk:\n",
    "                    final_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = sentence\n",
    "                else:\n",
    "                    if temp_chunk:\n",
    "                        temp_chunk += \". \" + sentence\n",
    "                    else:\n",
    "                        temp_chunk = sentence\n",
    "            if temp_chunk:\n",
    "                final_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    # Final safety check - if any chunk is still too large, split it by words\n",
    "    safe_chunks = []\n",
    "    for chunk in final_chunks:\n",
    "        if len(chunk) <= max_chars:\n",
    "            safe_chunks.append(chunk)\n",
    "        else:\n",
    "            # Emergency word-level splitting\n",
    "            words = chunk.split()\n",
    "            temp_chunk = \"\"\n",
    "            for word in words:\n",
    "                if len(temp_chunk) + len(word) + 1 > max_chars and temp_chunk:\n",
    "                    safe_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = word\n",
    "                else:\n",
    "                    if temp_chunk:\n",
    "                        temp_chunk += \" \" + word\n",
    "                    else:\n",
    "                        temp_chunk = word\n",
    "            if temp_chunk:\n",
    "                safe_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    return safe_chunks\n",
    "\n",
    "def gen_multi_format_documents(\n",
    "    folder_path: str,\n",
    ") -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Process documents of multiple formats (MD, CSV, PDF, Excel, etc.)\n",
    "    \"\"\"\n",
    "    openai_service_endoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    openai_deployment = os.environ[\"AZURE_EMBEDDING_NAME\"]\n",
    "\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")    \n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2023-07-01-preview\",\n",
    "        azure_endpoint=openai_service_endoint,\n",
    "        azure_deployment=openai_deployment,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "    items = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Supported file extensions\n",
    "    supported_extensions = {'.md', '.txt', '.csv', '.pdf', '.xlsx', '.xls', '.json'}\n",
    "    \n",
    "    # Find all supported files in the folder and subfolders\n",
    "    all_files = []\n",
    "    for ext in supported_extensions:\n",
    "        all_files.extend(folder.glob(f\"**/*{ext}\"))\n",
    "    \n",
    "    print(f\"Found {len(all_files)} supported files to process\")\n",
    "    \n",
    "    doc_id = 1\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            print(f\"\\nProcessing: {file_path.name} ({file_path.suffix})\")\n",
    "            \n",
    "            # Get the appropriate processor for this file type\n",
    "            processor = get_file_processor(file_path)\n",
    "            content = processor(file_path)\n",
    "            \n",
    "            if not content or len(content.strip()) < 50:\n",
    "                print(f\"⚠️  Skipping {file_path.name} - insufficient content\")\n",
    "                continue\n",
    "            \n",
    "            # Determine document type and title\n",
    "            file_extension = file_path.suffix.lower()\n",
    "            document_type = {\n",
    "                '.md': 'Markdown Document',\n",
    "                '.txt': 'Text Document', \n",
    "                '.csv': 'CSV Data Table',\n",
    "                '.pdf': 'PDF Document',\n",
    "                '.xlsx': 'Excel Spreadsheet',\n",
    "                '.xls': 'Excel Spreadsheet',\n",
    "                '.json': 'JSON Data'\n",
    "            }.get(file_extension, 'Unknown Document')\n",
    "            \n",
    "            # Extract title based on file type\n",
    "            if file_extension == '.md':\n",
    "                # Extract title from first heading or use filename\n",
    "                title = file_path.stem\n",
    "                lines = content.split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.startswith('# '):\n",
    "                        title = line[2:].strip()\n",
    "                        break\n",
    "            elif file_extension == '.csv':\n",
    "                # For CSV, try to extract a meaningful title from content\n",
    "                title = file_path.stem\n",
    "                # Look for title in first few lines\n",
    "                lines = content.split('\\n')[:5]\n",
    "                for line in lines:\n",
    "                    if len(line.strip()) > 10 and not line.startswith('Unnamed'):\n",
    "                        title = line.strip()[:100]  # Limit title length\n",
    "                        break\n",
    "            else:\n",
    "                title = file_path.stem\n",
    "            \n",
    "            # Preserve document metadata for referencing\n",
    "            original_filename = file_path.name\n",
    "            document_path = str(file_path.relative_to(folder))\n",
    "            document_stem = file_path.stem\n",
    "            \n",
    "            # Split content into chunks\n",
    "            content_chunks = chunk_text(content)\n",
    "            \n",
    "            print(f\"Document: {title} - Split into {len(content_chunks)} chunks\")\n",
    "            \n",
    "            # Process each chunk as a separate document\n",
    "            for chunk_idx, chunk in enumerate(content_chunks):\n",
    "                # Create unique ID for each chunk\n",
    "                chunk_id = f\"{doc_id}_{chunk_idx + 1}\" if len(content_chunks) > 1 else str(doc_id)\n",
    "                \n",
    "                # Preserve original title for reference, add chunk info only for display\n",
    "                display_title = title\n",
    "                if len(content_chunks) > 1:\n",
    "                    display_title = f\"{title} (Part {chunk_idx + 1})\"\n",
    "                \n",
    "                # Enhanced content with document reference information\n",
    "                enhanced_content = f\"Source Document: {original_filename}\\n\"\n",
    "                enhanced_content += f\"Document Type: {document_type}\\n\"\n",
    "                if len(content_chunks) > 1:\n",
    "                    enhanced_content += f\"Document Section: Part {chunk_idx + 1} of {len(content_chunks)}\\n\"\n",
    "                enhanced_content += f\"Original Title: {title}\\n\\n{chunk}\"\n",
    "                \n",
    "                url = f\"/docs/{document_path.replace(file_extension, '').replace('/', '-').lower()}\"\n",
    "                if len(content_chunks) > 1:\n",
    "                    url += f\"-part-{chunk_idx + 1}\"\n",
    "                \n",
    "                # Generate embeddings for enhanced content and title\n",
    "                try:\n",
    "                    print(f\"Processing chunk {chunk_idx + 1}: {len(enhanced_content)} chars (estimated {len(enhanced_content)//3} tokens)\")\n",
    "                    \n",
    "                    content_emb = client.embeddings.create(input=enhanced_content, model=openai_deployment)\n",
    "                    title_emb = client.embeddings.create(input=display_title, model=openai_deployment)\n",
    "                    \n",
    "                    rec = {\n",
    "                        \"id\": chunk_id,\n",
    "                        \"content\": enhanced_content,  # Contains source reference\n",
    "                        \"filepath\": document_path,     # Original relative path\n",
    "                        \"title\": display_title,       # Display title with chunk info\n",
    "                        \"url\": url,\n",
    "                        \"contentVector\": content_emb.data[0].embedding,\n",
    "                        \"titleVector\": title_emb.data[0].embedding,\n",
    "                        # Additional metadata for better referencing\n",
    "                        \"originalFilename\": original_filename,\n",
    "                        \"originalTitle\": title,\n",
    "                        \"documentStem\": document_stem,\n",
    "                        \"documentType\": document_type,\n",
    "                        \"fileExtension\": file_extension,\n",
    "                        \"chunkIndex\": chunk_idx + 1,\n",
    "                        \"totalChunks\": len(content_chunks),\n",
    "                        \"isChunked\": len(content_chunks) > 1\n",
    "                    }\n",
    "                    items.append(rec)\n",
    "                    print(f\"✓ Successfully processed: {display_title}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error processing chunk {chunk_idx + 1} of {title}: {e}\")\n",
    "                    print(f\"  Enhanced content length: {len(enhanced_content)} characters (estimated {len(enhanced_content)//3} tokens)\")\n",
    "                    print(f\"  Original chunk length: {len(chunk)} characters\")\n",
    "                    continue\n",
    "            \n",
    "            doc_id += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing file {file_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return items\n",
    "\n",
    "# Keep the old function for backward compatibility\n",
    "def gen_markdown_documents(folder_path: str) -> List[Dict[str, any]]:\n",
    "    \"\"\"Legacy function - use gen_multi_format_documents instead\"\"\"\n",
    "    return gen_multi_format_documents(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting fresh index creation pipeline...\n",
      "Index: parssedindexer1\n",
      "Endpoint: https://srch-cqbrigubozxty.search.windows.net/\n",
      "🗑️ Deleting existing index 'parssedindexer1'...\n",
      "deleting index parssedindexer1\n",
      "🗑️ Deleting existing index 'parssedindexer1'...\n",
      "deleting index parssedindexer1\n",
      "✅ Index deleted successfully\n",
      "🏗️ Creating new index 'parssedindexer1' with latest schema...\n",
      "✅ Index deleted successfully\n",
      "🏗️ Creating new index 'parssedindexer1' with latest schema...\n",
      "✅ Fresh index 'parssedindexer1' created successfully!\n",
      "📝 Schema includes all metadata fields: documentType, fileExtension, etc.\n",
      "✅ Fresh index 'parssedindexer1' created successfully!\n",
      "📝 Schema includes all metadata fields: documentType, fileExtension, etc.\n"
     ]
    }
   ],
   "source": [
    "search_endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "\n",
    "print(f\"🔄 Starting fresh index creation pipeline...\")\n",
    "print(f\"Index: {index_name}\")\n",
    "print(f\"Endpoint: {search_endpoint}\")\n",
    "\n",
    "search_index_client = SearchIndexClient(\n",
    "    search_endpoint, DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Always delete existing index for fresh start (testing mode)\n",
    "try:\n",
    "    existing_index = search_index_client.get_index(index_name)\n",
    "    print(f\"🗑️ Deleting existing index '{index_name}'...\")\n",
    "    delete_index(search_index_client, index_name)\n",
    "    print(f\"✅ Index deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"ℹ️ Index '{index_name}' did not exist (this is fine)\")\n",
    "\n",
    "# Create fresh index with latest schema\n",
    "print(f\"🏗️ Creating new index '{index_name}' with latest schema...\")\n",
    "index = create_index_definition(index_name)\n",
    "search_index_client.create_or_update_index(index)\n",
    "print(f\"✅ Fresh index '{index_name}' created successfully!\")\n",
    "print(f\"📝 Schema includes all metadata fields: documentType, fileExtension, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents for indexing...\n",
      "🔍 Supported file types: .md, .txt, .csv, .pdf, .xlsx, .xls, .json\n",
      "📂 Scanning folder: ./data/\n",
      "Found 5 supported files to process\n",
      "\n",
      "Processing: merged_statistic_id558217.csv (.csv)\n",
      "Document: Statistik als Exceldatei - Split into 1 chunks\n",
      "Processing chunk 1: 2156 chars (estimated 718 tokens)\n",
      "✓ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_Ausländerkriminalität in Deutschland.pdf (.pdf)\n",
      "✓ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_Ausländerkriminalität in Deutschland.pdf (.pdf)\n",
      "Document: Statista_2025_Ausländerkriminalität in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "Document: Statista_2025_Ausländerkriminalität in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "✓ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "\n",
      "📤 Uploading 11 document chunks to fresh index...\n",
      "✓ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "\n",
      "📤 Uploading 11 document chunks to fresh index...\n",
      "✅ Uploaded batch 1: 11 documents\n",
      "\n",
      "🎉 Document indexing completed!\n",
      "   📊 Total chunks processed: 11\n",
      "   ✅ Successful batches: 1\n",
      "\n",
      "📋 Documents processed by type:\n",
      "   📄 CSV Data Table: 1 chunks\n",
      "   📄 Markdown Document: 7 chunks\n",
      "   📄 PDF Document: 3 chunks\n",
      "✅ Uploaded batch 1: 11 documents\n",
      "\n",
      "🎉 Document indexing completed!\n",
      "   📊 Total chunks processed: 11\n",
      "   ✅ Successful batches: 1\n",
      "\n",
      "📋 Documents processed by type:\n",
      "   📄 CSV Data Table: 1 chunks\n",
      "   📄 Markdown Document: 7 chunks\n",
      "   📄 PDF Document: 3 chunks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing documents for indexing...\")\n",
    "\n",
    "# Configuration for document processing\n",
    "OVERWRITE_EXISTING = False  # Set to True to overwrite existing documents\n",
    "CHECK_FOR_DUPLICATES = True  # Set to True to avoid processing already indexed files\n",
    "\n",
    "# Change this path to point to your folder containing documents\n",
    "# Now supports: .md, .txt, .csv, .pdf, .xlsx, .xls, .json files\n",
    "document_folder = \"./data/\"  # Update this path as needed\n",
    "\n",
    "# Create search client for document operations\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "# Process all documents using the multi-format function\n",
    "print(f\"🔍 Supported file types: .md, .txt, .csv, .pdf, .xlsx, .xls, .json\")\n",
    "print(f\"📂 Scanning folder: {document_folder}\")\n",
    "\n",
    "docs = gen_multi_format_documents(document_folder)\n",
    "\n",
    "if len(docs) == 0:\n",
    "    print(\"⚠️ No documents found to process\")\n",
    "    print(f\"   Make sure your documents are in: {document_folder}\")\n",
    "    print(f\"   Supported extensions: .md, .txt, .csv, .pdf, .xlsx, .xls, .json\")\n",
    "else:\n",
    "    print(f\"\\n📤 Uploading {len(docs)} document chunks to fresh index...\")\n",
    "    \n",
    "    # Upload in batches to handle large document sets\n",
    "    batch_size = 50\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        try:\n",
    "            result = search_client.upload_documents(batch)\n",
    "            successful_batches += 1\n",
    "            print(f\"✅ Uploaded batch {i//batch_size + 1}: {len(batch)} documents\")\n",
    "        except Exception as e:\n",
    "            failed_batches += 1\n",
    "            print(f\"❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Document indexing completed!\")\n",
    "    print(f\"   📊 Total chunks processed: {len(docs)}\")\n",
    "    print(f\"   ✅ Successful batches: {successful_batches}\")\n",
    "    if failed_batches > 0:\n",
    "        print(f\"   ❌ Failed batches: {failed_batches}\")\n",
    "    \n",
    "    # Show summary by document type\n",
    "    type_summary = {}\n",
    "    for doc in docs:\n",
    "        doc_type = doc.get(\"documentType\", \"Unknown\")\n",
    "        type_summary[doc_type] = type_summary.get(doc_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\n📋 Documents processed by type:\")\n",
    "    for doc_type, count in sorted(type_summary.items()):\n",
    "        print(f\"   📄 {doc_type}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Fresh Index Ready!\n",
      "📊 Fresh Index Statistics:\n",
      "   📄 Total documents/chunks: 0\n",
      "   📁 Unique source files: 0\n",
      "   📈 Average chunks per file: 0.0\n",
      "\n",
      "🔍 Checking for encoding issues...\n",
      "📊 Fresh Index Statistics:\n",
      "   📄 Total documents/chunks: 0\n",
      "   📁 Unique source files: 0\n",
      "   📈 Average chunks per file: 0.0\n",
      "\n",
      "🔍 Checking for encoding issues...\n",
      "🚨 Found 3 documents with encoding issues:\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: Ã¤, Ã\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: Ã¤, Ã\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: Ã¤, Ã\n",
      "\n",
      "💡 Recommendation: Run the fresh index pipeline to fix encoding issues\n",
      "\n",
      "📁 Files in index:\n",
      "🚨 Found 3 documents with encoding issues:\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: Ã¤, Ã\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: Ã¤, Ã\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: Ã¤, Ã\n",
      "\n",
      "💡 Recommendation: Run the fresh index pipeline to fix encoding issues\n",
      "\n",
      "📁 Files in index:\n",
      "📁 Indexed Files:\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md (Markdown Document, 3 chunks)\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (Markdown Document, 1 chunks)\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (Markdown Document, 3 chunks)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf (PDF Document, 3 chunks)\n",
      "   📄 merged_statistic_id558217.csv (CSV Data Table, 1 chunks)\n",
      "\n",
      "🇩🇪 Testing German search:\n",
      "🇩🇪 Testing German search terms:\n",
      "\n",
      "🔍 Searching for: 'Kriminalität'\n",
      "📁 Indexed Files:\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md (Markdown Document, 3 chunks)\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (Markdown Document, 1 chunks)\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (Markdown Document, 3 chunks)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf (PDF Document, 3 chunks)\n",
      "   📄 merged_statistic_id558217.csv (CSV Data Table, 1 chunks)\n",
      "\n",
      "🇩🇪 Testing German search:\n",
      "🇩🇪 Testing German search terms:\n",
      "\n",
      "🔍 Searching for: 'Kriminalität'\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1) (from Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md)\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted (Part 3) (from Baier_2015_Migration und KriminalitÃ¤t_converted.md)\n",
      "\n",
      "🔍 Searching for: 'Ausländer'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 2) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 1) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "\n",
      "🔍 Searching for: 'Straftäter'\n",
      "   ⚠️ No results found - possible encoding issues\n",
      "\n",
      "🔍 Searching for: 'Deutschland'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 1) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 2) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 3) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1) (from Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md)\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted (Part 3) (from Baier_2015_Migration und KriminalitÃ¤t_converted.md)\n",
      "\n",
      "🔍 Searching for: 'Ausländer'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 2) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 1) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "\n",
      "🔍 Searching for: 'Straftäter'\n",
      "   ⚠️ No results found - possible encoding issues\n",
      "\n",
      "🔍 Searching for: 'Deutschland'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 1) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 2) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland (Part 3) (from Statista_2025_Ausländerkriminalität in Deutschland.pdf)\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for index management\n",
    "\n",
    "def clean_german_text_simple(text: str) -> str:\n",
    "    \"\"\"Clean up common German encoding issues\"\"\"\n",
    "    # Fix common UTF-8 encoding artifacts for German umlauts\n",
    "    replacements = {\n",
    "        'Ã¤': 'ä',\n",
    "        'Ã¶': 'ö', \n",
    "        'Ã¼': 'ü',\n",
    "        'Ã': 'ß',\n",
    "        'Ã„': 'Ä',\n",
    "        'Ã–': 'Ö',\n",
    "        'Ãœ': 'Ü',\n",
    "        'â€œ': '\"',\n",
    "        'â€': '\"',\n",
    "        'â€™': \"'\",\n",
    "        'â€\"': '–',\n",
    "        'â€\"': '—'\n",
    "    }\n",
    "    \n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def check_encoding_issues():\n",
    "    \"\"\"Check if there are encoding issues in the current index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get a few documents to check for encoding issues\n",
    "        results = search_client.search(\"*\", select=[\"id\", \"title\", \"content\", \"originalFilename\"], top=10)\n",
    "        \n",
    "        encoding_issues = []\n",
    "        for result in results:\n",
    "            title = result.get(\"title\", \"\")\n",
    "            content = result.get(\"content\", \"\")\n",
    "            filename = result.get(\"originalFilename\", \"\")\n",
    "            \n",
    "            # Check for common encoding artifacts\n",
    "            if any(artifact in title + content for artifact in ['Ã¤', 'Ã¶', 'Ã¼', 'Ã', 'â€']):\n",
    "                encoding_issues.append({\n",
    "                    \"id\": result[\"id\"],\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": title,\n",
    "                    \"issues\": [artifact for artifact in ['Ã¤', 'Ã¶', 'Ã¼', 'Ã'] if artifact in title + content]\n",
    "                })\n",
    "        \n",
    "        if encoding_issues:\n",
    "            print(f\"🚨 Found {len(encoding_issues)} documents with encoding issues:\")\n",
    "            for issue in encoding_issues[:5]:  # Show first 5\n",
    "                print(f\"   📄 {issue['filename']}: {', '.join(issue['issues'])}\")\n",
    "            if len(encoding_issues) > 5:\n",
    "                print(f\"   ... and {len(encoding_issues) - 5} more\")\n",
    "            print(f\"\\n💡 Recommendation: Run the fresh index pipeline to fix encoding issues\")\n",
    "        else:\n",
    "            print(f\"✅ No encoding issues detected in current index\")\n",
    "            \n",
    "        return len(encoding_issues)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking encoding: {e}\")\n",
    "        return -1\n",
    "\n",
    "def get_index_stats():\n",
    "    \"\"\"Get statistics about the current fresh index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get total document count\n",
    "        results = search_client.search(\"*\", select=[\"id\"], include_total_count=True)\n",
    "        total_docs = results.get_count()\n",
    "        \n",
    "        # Get document type breakdown (schema is fresh, so these fields exist)\n",
    "        file_results = search_client.search(\"*\", select=[\"originalFilename\", \"documentType\"])\n",
    "        \n",
    "        unique_files = set()\n",
    "        doc_types = {}\n",
    "        \n",
    "        for result in file_results:\n",
    "            if result.get(\"originalFilename\"):\n",
    "                unique_files.add(result[\"originalFilename\"])\n",
    "            if result.get(\"documentType\"):\n",
    "                doc_type = result[\"documentType\"]\n",
    "                doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "        \n",
    "        print(f\"📊 Fresh Index Statistics:\")\n",
    "        print(f\"   📄 Total documents/chunks: {total_docs}\")\n",
    "        print(f\"   📁 Unique source files: {len(unique_files)}\")\n",
    "        print(f\"   📈 Average chunks per file: {total_docs / len(unique_files) if unique_files else 0:.1f}\")\n",
    "        \n",
    "        if doc_types:\n",
    "            print(f\"\\n📋 Document Types:\")\n",
    "            for doc_type, count in sorted(doc_types.items()):\n",
    "                print(f\"   📄 {doc_type}: {count} chunks\")\n",
    "        \n",
    "        # Check for encoding issues\n",
    "        print(f\"\\n🔍 Checking for encoding issues...\")\n",
    "        check_encoding_issues()\n",
    "        \n",
    "        return total_docs, len(unique_files)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting index stats: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def delete_documents_by_filename(filename: str):\n",
    "    \"\"\"Delete all chunks from a specific source file\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Find all documents from this file\n",
    "        results = search_client.search(f'originalFilename:\"{filename}\"', select=[\"id\"])\n",
    "        doc_ids = [result[\"id\"] for result in results]\n",
    "        \n",
    "        if doc_ids:\n",
    "            # Delete the documents\n",
    "            delete_docs = [{\"@search.action\": \"delete\", \"id\": doc_id} for doc_id in doc_ids]\n",
    "            search_client.upload_documents(delete_docs)\n",
    "            print(f\"🗑️ Deleted {len(doc_ids)} chunks from file '{filename}'\")\n",
    "        else:\n",
    "            print(f\"ℹ️ No documents found for file '{filename}'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deleting documents: {e}\")\n",
    "\n",
    "def delete_documents_by_type(document_type: str):\n",
    "    \"\"\"Delete all documents of a specific type (e.g., 'CSV Data Table')\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        results = search_client.search(f'documentType:\"{document_type}\"', select=[\"id\"])\n",
    "        doc_ids = [result[\"id\"] for result in results]\n",
    "        \n",
    "        if doc_ids:\n",
    "            # Delete the documents\n",
    "            delete_docs = [{\"@search.action\": \"delete\", \"id\": doc_id} for doc_id in doc_ids]\n",
    "            search_client.upload_documents(delete_docs)\n",
    "            print(f\"🗑️ Deleted {len(doc_ids)} chunks of type '{document_type}'\")\n",
    "        else:\n",
    "            print(f\"ℹ️ No documents found for type '{document_type}'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deleting documents: {e}\")\n",
    "\n",
    "def list_indexed_files():\n",
    "    \"\"\"List all files that have been indexed with their types\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        results = search_client.search(\"*\", \n",
    "                                     select=[\"originalFilename\", \"documentType\", \"totalChunks\", \"fileExtension\"])\n",
    "        \n",
    "        files_info = {}\n",
    "        for result in results:\n",
    "            filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "            doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "            chunks = result.get(\"totalChunks\", 1)\n",
    "            extension = result.get(\"fileExtension\", \"\")\n",
    "            \n",
    "            if filename not in files_info:\n",
    "                files_info[filename] = {\n",
    "                    \"type\": doc_type,\n",
    "                    \"chunks\": chunks,\n",
    "                    \"extension\": extension\n",
    "                }\n",
    "        \n",
    "        print(\"📁 Indexed Files:\")\n",
    "        for filename, info in sorted(files_info.items()):\n",
    "            print(f\"   📄 {filename} ({info['type']}, {info['chunks']} chunks)\")\n",
    "            \n",
    "        return files_info\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error listing files: {e}\")\n",
    "        return {}\n",
    "\n",
    "def search_by_file_type(file_type: str, query: str = \"*\", top: int = 5):\n",
    "    \"\"\"Search within specific file types\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Use documentType filter (schema is fresh, so this field exists)\n",
    "        filter_expr = f'documentType eq \"{file_type}\"'\n",
    "        \n",
    "        results = search_client.search(\n",
    "            query, \n",
    "            filter=filter_expr,\n",
    "            select=[\"title\", \"originalFilename\", \"documentType\", \"content\"],\n",
    "            top=top\n",
    "        )\n",
    "        \n",
    "        print(f\"🔍 Search results for '{query}' in {file_type} files:\")\n",
    "        for result in results:\n",
    "            print(f\"   📄 {result['title']} (from {result['originalFilename']})\")\n",
    "            print(f\"      Preview: {result['content'][:200]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error searching: {e}\")\n",
    "\n",
    "def test_vector_search(query: str = \"product data\", top: int = 3):\n",
    "    \"\"\"Test vector search functionality with the fresh index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        print(f\"🔍 Testing vector search for: '{query}'\")\n",
    "        \n",
    "        # Perform hybrid search (text + vector)\n",
    "        results = search_client.search(\n",
    "            query,\n",
    "            select=[\"title\", \"originalFilename\", \"documentType\", \"content\"],\n",
    "            top=top,\n",
    "            query_type=\"semantic\",\n",
    "            semantic_configuration_name=\"default\"\n",
    "        )\n",
    "        \n",
    "        print(f\"📋 Top {top} results:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"   {i}. {result['title']} ({result['documentType']})\")\n",
    "            print(f\"      File: {result['originalFilename']}\")\n",
    "            print(f\"      Preview: {result['content'][:150]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in vector search: {e}\")\n",
    "\n",
    "def test_german_search():\n",
    "    \"\"\"Test search with German terms to check for encoding issues\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Test with German terms that might have encoding issues\n",
    "        test_queries = [\n",
    "            \"Kriminalität\",\n",
    "            \"Ausländer\", \n",
    "            \"Straftäter\",\n",
    "            \"Deutschland\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"🇩🇪 Testing German search terms:\")\n",
    "        for query in test_queries:\n",
    "            print(f\"\\n🔍 Searching for: '{query}'\")\n",
    "            results = search_client.search(\n",
    "                query,\n",
    "                select=[\"title\", \"originalFilename\", \"content\"],\n",
    "                top=3\n",
    "            )\n",
    "            \n",
    "            result_count = 0\n",
    "            for result in results:\n",
    "                result_count += 1\n",
    "                title = result['title']\n",
    "                filename = result['originalFilename']\n",
    "                print(f\"   📄 {title} (from {filename})\")\n",
    "            \n",
    "            if result_count == 0:\n",
    "                print(f\"   ⚠️ No results found - possible encoding issues\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in German search test: {e}\")\n",
    "\n",
    "# Show current statistics for the fresh index\n",
    "print(\"🎯 Fresh Index Ready!\")\n",
    "get_index_stats()\n",
    "print(\"\\n📁 Files in index:\")\n",
    "list_indexed_files()\n",
    "print(\"\\n🇩🇪 Testing German search:\")\n",
    "test_german_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing fresh index with sample searches...\n",
      "\n",
      "1️⃣ Testing general content search:\n",
      "🔍 Testing vector search for: 'data analysis'\n",
      "📋 Top 3 results:\n",
      "2️⃣ Only one document type found, skipping type-specific search\n",
      "\n",
      "3️⃣ Testing semantic search:\n",
      "   📊 Semantic search results:\n",
      "\n",
      "✅ Fresh index testing completed!\n",
      "\n",
      "============================================================\n",
      "🎉 FRESH INDEX PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "✅ Index deleted and recreated with latest schema\n",
      "✅ All documents processed and uploaded\n",
      "✅ Utility functions ready for use\n",
      "✅ Sample searches tested\n",
      "\n",
      "💡 Next steps:\n",
      "   • Use the utility functions above to manage your index\n",
      "   • Run search_by_file_type() to search specific document types\n",
      "   • Use delete_documents_by_filename() to remove specific files\n",
      "   • The index is now ready for your application!\n",
      "============================================================\n",
      "2️⃣ Only one document type found, skipping type-specific search\n",
      "\n",
      "3️⃣ Testing semantic search:\n",
      "   📊 Semantic search results:\n",
      "\n",
      "✅ Fresh index testing completed!\n",
      "\n",
      "============================================================\n",
      "🎉 FRESH INDEX PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "✅ Index deleted and recreated with latest schema\n",
      "✅ All documents processed and uploaded\n",
      "✅ Utility functions ready for use\n",
      "✅ Sample searches tested\n",
      "\n",
      "💡 Next steps:\n",
      "   • Use the utility functions above to manage your index\n",
      "   • Run search_by_file_type() to search specific document types\n",
      "   • Use delete_documents_by_filename() to remove specific files\n",
      "   • The index is now ready for your application!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Index Schema Update Helper\n",
    "\n",
    "# Step 4: Test the Fresh Index with Sample Searches\n",
    "\n",
    "def perform_test_searches():\n",
    "    \"\"\"Run sample searches to test the fresh index functionality\"\"\"\n",
    "    \n",
    "    print(\"🧪 Testing fresh index with sample searches...\\n\")\n",
    "    \n",
    "    # Test 1: General content search\n",
    "    print(\"1️⃣ Testing general content search:\")\n",
    "    test_vector_search(\"data analysis\", top=3)\n",
    "    \n",
    "    # Test 2: Search by document type if we have multiple types\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get available document types\n",
    "        type_results = search_client.search(\"*\", select=[\"documentType\"], top=100)\n",
    "        doc_types = set()\n",
    "        for result in type_results:\n",
    "            if result.get(\"documentType\"):\n",
    "                doc_types.add(result[\"documentType\"])\n",
    "        \n",
    "        if len(doc_types) > 1:\n",
    "            print(\"2️⃣ Testing search by document type:\")\n",
    "            for doc_type in sorted(doc_types):\n",
    "                print(f\"\\n   📄 Searching in {doc_type}:\")\n",
    "                search_by_file_type(doc_type, \"*\", top=2)\n",
    "        else:\n",
    "            print(\"2️⃣ Only one document type found, skipping type-specific search\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in document type search test: {e}\")\n",
    "    \n",
    "    # Test 3: Semantic search\n",
    "    print(\"\\n3️⃣ Testing semantic search:\")\n",
    "    try:\n",
    "        results = search_client.search(\n",
    "            \"statistics and numbers\",\n",
    "            select=[\"title\", \"originalFilename\", \"content\"],\n",
    "            top=3,\n",
    "            query_type=\"semantic\",\n",
    "            semantic_configuration_name=\"default\"\n",
    "        )\n",
    "        \n",
    "        print(\"   📊 Semantic search results:\")\n",
    "        for result in results:\n",
    "            print(f\"   • {result['title']} (from {result['originalFilename']})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Semantic search not fully configured: {e}\")\n",
    "    \n",
    "    print(\"\\n✅ Fresh index testing completed!\")\n",
    "\n",
    "# Run the test searches\n",
    "perform_test_searches()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 FRESH INDEX PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Index deleted and recreated with latest schema\")\n",
    "print(\"✅ All documents processed and uploaded\")  \n",
    "print(\"✅ Utility functions ready for use\")\n",
    "print(\"✅ Sample searches tested\")\n",
    "print(\"\\n💡 Next steps:\")\n",
    "print(\"   • Use the utility functions above to manage your index\")\n",
    "print(\"   • Run search_by_file_type() to search specific document types\")\n",
    "print(\"   • Use delete_documents_by_filename() to remove specific files\")\n",
    "print(\"   • The index is now ready for your application!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for 'ausländische Straftatverdächtige':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   Title: Statista_2025_Ausländerkriminalität in Deutschland (Part 1)\n",
      "   ✅ No encoding issues\n",
      "   🎯 Contains target statistics!\n",
      "      Excerpt: ...engesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Auslände...\n",
      "   Content preview: Source Document: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "Document Type: PDF Document\n",
      "Document Section: Part 1 of 3\n",
      "Original Title: Statista_2025_Ausländerkriminalität in Deutschland...\n",
      "\n",
      "📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "   Title: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1)\n",
      "   ❌ Encoding issues found!\n",
      "      Found 'Ã¤' in content\n",
      "      Found 'Ã' in content\n",
      "   Content preview: Source Document: Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "Document Type: Markdown Document\n",
      "Document Section: Part 1 of 3\n",
      "Original Title: Baier_2015_Migration und KriminalitÃ¤t_converted\n",
      "\n",
      "<!...\n",
      "\n",
      "📄 merged_statistic_id558217.csv\n",
      "   Title: Statistik als Exceldatei\n",
      "   ✅ No encoding issues\n",
      "   🎯 Contains target statistics!\n",
      "      Excerpt: ...engesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Auslände...\n",
      "   Content preview: Source Document: merged_statistic_id558217.csv\n",
      "Document Type: CSV Data Table\n",
      "Original Title: Statistik als Exceldatei\n",
      "\n",
      "Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4\n",
      "Statistik als Exceldatei\n",
      "A...\n",
      "\n",
      "📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   Title: Statista_2025_Ausländerkriminalität in Deutschland (Part 2)\n",
      "   ✅ No encoding issues\n",
      "   Content preview: Source Document: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "Document Type: PDF Document\n",
      "Document Section: Part 2 of 3\n",
      "Original Title: Statista_2025_Ausländerkriminalität in Deutschland...\n",
      "\n",
      "==================================================\n",
      "📊 Testing CSV extraction from: merged_statistic_id558217.csv\n",
      "   Columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "   Shape: (38, 5)\n",
      "   🎯 Found target row 7:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. D...\n",
      "\n",
      "   First few meaningful rows:\n",
      "   Row 1: Statistik als Exceldatei...\n",
      "   Row 2: Anzahl der ausländischen Straftatverdächtigen in Deutschland nach Straftatengruppen im Jahr 2024...\n",
      "   Row 6: Quellenangaben Beschreibung...\n"
     ]
    }
   ],
   "source": [
    "# Quick Encoding and Content Check\n",
    "\n",
    "def check_current_encoding_issues():\n",
    "    \"\"\"Check if there are encoding issues in the current index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Search for documents that might contain the specific information\n",
    "        results = search_client.search(\"ausländische Straftatverdächtige\", select=[\"id\", \"title\", \"content\", \"originalFilename\"], top=5)\n",
    "        \n",
    "        print(\"🔍 Searching for 'ausländische Straftatverdächtige':\")\n",
    "        found_results = False\n",
    "        for result in results:\n",
    "            found_results = True\n",
    "            title = result.get(\"title\", \"\")\n",
    "            content = result.get(\"content\", \"\")\n",
    "            filename = result.get(\"originalFilename\", \"\")\n",
    "            \n",
    "            print(f\"\\n📄 {filename}\")\n",
    "            print(f\"   Title: {title}\")\n",
    "            \n",
    "            # Check for encoding issues\n",
    "            if any(artifact in title + content for artifact in ['Ã¤', 'Ã¶', 'Ã¼', 'Ã']):\n",
    "                print(f\"   ❌ Encoding issues found!\")\n",
    "                # Show the problematic content\n",
    "                for artifact in ['Ã¤', 'Ã¶', 'Ã¼', 'Ã']:\n",
    "                    if artifact in content:\n",
    "                        print(f\"      Found '{artifact}' in content\")\n",
    "            else:\n",
    "                print(f\"   ✅ No encoding issues\")\n",
    "            \n",
    "            # Look for the specific statistic we want\n",
    "            if \"913.000\" in content or \"913,000\" in content or \"330.000\" in content:\n",
    "                print(f\"   🎯 Contains target statistics!\")\n",
    "                # Show excerpt\n",
    "                if \"330.000\" in content:\n",
    "                    start = content.find(\"330.000\") - 50\n",
    "                    end = content.find(\"330.000\") + 100\n",
    "                    excerpt = content[max(0, start):end]\n",
    "                    print(f\"      Excerpt: ...{excerpt}...\")\n",
    "            \n",
    "            print(f\"   Content preview: {content[:200]}...\")\n",
    "        \n",
    "        if not found_results:\n",
    "            print(\"   ⚠️ No results found for German search term\")\n",
    "            \n",
    "            # Try with encoding artifacts\n",
    "            print(f\"\\n🔍 Trying with encoding artifacts:\")\n",
    "            artifact_results = search_client.search(\"auslÃ¤ndische\", select=[\"id\", \"title\", \"originalFilename\"], top=3)\n",
    "            for result in artifact_results:\n",
    "                print(f\"   📄 {result.get('originalFilename', 'Unknown')}: {result.get('title', 'No title')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking encoding: {e}\")\n",
    "\n",
    "def test_csv_content_extraction():\n",
    "    \"\"\"Test how the current CSV file is being processed\"\"\"\n",
    "    csv_file_path = Path(\"./data/merged_statistic_id558217.csv\")\n",
    "    \n",
    "    if not csv_file_path.exists():\n",
    "        print(f\"❌ CSV file not found: {csv_file_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 Testing CSV extraction from: {csv_file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file as it would be processed\n",
    "        df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        \n",
    "        # Look for the specific row with the statistics\n",
    "        for idx, row in df.iterrows():\n",
    "            row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])\n",
    "            if \"330.000\" in row_text or \"330,000\" in row_text:\n",
    "                print(f\"   🎯 Found target row {idx}: {row_text[:200]}...\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"   ⚠️ Target statistics not found in processed content\")\n",
    "            \n",
    "        # Show first few meaningful rows\n",
    "        print(f\"\\n   First few meaningful rows:\")\n",
    "        for idx, row in df.iterrows():\n",
    "            row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])\n",
    "            if len(row_text.strip()) > 20:\n",
    "                print(f\"   Row {idx}: {row_text[:150]}...\")\n",
    "                if idx >= 3:\n",
    "                    break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading CSV: {e}\")\n",
    "\n",
    "# Run the checks\n",
    "check_current_encoding_issues()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "test_csv_content_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing: 'ausländische Straftatverdächtige 2024'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "\n",
      "🔍 Testing: '330.000 ausländische'\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "      ❌ Encoding issues present\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "\n",
      "🔍 Testing: '913.000 ausländische'\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "      ❌ Encoding issues present\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "\n",
      "🔍 Testing: 'Straftatverdächtige Deutschland 2024'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "\n",
      "🔍 Testing: 'Delikten gegen strafrechtliche Nebengesetze'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "\n",
      "📊 Checking if CSV file is indexed:\n",
      "   📄 Found: merged_statistic_id558217.csv\n",
      "\n",
      "========================================\n",
      "📝 Raw CSV content check:\n",
      "   Total lines: 39\n",
      "   🎯 Found target data at line 9:\n",
      "      ,,,, Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die häufigsten Straftaten von Ausländern; gefolgt von Rohheits- und Freiheitsdelikten mit circa 246.000. Insgesamt gab es etwa 913.000 ausländische Tatverdächtige.**\n",
      "      ✅ Raw file encoding looks correct\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "\n",
      "🔍 Testing: '330.000 ausländische'\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "      ❌ Encoding issues present\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "\n",
      "🔍 Testing: '913.000 ausländische'\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "      ❌ Encoding issues present\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "\n",
      "🔍 Testing: 'Straftatverdächtige Deutschland 2024'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "\n",
      "🔍 Testing: 'Delikten gegen strafrechtliche Nebengesetze'\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      🎯 Contains target statistics!\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      🎯 Contains target statistics!\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "\n",
      "📊 Checking if CSV file is indexed:\n",
      "   📄 Found: merged_statistic_id558217.csv\n",
      "\n",
      "========================================\n",
      "📝 Raw CSV content check:\n",
      "   Total lines: 39\n",
      "   🎯 Found target data at line 9:\n",
      "      ,,,, Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die häufigsten Straftaten von Ausländern; gefolgt von Rohheits- und Freiheitsdelikten mit circa 246.000. Insgesamt gab es etwa 913.000 ausländische Tatverdächtige.**\n",
      "      ✅ Raw file encoding looks correct\n"
     ]
    }
   ],
   "source": [
    "# Focused Test for the Specific Issue\n",
    "\n",
    "def test_specific_search():\n",
    "    \"\"\"Test search for the exact statistics mentioned in the logs\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Test searches that should find the data\n",
    "        test_queries = [\n",
    "            \"ausländische Straftatverdächtige 2024\",\n",
    "            \"330.000 ausländische\", \n",
    "            \"913.000 ausländische\",\n",
    "            \"Straftatverdächtige Deutschland 2024\",\n",
    "            \"Delikten gegen strafrechtliche Nebengesetze\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"🔍 Testing: '{query}'\")\n",
    "            results = search_client.search(query, select=[\"originalFilename\", \"title\", \"content\"], top=3)\n",
    "            \n",
    "            found_any = False\n",
    "            for result in results:\n",
    "                found_any = True\n",
    "                filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "                title = result.get(\"title\", \"No title\")\n",
    "                content = result.get(\"content\", \"\")\n",
    "                \n",
    "                print(f\"   📄 {filename}\")\n",
    "                \n",
    "                # Check for encoding issues in results\n",
    "                if \"Ã\" in title or \"Ã\" in content:\n",
    "                    print(f\"      ❌ Encoding issues present\")\n",
    "                \n",
    "                # Check if this contains our target stats\n",
    "                if any(stat in content for stat in [\"330.000\", \"330,000\", \"913.000\", \"913,000\"]):\n",
    "                    print(f\"      🎯 Contains target statistics!\")\n",
    "                    \n",
    "            if not found_any:\n",
    "                print(f\"      ⚠️ No results found\")\n",
    "            print()\n",
    "        \n",
    "        # Test if the CSV file is indexed at all\n",
    "        print(f\"📊 Checking if CSV file is indexed:\")\n",
    "        csv_results = search_client.search(\"merged_statistic_id558217\", select=[\"originalFilename\", \"title\"], top=5)\n",
    "        \n",
    "        csv_found = False\n",
    "        for result in csv_results:\n",
    "            csv_found = True\n",
    "            print(f\"   📄 Found: {result.get('originalFilename', 'Unknown')}\")\n",
    "        \n",
    "        if not csv_found:\n",
    "            print(f\"   ⚠️ CSV file not found in index - may not be processed correctly\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in search test: {e}\")\n",
    "\n",
    "def check_raw_csv_content():\n",
    "    \"\"\"Check the raw content of the CSV file to see what should be indexed\"\"\"\n",
    "    csv_file = \"./data/merged_statistic_id558217.csv\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"📝 Raw CSV content check:\")\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        print(f\"   Total lines: {len(lines)}\")\n",
    "        \n",
    "        # Find the line with our target data\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"330.000\" in line or \"Bei Delikten gegen strafrechtliche\" in line:\n",
    "                print(f\"   🎯 Found target data at line {i+1}:\")\n",
    "                print(f\"      {line.strip()}\")\n",
    "                \n",
    "                # Check for encoding issues in the raw file\n",
    "                if \"Ã\" in line:\n",
    "                    print(f\"      ❌ Raw file has encoding issues!\")\n",
    "                else:\n",
    "                    print(f\"      ✅ Raw file encoding looks correct\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"   ⚠️ Target data not found in raw file\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading raw CSV: {e}\")\n",
    "\n",
    "# Run the focused tests\n",
    "test_specific_search()\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "check_raw_csv_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing improved CSV processing:\n",
      "📊 Processing German statistics CSV: merged_statistic_id558217.csv\n",
      "   ✅ Extracted 19 description rows and 0 data rows\n",
      "📋 Extracted content:\n",
      "==================================================\n",
      "German Statistical Data from merged_statistic_id558217.csv\n",
      "\n",
      "📊 WICHTIGE STATISTIK: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die häufigsten Straftaten von Ausländern; gefolgt von Rohheits- und Freiheitsdelikten mit circa 246.000. Insgesamt gab es etwa 913.000 ausländische Tatverdächtige.**\n",
      "BESCHREIBUNG UND KONTEXT:\n",
      "Zeile 4: Anzahl der ausländischen Straftatverdächtigen in Deutschland nach Straftatengruppen im Jahr 2024\n",
      "Zeile 9: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die häufigsten Straftaten von Ausländern; gefolgt von Rohheits- und Freiheitsdelikten mit circa 246.000. Insgesamt gab es etwa 913.000 ausländische Tatverdächtige.**\n",
      "Zeile 10: Quelle Bundeskriminalamt\n",
      "Zeile 11: Erhebung durch Bundeskriminalamt\n",
      "Zeile 18: Hinweis  * Bei diesen Straftatengruppen handelt es sich um nach weiteren Kriterien erstellte Zusammenfassungen von Straftaten die teilweise auch bereits in anderen gezeigten Kategorien erfasst sind. ** Aufgrund der Zählung einzelner Straftaten in mehreren dieser Gruppen und der Mehrfachzählung eines einzelnen Tatverdächtigen bei verschiedenen von ihm begangenen Straftaten können die gezeigten Werte nicht zur Gesamtzahl der Tatverdächtigen addiert werden. \n",
      "Zeile 22: Veröffentlichung durch Bundeskriminalamt\n",
      "Zeile 24: Herkunftsverweis Polizeiliche Kriminalstatistik (PKS) 2024 - Zeitreihen - Grundtabelle Fälle\n",
      "Zeile 27: Ausländische Straftatverdächtige in Deutschland nach Straftatengruppen 2024\n",
      "Zeile 28: Anzahl der ausländischen Straftatverdächtigen in Deutschland nach Straftatengruppen im Jahr 2024\n",
      "Zeile 30: Strafrechtliche Nebengesetze (v.a. Ausländergesetze) 330 141\n",
      "\n",
      "QUELLE: Bundeskriminalamt (BKA)\n",
      "JAHR: 2024\n",
      "REGION: Deutschland\n",
      "==================================================\n",
      "✅ Key statistics found in processed content!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'German Statistical Data from merged_statistic_id558217.csv\\n\\n📊 WICHTIGE STATISTIK: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die häufigsten Straftaten von Ausländern; gefolgt von Rohheits- und Freiheitsdelikten mit circa\\xa0246.000. Insgesamt gab es etwa 913.000 ausländische Tatverdächtige.**\\nBESCHREIBUNG UND KONTEXT:\\nZeile 4: Anzahl der ausländischen Straftatverdächtigen in Deutschland nach Straftatengruppen im Jahr 2024\\nZeile 9: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die häufigsten Straftaten von Ausländern; gefolgt von Rohheits- und Freiheitsdelikten mit circa\\xa0246.000. Insgesamt gab es etwa 913.000 ausländische Tatverdächtige.**\\nZeile 10: Quelle Bundeskriminalamt\\nZeile 11: Erhebung durch Bundeskriminalamt\\nZeile 18: Hinweis  * Bei diesen Straftatengruppen handelt es sich um nach weiteren Kriterien erstellte Zusammenfassungen von Straftaten die teilweise auch bereits in anderen gezeigten Kategorien erfasst sind. ** Aufgrund der Zählung einzelner Straftaten in mehreren dieser Gruppen und der Mehrfachzählung eines einzelnen Tatverdächtigen bei verschiedenen von ihm begangenen Straftaten können die gezeigten Werte nicht zur Gesamtzahl der Tatverdächtigen addiert werden. \\nZeile 22: Veröffentlichung durch Bundeskriminalamt\\nZeile 24: Herkunftsverweis Polizeiliche Kriminalstatistik (PKS) 2024 - Zeitreihen - Grundtabelle Fälle\\nZeile 27: Ausländische Straftatverdächtige in Deutschland nach Straftatengruppen 2024\\nZeile 28: Anzahl der ausländischen Straftatverdächtigen in Deutschland nach Straftatengruppen im Jahr 2024\\nZeile 30: Strafrechtliche Nebengesetze (v.a. Ausländergesetze) 330 141\\n\\nQUELLE: Bundeskriminalamt (BKA)\\nJAHR: 2024\\nREGION: Deutschland'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improved CSV Processing for German Statistical Data\n",
    "\n",
    "def process_german_statistics_csv(file_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Specialized processor for German statistics CSV files like BKA crime statistics\n",
    "    This handles messy CSV formats with metadata rows and German text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"📊 Processing German statistics CSV: {file_path.name}\")\n",
    "        \n",
    "        # Read the raw file with proper encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        content_parts = []\n",
    "        content_parts.append(f\"German Statistical Data from {file_path.name}\")\n",
    "        content_parts.append(\"\")\n",
    "        \n",
    "        # Extract meaningful rows with German text and numbers\n",
    "        meaningful_rows = []\n",
    "        data_rows = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines and pure comma lines\n",
    "            if not line or line.replace(',', '').strip() == '':\n",
    "                continue\n",
    "                \n",
    "            # Parse CSV row\n",
    "            try:\n",
    "                row_parts = [part.strip().strip('\"') for part in line.split(',')]\n",
    "                row_text = ' '.join([part for part in row_parts if part and part != 'nan'])\n",
    "                \n",
    "                # Check if this row contains meaningful German text (more than just column headers)\n",
    "                if len(row_text) > 20:\n",
    "                    # Key statistical content - look for German keywords and numbers\n",
    "                    if any(keyword in row_text.lower() for keyword in [\n",
    "                        'straftat', 'verdächtig', 'kriminal', 'delikt', 'deutschland', \n",
    "                        'polizei', 'ermittelt', 'ausländer', 'rohheit', 'diebstahl'\n",
    "                    ]):\n",
    "                        meaningful_rows.append(f\"Zeile {i+1}: {row_text}\")\n",
    "                        \n",
    "                        # If it contains large numbers, it's likely key statistical data\n",
    "                        if any(num in row_text for num in ['000', '.000', ',000']):\n",
    "                            content_parts.append(f\"📊 WICHTIGE STATISTIK: {row_text}\")\n",
    "                    \n",
    "                    # Data rows with specific crime categories and numbers\n",
    "                    elif any(crime_type in row_text for crime_type in [\n",
    "                        'Strafrechtliche Nebengesetze', 'Rohheitsdelikte', 'Diebstahlsdelikte',\n",
    "                        'Vermögens- und Fälschungsdelikte', 'Gewaltkriminalität', 'Rauschgiftkriminalität'\n",
    "                    ]):\n",
    "                        data_rows.append(row_text)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                # If CSV parsing fails, just treat as text\n",
    "                if len(line) > 20 and any(keyword in line.lower() for keyword in [\n",
    "                    'straftat', 'verdächtig', 'deutschland', 'polizei', 'ausländer'\n",
    "                ]):\n",
    "                    meaningful_rows.append(f\"Text Zeile {i+1}: {line}\")\n",
    "        \n",
    "        # Add meaningful content to the result\n",
    "        if meaningful_rows:\n",
    "            content_parts.append(\"BESCHREIBUNG UND KONTEXT:\")\n",
    "            content_parts.extend(meaningful_rows[:10])  # Limit to avoid too much metadata\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        if data_rows:\n",
    "            content_parts.append(\"STATISTISCHE DATEN:\")\n",
    "            content_parts.extend(data_rows)\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        # Add source information\n",
    "        content_parts.append(\"QUELLE: Bundeskriminalamt (BKA)\")\n",
    "        content_parts.append(\"JAHR: 2024\")\n",
    "        content_parts.append(\"REGION: Deutschland\")\n",
    "        \n",
    "        result = '\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up any encoding issues\n",
    "        result = result.replace('Ã¤', 'ä').replace('Ã¶', 'ö').replace('Ã¼', 'ü').replace('Ã', 'ß')\n",
    "        \n",
    "        print(f\"   ✅ Extracted {len(meaningful_rows)} description rows and {len(data_rows)} data rows\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error processing German statistics CSV: {e}\")\n",
    "        return f\"Error processing German statistics file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def test_improved_csv_processing():\n",
    "    \"\"\"Test the improved CSV processing on our specific file\"\"\"\n",
    "    csv_file = Path(\"./data/merged_statistic_id558217.csv\")\n",
    "    \n",
    "    if csv_file.exists():\n",
    "        print(\"🧪 Testing improved CSV processing:\")\n",
    "        result = process_german_statistics_csv(csv_file)\n",
    "        \n",
    "        print(\"📋 Extracted content:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(result)\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Check if key information is present\n",
    "        if \"330.000\" in result and \"ausländische\" in result:\n",
    "            print(\"✅ Key statistics found in processed content!\")\n",
    "        else:\n",
    "            print(\"⚠️ Key statistics may be missing from processed content\")\n",
    "            \n",
    "        return result\n",
    "    else:\n",
    "        print(\"❌ CSV file not found for testing\")\n",
    "        return None\n",
    "\n",
    "# Test the improved processing\n",
    "test_improved_csv_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 VERIFICATION: Testing German search after encoding fix\n",
      "==================================================\n",
      "\n",
      "🔍 Testing: 'im Jahr 2024 wie viel ausländische Straftatverdächtige von der Polizei ermittelt'\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Testing: '330.000 ausländische Straftatverdächtige'\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Testing: 'BKA Statistik Deutschland 2024'\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Testing: 'ausländische Straftatverdächtige Deutschland'\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Checking for remaining encoding artifacts:\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Testing: '330.000 ausländische Straftatverdächtige'\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Testing: 'BKA Statistik Deutschland 2024'\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Testing: 'ausländische Straftatverdächtige Deutschland'\n",
      "   📄 merged_statistic_id558217.csv\n",
      "   📝 BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\n",
      "   🎯 ✅ CONTAINS TOTAL: 913.000\n",
      "   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   📄 Preview: BKA Kriminalstatistik 2024 - Ausländische Straftatverdächtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 ausländische Straftatverdächtige von der Polizei ermittelt; vor allem mit Bezug auf Ausländergesetze. Damit waren dies die h...\n",
      "\n",
      "🔍 Checking for remaining encoding artifacts:\n",
      "   ✅ No encoding artifacts found in index\n",
      "\n",
      "🎉 VERIFICATION COMPLETE!\n",
      "   Your backend should now properly find German statistical data\n",
      "   The assistant should be able to answer questions about 2024 crime statistics\n",
      "\n",
      "📊 Final Index Status:\n",
      "   📄 Total documents in index: 1\n",
      "   📊 Found: BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n",
      "   ✅ No encoding artifacts found in index\n",
      "\n",
      "🎉 VERIFICATION COMPLETE!\n",
      "   Your backend should now properly find German statistical data\n",
      "   The assistant should be able to answer questions about 2024 crime statistics\n",
      "\n",
      "📊 Final Index Status:\n",
      "   📄 Total documents in index: 1\n",
      "   📊 Found: BKA Statistik 2024: Ausländische Straftatverdächtige Deutschland\n"
     ]
    }
   ],
   "source": [
    "# Verification: Test that German search now works properly\n",
    "\n",
    "print(\"🔍 VERIFICATION: Testing German search after encoding fix\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    \n",
    "    # Test the exact question from your logs\n",
    "    test_queries = [\n",
    "        \"im Jahr 2024 wie viel ausländische Straftatverdächtige von der Polizei ermittelt\",\n",
    "        \"330.000 ausländische Straftatverdächtige\", \n",
    "        \"BKA Statistik Deutschland 2024\",\n",
    "        \"ausländische Straftatverdächtige Deutschland\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n🔍 Testing: '{query}'\")\n",
    "        \n",
    "        results = search_client.search(\n",
    "            query,\n",
    "            select=[\"title\", \"originalFilename\", \"content\"],\n",
    "            top=3,\n",
    "            query_type=\"semantic\",\n",
    "            semantic_configuration_name=\"default\"\n",
    "        )\n",
    "        \n",
    "        found_results = False\n",
    "        for result in results:\n",
    "            found_results = True\n",
    "            title = result.get(\"title\", \"No title\")\n",
    "            filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "            content = result.get(\"content\", \"\")\n",
    "            \n",
    "            print(f\"   📄 {filename}\")\n",
    "            print(f\"   📝 {title}\")\n",
    "            \n",
    "            # Check if this contains the key statistics\n",
    "            if \"330.000\" in content:\n",
    "                print(f\"   🎯 ✅ CONTAINS KEY STATISTIC: 330.000\")\n",
    "            if \"913.000\" in content:\n",
    "                print(f\"   🎯 ✅ CONTAINS TOTAL: 913.000\")\n",
    "            if \"ausländische\" in content and \"Straftatverdächtige\" in content:\n",
    "                print(f\"   🎯 ✅ CONTAINS GERMAN TERMS (no encoding artifacts)\")\n",
    "            \n",
    "            # Show a preview\n",
    "            preview = content[:300].replace('\\n', ' ')\n",
    "            print(f\"   📄 Preview: {preview}...\")\n",
    "            \n",
    "        if not found_results:\n",
    "            print(f\"   ❌ No results found\")\n",
    "    \n",
    "    # Final test: Check if there are any encoding artifacts left\n",
    "    print(f\"\\n🔍 Checking for remaining encoding artifacts:\")\n",
    "    artifact_search = search_client.search(\"Ã\", select=[\"title\", \"originalFilename\"], top=5)\n",
    "    \n",
    "    artifacts_found = False\n",
    "    for result in artifact_search:\n",
    "        artifacts_found = True\n",
    "        print(f\"   ⚠️ Still has artifacts: {result.get('originalFilename', 'Unknown')}\")\n",
    "    \n",
    "    if not artifacts_found:\n",
    "        print(f\"   ✅ No encoding artifacts found in index\")\n",
    "    \n",
    "    print(f\"\\n🎉 VERIFICATION COMPLETE!\")\n",
    "    print(f\"   Your backend should now properly find German statistical data\")\n",
    "    print(f\"   The assistant should be able to answer questions about 2024 crime statistics\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Verification error: {e}\")\n",
    "\n",
    "# Show final index status\n",
    "print(f\"\\n📊 Final Index Status:\")\n",
    "try:\n",
    "    results = search_client.search(\"*\", select=[\"id\"], include_total_count=True)\n",
    "    total_docs = results.get_count()\n",
    "    print(f\"   📄 Total documents in index: {total_docs}\")\n",
    "    \n",
    "    # Check if our specific document is there\n",
    "    bka_results = search_client.search(\"BKA\", select=[\"originalFilename\", \"title\"])\n",
    "    for result in bka_results:\n",
    "        print(f\"   📊 Found: {result.get('title', 'No title')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Status check error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Debugging file discovery in: /workspaces/chainlit-agent/data/product_info/data\n",
      "📁 Folder exists: True\n",
      "📂 Contents of data:\n",
      "   merged_statistic_id558217.csv (FILE)\n",
      "   Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (FILE)\n",
      "   Baier_2015_Migration und KriminalitÃ¤t_converted.md (FILE)\n",
      "   City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (FILE)\n",
      "   Statista_2025_Ausländerkriminalität in Deutschland.pdf (FILE)\n",
      "\n",
      "📄 Files with extension .csv:\n",
      "   data/merged_statistic_id558217.csv\n",
      "\n",
      "📄 Files with extension .pdf:\n",
      "   data/Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "\n",
      "📄 Files with extension .md:\n",
      "   data/Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "   data/Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "   data/City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "\n",
      "📊 Total supported files found: 11\n",
      "\n",
      "🧪 Testing file processing:\n",
      "   📄 merged_statistic_id558217.csv: processor = process_csv_file\n",
      "      ✅ Content length: 2037 characters\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf: processor = process_pdf_file\n",
      "      ✅ Content length: 40673 characters\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 53298 characters\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 49218 characters\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 7463 characters\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 53298 characters\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 49218 characters\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 7463 characters\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 53298 characters\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 49218 characters\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: processor = process_text_file\n",
      "      ✅ Content length: 7463 characters\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check what files are actually being found and processed\n",
    "\n",
    "def debug_file_discovery():\n",
    "    \"\"\"Debug what files are being found in the data directory\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    folder = Path(\"./data/\")\n",
    "    supported_extensions = {'.md', '.txt', '.csv', '.pdf', '.xlsx', '.xls', '.json'}\n",
    "    \n",
    "    print(f\"🔍 Debugging file discovery in: {folder.absolute()}\")\n",
    "    print(f\"📁 Folder exists: {folder.exists()}\")\n",
    "    \n",
    "    if folder.exists():\n",
    "        print(f\"📂 Contents of {folder}:\")\n",
    "        for item in folder.iterdir():\n",
    "            print(f\"   {item.name} ({'DIR' if item.is_dir() else 'FILE'})\")\n",
    "    \n",
    "    # Find all supported files\n",
    "    all_files = []\n",
    "    for ext in supported_extensions:\n",
    "        found_files = list(folder.glob(f\"**/*{ext}\"))\n",
    "        if found_files:\n",
    "            print(f\"\\n📄 Files with extension {ext}:\")\n",
    "            for f in found_files:\n",
    "                print(f\"   {f}\")\n",
    "                all_files.extend(found_files)\n",
    "    \n",
    "    print(f\"\\n📊 Total supported files found: {len(all_files)}\")\n",
    "    \n",
    "    # Test processing each file\n",
    "    print(f\"\\n🧪 Testing file processing:\")\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            processor = get_file_processor(file_path)\n",
    "            print(f\"   📄 {file_path.name}: processor = {processor.__name__}\")\n",
    "            \n",
    "            # Try to process\n",
    "            content = processor(file_path)\n",
    "            if content:\n",
    "                content_len = len(content.strip())\n",
    "                print(f\"      ✅ Content length: {content_len} characters\")\n",
    "                if content_len < 50:\n",
    "                    print(f\"      ⚠️ Content too short - would be skipped\")\n",
    "            else:\n",
    "                print(f\"      ❌ No content returned\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Error processing {file_path.name}: {e}\")\n",
    "\n",
    "debug_file_discovery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Total documents in index: 1\n",
      "📁 Files represented: 1\n",
      "\n",
      "📋 Files in index:\n",
      "   📄 merged_statistic_id558217.csv\n",
      "      Type: German Crime Statistics\n",
      "      Chunks: 1\n",
      "      Sample title: BKA Statistik 2024: Ausländische Straftatverdächtige Deutsch...\n",
      "\n",
      "🔍 Checking for missing files:\n",
      "❌ Missing files:\n",
      "   🚫 Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "   🚫 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "   🚫 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "   🚫 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   🚫 product_reviews.csv\n",
      "   🚫 product_sales.csv\n",
      "   🚫 products.csv\n"
     ]
    }
   ],
   "source": [
    "# Check what's actually in the index\n",
    "\n",
    "def check_index_contents():\n",
    "    \"\"\"Check what documents are actually in the index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get all documents\n",
    "        results = search_client.search(\"*\", \n",
    "                                     select=[\"id\", \"originalFilename\", \"documentType\", \"title\", \"totalChunks\"], \n",
    "                                     top=50)\n",
    "        \n",
    "        files_summary = {}\n",
    "        total_docs = 0\n",
    "        \n",
    "        for result in results:\n",
    "            total_docs += 1\n",
    "            filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "            doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "            title = result.get(\"title\", \"No title\")\n",
    "            total_chunks = result.get(\"totalChunks\", 1)\n",
    "            \n",
    "            if filename not in files_summary:\n",
    "                files_summary[filename] = {\n",
    "                    \"type\": doc_type,\n",
    "                    \"chunks\": total_chunks,\n",
    "                    \"sample_title\": title\n",
    "                }\n",
    "        \n",
    "        print(f\"📊 Total documents in index: {total_docs}\")\n",
    "        print(f\"📁 Files represented: {len(files_summary)}\")\n",
    "        print(f\"\\n📋 Files in index:\")\n",
    "        \n",
    "        for filename, info in sorted(files_summary.items()):\n",
    "            print(f\"   📄 {filename}\")\n",
    "            print(f\"      Type: {info['type']}\")\n",
    "            print(f\"      Chunks: {info['chunks']}\")\n",
    "            print(f\"      Sample title: {info['sample_title'][:60]}...\")\n",
    "            print()\n",
    "            \n",
    "        # Check which files from the data directory are missing\n",
    "        print(f\"🔍 Checking for missing files:\")\n",
    "        expected_files = [\n",
    "            \"merged_statistic_id558217.csv\",\n",
    "            \"Baier_2015_Migration und KriminalitÃ¤t_converted.md\",\n",
    "            \"City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\",\n",
    "            \"Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\",\n",
    "            \"Statista_2025_Ausländerkriminalität in Deutschland.pdf\",\n",
    "            \"product_reviews.csv\",\n",
    "            \"product_sales.csv\", \n",
    "            \"products.csv\"\n",
    "        ]\n",
    "        \n",
    "        indexed_files = set(files_summary.keys())\n",
    "        missing_files = []\n",
    "        \n",
    "        for expected in expected_files:\n",
    "            if expected not in indexed_files:\n",
    "                missing_files.append(expected)\n",
    "        \n",
    "        if missing_files:\n",
    "            print(f\"❌ Missing files:\")\n",
    "            for missing in missing_files:\n",
    "                print(f\"   🚫 {missing}\")\n",
    "        else:\n",
    "            print(f\"✅ All expected files are indexed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking index: {e}\")\n",
    "\n",
    "check_index_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐛 DEBUG: Starting debug of gen_multi_format_documents\n",
      "📁 Current working directory: /workspaces/chainlit-agent/data/product_info\n",
      "🎯 Target folder: ./data/\n",
      "📂 Folder absolute path: /workspaces/chainlit-agent/data/product_info/data\n",
      "📂 Folder exists: True\n",
      "\n",
      "📋 Contents of data:\n",
      "   📄 merged_statistic_id558217.csv (.csv)\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md (.md)\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf (.pdf)\n",
      "\n",
      "🔍 Looking for files with supported extensions:\n",
      "   .txt: 0 files\n",
      "   .json: 0 files\n",
      "   .csv: 1 files\n",
      "      📄 data/merged_statistic_id558217.csv\n",
      "   .pdf: 1 files\n",
      "      📄 data/Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   .xls: 0 files\n",
      "   .xlsx: 0 files\n",
      "   .md: 3 files\n",
      "      📄 data/Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      📄 data/Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "      📄 data/City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "\n",
      "📊 Total files to process: 5\n",
      "\n",
      "🧪 Testing processing of first file: merged_statistic_id558217.csv\n",
      "   📋 Processor function: process_csv_file\n",
      "   ✅ Content extracted: 2037 characters\n",
      "   ✅ Content sufficient for processing\n",
      "\n",
      "🚀 Calling actual gen_multi_format_documents...\n",
      "Found 5 supported files to process\n",
      "\n",
      "Processing: merged_statistic_id558217.csv (.csv)\n",
      "Document: Statistik als Exceldatei - Split into 1 chunks\n",
      "Processing chunk 1: 2156 chars (estimated 718 tokens)\n",
      "✓ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_Ausländerkriminalität in Deutschland.pdf (.pdf)\n",
      "Document: Statista_2025_Ausländerkriminalität in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "✓ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "   ✅ Function returned 11 documents\n",
      "   📋 Document details:\n",
      "      1. merged_statistic_id558217.csv (CSV Data Table)\n",
      "         Title: Statistik als Exceldatei...\n",
      "      2. Statista_2025_Ausländerkriminalität in Deutschland.pdf (PDF Document)\n",
      "         Title: Statista_2025_Ausländerkriminalität in Deutschla...\n",
      "      3. Statista_2025_Ausländerkriminalität in Deutschland.pdf (PDF Document)\n",
      "         Title: Statista_2025_Ausländerkriminalität in Deutschla...\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE DEBUG: Test what gen_multi_format_documents actually does\n",
    "\n",
    "def debug_gen_multi_format_documents():\n",
    "    \"\"\"Debug version of gen_multi_format_documents to see what's happening\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    folder_path = \"./data/\"\n",
    "    \n",
    "    print(f\"🐛 DEBUG: Starting debug of gen_multi_format_documents\")\n",
    "    print(f\"📁 Current working directory: {os.getcwd()}\")\n",
    "    print(f\"🎯 Target folder: {folder_path}\")\n",
    "    \n",
    "    folder = Path(folder_path)\n",
    "    print(f\"📂 Folder absolute path: {folder.absolute()}\")\n",
    "    print(f\"📂 Folder exists: {folder.exists()}\")\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"❌ Folder does not exist!\")\n",
    "        return\n",
    "    \n",
    "    # Check what's in the folder\n",
    "    print(f\"\\n📋 Contents of {folder}:\")\n",
    "    for item in folder.iterdir():\n",
    "        if item.is_file():\n",
    "            print(f\"   📄 {item.name} ({item.suffix})\")\n",
    "        else:\n",
    "            print(f\"   📁 {item.name}/\")\n",
    "    \n",
    "    # Find supported files\n",
    "    supported_extensions = {'.md', '.txt', '.csv', '.pdf', '.xlsx', '.xls', '.json'}\n",
    "    all_files = []\n",
    "    \n",
    "    print(f\"\\n🔍 Looking for files with supported extensions:\")\n",
    "    for ext in supported_extensions:\n",
    "        found_files = list(folder.glob(f\"**/*{ext}\"))\n",
    "        if found_files:\n",
    "            print(f\"   {ext}: {len(found_files)} files\")\n",
    "            for f in found_files:\n",
    "                print(f\"      📄 {f}\")\n",
    "                all_files.append(f)\n",
    "        else:\n",
    "            print(f\"   {ext}: 0 files\")\n",
    "    \n",
    "    print(f\"\\n📊 Total files to process: {len(all_files)}\")\n",
    "    \n",
    "    # Try to process just the first file to see what happens\n",
    "    if all_files:\n",
    "        test_file = all_files[0]\n",
    "        print(f\"\\n🧪 Testing processing of first file: {test_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            processor = get_file_processor(test_file)\n",
    "            print(f\"   📋 Processor function: {processor.__name__}\")\n",
    "            \n",
    "            content = processor(test_file)\n",
    "            if content:\n",
    "                print(f\"   ✅ Content extracted: {len(content)} characters\")\n",
    "                if len(content.strip()) < 50:\n",
    "                    print(f\"   ⚠️ Content too short, would be skipped\")\n",
    "                else:\n",
    "                    print(f\"   ✅ Content sufficient for processing\")\n",
    "            else:\n",
    "                print(f\"   ❌ No content extracted\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error processing: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Now let's see what happens if we call the actual function\n",
    "    print(f\"\\n🚀 Calling actual gen_multi_format_documents...\")\n",
    "    try:\n",
    "        result_docs = gen_multi_format_documents(folder_path)\n",
    "        print(f\"   ✅ Function returned {len(result_docs)} documents\")\n",
    "        \n",
    "        if result_docs:\n",
    "            print(f\"   📋 Document details:\")\n",
    "            for i, doc in enumerate(result_docs[:3]):  # Show first 3\n",
    "                print(f\"      {i+1}. {doc.get('originalFilename', 'Unknown')} ({doc.get('documentType', 'Unknown')})\")\n",
    "                print(f\"         Title: {doc.get('title', 'No title')[:50]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error in gen_multi_format_documents: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "debug_gen_multi_format_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 Checking current 'docs' variable...\n",
      "   Type: <class 'list'>\n",
      "   Length: 11\n",
      "\n",
      "📋 First few documents in 'docs':\n",
      "   1. File: merged_statistic_id558217.csv\n",
      "      Type: CSV Data Table\n",
      "      Title: Statistik als Exceldatei...\n",
      "      Content preview: Source Document: merged_statistic_id558217.csv\n",
      "Document Type: CSV Data Table\n",
      "Ori...\n",
      "\n",
      "   2. File: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Title: Statista_2025_Ausländerkriminalität in Deutschla...\n",
      "      Content preview: Source Document: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "Docume...\n",
      "\n",
      "   3. File: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Title: Statista_2025_Ausländerkriminalität in Deutschla...\n",
      "      Content preview: Source Document: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "Docume...\n",
      "\n",
      "   4. File: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Title: Statista_2025_Ausländerkriminalität in Deutschla...\n",
      "      Content preview: Source Document: Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "Docume...\n",
      "\n",
      "   5. File: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      Type: Markdown Document\n",
      "      Title: Goeckenjan_2019_FluchtalsSicherheitsproblem_conver...\n",
      "      Content preview: Source Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "Docume...\n",
      "\n",
      "📊 Document chunks by file:\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: 3 chunks\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: 1 chunks\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: 3 chunks\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf: 3 chunks\n",
      "   📄 merged_statistic_id558217.csv: 1 chunks\n"
     ]
    }
   ],
   "source": [
    "# Check what's currently in the 'docs' variable\n",
    "\n",
    "print(f\"🧐 Checking current 'docs' variable...\")\n",
    "print(f\"   Type: {type(docs)}\")\n",
    "print(f\"   Length: {len(docs)}\")\n",
    "\n",
    "if docs:\n",
    "    print(f\"\\n📋 First few documents in 'docs':\")\n",
    "    for i, doc in enumerate(docs[:5]):\n",
    "        print(f\"   {i+1}. File: {doc.get('originalFilename', 'Unknown')}\")\n",
    "        print(f\"      Type: {doc.get('documentType', 'Unknown')}\")\n",
    "        print(f\"      Title: {doc.get('title', 'No title')[:50]}...\")\n",
    "        print(f\"      Content preview: {doc.get('content', 'No content')[:80]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Check file distribution\n",
    "    file_counts = {}\n",
    "    for doc in docs:\n",
    "        filename = doc.get('originalFilename', 'Unknown')\n",
    "        file_counts[filename] = file_counts.get(filename, 0) + 1\n",
    "    \n",
    "    print(f\"📊 Document chunks by file:\")\n",
    "    for filename, count in sorted(file_counts.items()):\n",
    "        print(f\"   📄 {filename}: {count} chunks\")\n",
    "else:\n",
    "    print(\"   ❌ 'docs' variable is empty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing upload of current documents...\n",
      "📊 Current docs count: 11\n",
      "\n",
      "🔄 Attempting to upload 11 documents to index...\n",
      "   📤 Uploading batch 1: 5 documents\n",
      "   ✅ Batch 1 uploaded successfully\n",
      "      Files: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md, merged_statistic_id558217.csv, Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "   📤 Uploading batch 2: 5 documents\n",
      "   ✅ Batch 2 uploaded successfully\n",
      "      Files: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md, Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "   📤 Uploading batch 3: 1 documents\n",
      "   ✅ Batch 3 uploaded successfully\n",
      "      Files: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "\n",
      "📊 Upload test results:\n",
      "   ✅ Successful batches: 3\n",
      "   ❌ Failed batches: 0\n",
      "\n",
      "📋 After upload test - Index contains:\n",
      "   📊 Total documents: 6\n",
      "   📁 Unique files: 3\n",
      "      📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      📄 merged_statistic_id558217.csv\n"
     ]
    }
   ],
   "source": [
    "# TEST UPLOAD: Try to upload the docs that are currently in memory\n",
    "\n",
    "print(f\"🧪 Testing upload of current documents...\")\n",
    "print(f\"📊 Current docs count: {len(docs)}\")\n",
    "\n",
    "if len(docs) > 0:\n",
    "    print(f\"\\n🔄 Attempting to upload {len(docs)} documents to index...\")\n",
    "    \n",
    "    # Create search client  \n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    \n",
    "    # Upload in small batches for testing\n",
    "    batch_size = 5\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        try:\n",
    "            print(f\"   📤 Uploading batch {i//batch_size + 1}: {len(batch)} documents\")\n",
    "            result = search_client.upload_documents(batch)\n",
    "            successful_batches += 1\n",
    "            print(f\"   ✅ Batch {i//batch_size + 1} uploaded successfully\")\n",
    "            \n",
    "            # Show which files were in this batch\n",
    "            batch_files = [doc.get('originalFilename', 'Unknown') for doc in batch]\n",
    "            print(f\"      Files: {', '.join(set(batch_files))}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_batches += 1\n",
    "            print(f\"   ❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "            \n",
    "            # Show details of the failed batch\n",
    "            print(f\"      Batch contents:\")\n",
    "            for j, doc in enumerate(batch):\n",
    "                print(f\"        {j+1}. {doc.get('originalFilename', 'Unknown')} - {doc.get('title', 'No title')[:40]}...\")\n",
    "    \n",
    "    print(f\"\\n📊 Upload test results:\")\n",
    "    print(f\"   ✅ Successful batches: {successful_batches}\")\n",
    "    print(f\"   ❌ Failed batches: {failed_batches}\")\n",
    "    \n",
    "    # Check index after upload\n",
    "    results = search_client.search(\"*\", select=[\"originalFilename\"], top=20)\n",
    "    indexed_files = set()\n",
    "    total_in_index = 0\n",
    "    for result in results:\n",
    "        total_in_index += 1\n",
    "        if result.get(\"originalFilename\"):\n",
    "            indexed_files.add(result[\"originalFilename\"])\n",
    "    \n",
    "    print(f\"\\n📋 After upload test - Index contains:\")\n",
    "    print(f\"   📊 Total documents: {total_in_index}\")\n",
    "    print(f\"   📁 Unique files: {len(indexed_files)}\")\n",
    "    for filename in sorted(indexed_files):\n",
    "        print(f\"      📄 {filename}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No documents to upload!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking document IDs in index...\n",
      "📊 Total documents in index: 12\n",
      "\n",
      "📋 Documents by file:\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md:\n",
      "      Expected chunks: 3\n",
      "      Found chunks: 3 -> [1, 2, 3]\n",
      "   📄 City of Munich_2017_Flugblatt_Kriminalitaet_converted.md:\n",
      "      Expected chunks: 1\n",
      "      Found chunks: 1 -> [1]\n",
      "   📄 Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md:\n",
      "      Expected chunks: 3\n",
      "      Found chunks: 3 -> [1, 2, 3]\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf:\n",
      "      Expected chunks: 3\n",
      "      Found chunks: 3 -> [1, 2, 3]\n",
      "   📄 merged_statistic_id558217.csv:\n",
      "      Expected chunks: 1\n",
      "      Found chunks: 2 -> [1, 1]\n",
      "\n",
      "✅ No duplicate IDs found\n",
      "\n",
      "🧐 Checking original 'docs' variable for ID patterns:\n",
      "✅ No duplicate IDs in memory docs\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check document IDs for duplicates\n",
    "\n",
    "print(f\"🔍 Checking document IDs in index...\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "# Get all documents with their IDs and filenames\n",
    "results = search_client.search(\"*\", select=[\"id\", \"originalFilename\", \"chunkIndex\", \"totalChunks\"], top=50)\n",
    "\n",
    "documents = []\n",
    "id_counts = {}\n",
    "file_chunks = {}\n",
    "\n",
    "for result in results:\n",
    "    doc_id = result.get(\"id\", \"No ID\")\n",
    "    filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "    chunk_idx = result.get(\"chunkIndex\", 0)\n",
    "    total_chunks = result.get(\"totalChunks\", 1)\n",
    "    \n",
    "    documents.append({\n",
    "        \"id\": doc_id,\n",
    "        \"filename\": filename,\n",
    "        \"chunk\": chunk_idx,\n",
    "        \"total\": total_chunks\n",
    "    })\n",
    "    \n",
    "    # Count ID frequency\n",
    "    id_counts[doc_id] = id_counts.get(doc_id, 0) + 1\n",
    "    \n",
    "    # Track file chunks\n",
    "    if filename not in file_chunks:\n",
    "        file_chunks[filename] = {\"expected\": total_chunks, \"found\": []}\n",
    "    file_chunks[filename][\"found\"].append(chunk_idx)\n",
    "\n",
    "print(f\"📊 Total documents in index: {len(documents)}\")\n",
    "print(f\"\\n📋 Documents by file:\")\n",
    "for filename, info in sorted(file_chunks.items()):\n",
    "    expected = info[\"expected\"]\n",
    "    found = sorted(info[\"found\"])\n",
    "    print(f\"   📄 {filename}:\")\n",
    "    print(f\"      Expected chunks: {expected}\")\n",
    "    print(f\"      Found chunks: {len(found)} -> {found}\")\n",
    "    if len(found) != expected:\n",
    "        missing = [i for i in range(1, expected + 1) if i not in found]\n",
    "        if missing:\n",
    "            print(f\"      ❌ Missing chunks: {missing}\")\n",
    "\n",
    "# Check for duplicate IDs\n",
    "duplicate_ids = {doc_id: count for doc_id, count in id_counts.items() if count > 1}\n",
    "if duplicate_ids:\n",
    "    print(f\"\\n❌ Found duplicate IDs:\")\n",
    "    for doc_id, count in duplicate_ids.items():\n",
    "        print(f\"   🔄 ID '{doc_id}' appears {count} times\")\n",
    "        \n",
    "    # Show which documents have the same ID\n",
    "    for doc_id in duplicate_ids:\n",
    "        matching_docs = [doc for doc in documents if doc[\"id\"] == doc_id]\n",
    "        print(f\"\\n   📋 Documents with ID '{doc_id}':\")\n",
    "        for doc in matching_docs:\n",
    "            print(f\"      📄 {doc['filename']} (chunk {doc['chunk']}/{doc['total']})\")\n",
    "else:\n",
    "    print(f\"\\n✅ No duplicate IDs found\")\n",
    "\n",
    "# Check the original docs variable for ID patterns\n",
    "print(f\"\\n🧐 Checking original 'docs' variable for ID patterns:\")\n",
    "doc_ids_in_memory = [doc.get(\"id\", \"No ID\") for doc in docs]\n",
    "memory_id_counts = {}\n",
    "for doc_id in doc_ids_in_memory:\n",
    "    memory_id_counts[doc_id] = memory_id_counts.get(doc_id, 0) + 1\n",
    "\n",
    "duplicate_memory_ids = {doc_id: count for doc_id, count in memory_id_counts.items() if count > 1}\n",
    "if duplicate_memory_ids:\n",
    "    print(f\"❌ Found duplicate IDs in memory:\")\n",
    "    for doc_id, count in duplicate_memory_ids.items():\n",
    "        print(f\"   🔄 ID '{doc_id}' appears {count} times in docs variable\")\n",
    "        \n",
    "    # Show which documents in memory have the same ID\n",
    "    for doc_id in duplicate_memory_ids:\n",
    "        matching_docs = [(i, doc) for i, doc in enumerate(docs) if doc.get(\"id\") == doc_id]\n",
    "        print(f\"\\n   📋 Documents in memory with ID '{doc_id}':\")\n",
    "        for i, doc in matching_docs:\n",
    "            print(f\"      {i}. {doc.get('originalFilename', 'Unknown')} (chunk {doc.get('chunkIndex', '?')}/{doc.get('totalChunks', '?')})\")\n",
    "else:\n",
    "    print(f\"✅ No duplicate IDs in memory docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning index and re-running complete pipeline...\n",
      "🗑️ Deleting existing index...\n",
      "✅ Index deleted successfully\n",
      "🏗️ Creating fresh index...\n",
      "✅ Fresh index created successfully\n",
      "📂 Processing documents...\n",
      "Found 5 supported files to process\n",
      "\n",
      "Processing: merged_statistic_id558217.csv (.csv)\n",
      "Document: Statistik als Exceldatei - Split into 1 chunks\n",
      "Processing chunk 1: 2156 chars (estimated 718 tokens)\n",
      "✓ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_Ausländerkriminalität in Deutschland.pdf (.pdf)\n",
      "Document: Statista_2025_Ausländerkriminalität in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "✓ Successfully processed: Statista_2025_Ausländerkriminalität in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "✓ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "✓ Successfully processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "✓ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "📊 Processed 11 document chunks\n",
      "📤 Uploading documents to index...\n",
      "✅ Uploaded batch 1: 11 documents\n",
      "\n",
      "📊 Upload completed:\n",
      "   ✅ Successful batches: 1\n",
      "\n",
      "🔍 Verifying index contents...\n",
      "📊 Final index contains 0 documents from 0 files:\n",
      "\n",
      "✅ Pipeline completed successfully!\n",
      "🎉 All 0 files have been indexed with 0 total chunks\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Clean index and re-run the complete pipeline\n",
    "\n",
    "print(\"🧹 Cleaning index and re-running complete pipeline...\")\n",
    "\n",
    "# Step 1: Delete existing index\n",
    "search_index_client = SearchIndexClient(\n",
    "    search_endpoint, DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"🗑️ Deleting existing index...\")\n",
    "    search_index_client.delete_index(index_name)\n",
    "    print(\"✅ Index deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"ℹ️ Index deletion result: {e}\")\n",
    "\n",
    "# Step 2: Create fresh index\n",
    "print(\"🏗️ Creating fresh index...\")\n",
    "index = create_index_definition(index_name)\n",
    "search_index_client.create_or_update_index(index)\n",
    "print(\"✅ Fresh index created successfully\")\n",
    "\n",
    "# Step 3: Process documents\n",
    "print(\"📂 Processing documents...\")\n",
    "docs = gen_multi_format_documents(\"./data/\")\n",
    "print(f\"📊 Processed {len(docs)} document chunks\")\n",
    "\n",
    "# Step 4: Upload documents\n",
    "if len(docs) > 0:\n",
    "    print(\"📤 Uploading documents to index...\")\n",
    "    \n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    \n",
    "    # Upload in batches\n",
    "    batch_size = 50\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        try:\n",
    "            result = search_client.upload_documents(batch)\n",
    "            successful_batches += 1\n",
    "            print(f\"✅ Uploaded batch {i//batch_size + 1}: {len(batch)} documents\")\n",
    "        except Exception as e:\n",
    "            failed_batches += 1\n",
    "            print(f\"❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "    \n",
    "    print(f\"\\n📊 Upload completed:\")\n",
    "    print(f\"   ✅ Successful batches: {successful_batches}\")\n",
    "    if failed_batches > 0:\n",
    "        print(f\"   ❌ Failed batches: {failed_batches}\")\n",
    "    \n",
    "    # Step 5: Verify the result\n",
    "    print(\"\\n🔍 Verifying index contents...\")\n",
    "    results = search_client.search(\"*\", select=[\"originalFilename\", \"documentType\"], top=50)\n",
    "    \n",
    "    files_summary = {}\n",
    "    total_docs = 0\n",
    "    \n",
    "    for result in results:\n",
    "        total_docs += 1\n",
    "        filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "        doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "        \n",
    "        if filename not in files_summary:\n",
    "            files_summary[filename] = {\"type\": doc_type, \"count\": 0}\n",
    "        files_summary[filename][\"count\"] += 1\n",
    "    \n",
    "    print(f\"📊 Final index contains {total_docs} documents from {len(files_summary)} files:\")\n",
    "    for filename, info in sorted(files_summary.items()):\n",
    "        print(f\"   📄 {filename}: {info['count']} chunks ({info['type']})\")\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline completed successfully!\")\n",
    "    print(f\"🎉 All {len(files_summary)} files have been indexed with {total_docs} total chunks\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No documents were processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Final verification of the indexed content...\n",
      "📊 INDEX SUMMARY:\n",
      "   Total documents indexed: 11\n",
      "   Unique files: 5\n",
      "\n",
      "📋 DETAILED FILE BREAKDOWN:\n",
      "   ✅ Baier_2015_Migration und KriminalitÃ¤t_converted.md\n",
      "      Type: Markdown Document\n",
      "      Expected chunks: 3\n",
      "      Indexed chunks: 3\n",
      "\n",
      "   ✅ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "      Type: Markdown Document\n",
      "      Expected chunks: 1\n",
      "      Indexed chunks: 1\n",
      "\n",
      "   ✅ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      Type: Markdown Document\n",
      "      Expected chunks: 3\n",
      "      Indexed chunks: 3\n",
      "\n",
      "   ✅ Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Expected chunks: 3\n",
      "      Indexed chunks: 3\n",
      "\n",
      "   ✅ merged_statistic_id558217.csv\n",
      "      Type: CSV Data Table\n",
      "      Expected chunks: 1\n",
      "      Indexed chunks: 1\n",
      "\n",
      "❌ MISSING FILES:\n",
      "   🚫 Statista_2025_Ausländerkriminalität in Deutschland.pdf\n",
      "\n",
      "🧪 TESTING GERMAN SEARCH:\n",
      "   📄 Statista_2025_Ausländerkriminalität in Deutschland.pdf: Statista_2025_Ausländerkriminalität in Deutschland (Part 1...\n",
      "   📄 Baier_2015_Migration und KriminalitÃ¤t_converted.md: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1)...\n",
      "   📄 merged_statistic_id558217.csv: Statistik als Exceldatei...\n",
      "\n",
      "============================================================\n",
      "⚠️ ISSUES DETECTED:\n",
      "   - 1 expected files missing\n",
      "   - Some files have incorrect chunk counts\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL VERIFICATION: Confirm all files are properly indexed\n",
    "\n",
    "print(\"🔍 Final verification of the indexed content...\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "# Get comprehensive stats\n",
    "results = search_client.search(\"*\", select=[\"originalFilename\", \"documentType\", \"chunkIndex\", \"totalChunks\"], top=50)\n",
    "\n",
    "file_details = {}\n",
    "total_documents = 0\n",
    "\n",
    "for result in results:\n",
    "    total_documents += 1\n",
    "    filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "    doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "    chunk_idx = result.get(\"chunkIndex\", 1)\n",
    "    total_chunks = result.get(\"totalChunks\", 1)\n",
    "    \n",
    "    if filename not in file_details:\n",
    "        file_details[filename] = {\n",
    "            \"type\": doc_type,\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"found_chunks\": [],\n",
    "            \"chunks_indexed\": 0\n",
    "        }\n",
    "    \n",
    "    file_details[filename][\"found_chunks\"].append(chunk_idx)\n",
    "    file_details[filename][\"chunks_indexed\"] += 1\n",
    "\n",
    "print(f\"📊 INDEX SUMMARY:\")\n",
    "print(f\"   Total documents indexed: {total_documents}\")\n",
    "print(f\"   Unique files: {len(file_details)}\")\n",
    "print()\n",
    "\n",
    "print(f\"📋 DETAILED FILE BREAKDOWN:\")\n",
    "all_files_complete = True\n",
    "\n",
    "for filename, details in sorted(file_details.items()):\n",
    "    expected = details[\"total_chunks\"]\n",
    "    found = len(details[\"found_chunks\"])\n",
    "    chunks_found = sorted(details[\"found_chunks\"])\n",
    "    \n",
    "    status = \"✅\" if found == expected else \"⚠️\"\n",
    "    if found != expected:\n",
    "        all_files_complete = False\n",
    "    \n",
    "    print(f\"   {status} {filename}\")\n",
    "    print(f\"      Type: {details['type']}\")\n",
    "    print(f\"      Expected chunks: {expected}\")\n",
    "    print(f\"      Indexed chunks: {found}\")\n",
    "    \n",
    "    if found != expected:\n",
    "        expected_chunks = list(range(1, expected + 1))\n",
    "        missing = [i for i in expected_chunks if i not in chunks_found]\n",
    "        duplicates = [i for i in chunks_found if chunks_found.count(i) > 1]\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"      ❌ Missing chunks: {missing}\")\n",
    "        if duplicates:\n",
    "            print(f\"      🔄 Duplicate chunks: {duplicates}\")\n",
    "    print()\n",
    "\n",
    "# Expected files check\n",
    "expected_files = [\n",
    "    \"merged_statistic_id558217.csv\",\n",
    "    \"Baier_2015_Migration und KriminalitÃ¤t_converted.md\", \n",
    "    \"City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\",\n",
    "    \"Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\",\n",
    "    \"Statista_2025_Ausländerkriminalität in Deutschland.pdf\"\n",
    "]\n",
    "\n",
    "indexed_files = set(file_details.keys())\n",
    "missing_files = [f for f in expected_files if f not in indexed_files]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"❌ MISSING FILES:\")\n",
    "    for missing in missing_files:\n",
    "        print(f\"   🚫 {missing}\")\n",
    "    all_files_complete = False\n",
    "else:\n",
    "    print(f\"✅ ALL EXPECTED FILES ARE INDEXED\")\n",
    "\n",
    "# Test German search\n",
    "print(f\"\\n🧪 TESTING GERMAN SEARCH:\")\n",
    "german_results = search_client.search(\"ausländische Straftatverdächtige\", top=3)\n",
    "german_found = False\n",
    "for result in german_results:\n",
    "    german_found = True\n",
    "    filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "    title = result.get(\"title\", \"No title\")\n",
    "    print(f\"   📄 {filename}: {title[:60]}...\")\n",
    "\n",
    "if not german_found:\n",
    "    print(f\"   ⚠️ No results found for German search term\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\n{'='*60}\")\n",
    "if all_files_complete and not missing_files and german_found:\n",
    "    print(f\"🎉 SUCCESS: All files properly indexed and searchable!\")\n",
    "    print(f\"✅ {len(file_details)} files indexed with {total_documents} total chunks\")\n",
    "    print(f\"✅ German search functionality working\")\n",
    "else:\n",
    "    print(f\"⚠️ ISSUES DETECTED:\")\n",
    "    if missing_files:\n",
    "        print(f\"   - {len(missing_files)} expected files missing\")\n",
    "    if not all_files_complete:\n",
    "        print(f\"   - Some files have incorrect chunk counts\")\n",
    "    if not german_found:\n",
    "        print(f\"   - German search not working\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
