{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Format Document Search Index Pipeline\n",
    "\n",
    "This notebook creates a comprehensive document search index from multiple file types, **always starting fresh** for testing and development. Every run will delete and recreate the index to ensure the schema is always up to date.\n",
    "\n",
    "## Supported File Types\n",
    "\n",
    "- **ðŸ“ Markdown files (.md)** - With smart title extraction from headings\n",
    "- **ðŸ“„ Text files (.txt)** - Plain text processing  \n",
    "- **ðŸ“Š CSV files (.csv)** - Intelligent table parsing with special handling for statistics files\n",
    "- **ðŸ“‘ PDF files (.pdf)** - Text extraction from all pages\n",
    "- **ðŸ“ˆ Excel files (.xlsx/.xls)** - Multi-sheet processing with column structure\n",
    "- **âš™ï¸ JSON files (.json)** - Structured data formatting\n",
    "\n",
    "## Pipeline Flow\n",
    "\n",
    "1. **ðŸ—‘ï¸ Delete existing index** - Ensures clean state\n",
    "2. **ðŸ—ï¸ Create fresh index** - With latest schema including metadata fields\n",
    "3. **ðŸ“ Process documents** - Smart content extraction and chunking\n",
    "4. **ðŸ“¤ Upload to index** - Batch processing with error handling\n",
    "5. **ðŸ“Š Show statistics** - Summary of indexed content\n",
    "\n",
    "## Requirements\n",
    "\n",
    "You'll need credentials for the following Azure services:\n",
    "\n",
    "- **Azure Search Service** - For the vector search index\n",
    "- **Azure OpenAI Service** - For text embeddings\n",
    "\n",
    "Find the names and keys in the Azure Portal and add them to a `.env` file in the root of this repository. The `.env` file is not checked in to source control. You can use the [`.env.sample`](../../.env.sample) file as a template.\n",
    "\n",
    "## Fresh Index Benefits\n",
    "\n",
    "This \"always fresh\" approach is perfect for:\n",
    "- **Testing new features** - No schema conflicts\n",
    "- **Iterative development** - Clean state every run  \n",
    "- **Content updates** - All changes are captured\n",
    "- **Schema evolution** - New fields work immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SemanticSearch,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters\n",
    ")\n",
    "from typing import List, Dict\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import mimetypes\n",
    "\n",
    "# Additional imports for document processing\n",
    "try:\n",
    "    import PyPDF2\n",
    "    PDF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PDF_AVAILABLE = False\n",
    "    print(\"Warning: PyPDF2 not available. Install with: pip install PyPDF2\")\n",
    "\n",
    "try:\n",
    "    import openpyxl\n",
    "    EXCEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    EXCEL_AVAILABLE = False\n",
    "    print(\"Warning: openpyxl not available. Install with: pip install openpyxl\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index(search_index_client: SearchIndexClient, search_index: str):\n",
    "    print(f\"deleting index {search_index}\")\n",
    "    search_index_client.delete_index(search_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document processing functions for different file types with proper encoding handling\n",
    "\n",
    "def detect_encoding(file_path: Path) -> str:\n",
    "    \"\"\"Detect the encoding of a text file, with preference for UTF-8 and German-specific encodings\"\"\"\n",
    "    try:\n",
    "        # Try common encodings for German text\n",
    "        encodings_to_try = ['utf-8', 'utf-8-sig', 'iso-8859-1', 'cp1252', 'latin1']\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    # Read a sample to test encoding\n",
    "                    sample = f.read(1024)\n",
    "                    # Check if it contains common German characters properly\n",
    "                    if any(char in sample for char in ['Ã¤', 'Ã¶', 'Ã¼', 'ÃŸ', 'Ã„', 'Ã–', 'Ãœ']):\n",
    "                        return encoding\n",
    "                    elif encoding == 'utf-8' and not any(char in sample for char in ['Ãƒ']):\n",
    "                        # UTF-8 without encoding artifacts\n",
    "                        return encoding\n",
    "            except (UnicodeDecodeError, UnicodeError):\n",
    "                continue\n",
    "        \n",
    "        # Default to utf-8 if nothing works\n",
    "        return 'utf-8'\n",
    "    except Exception:\n",
    "        return 'utf-8'\n",
    "\n",
    "def clean_german_text(text: str) -> str:\n",
    "    \"\"\"Clean up common German encoding issues\"\"\"\n",
    "    # Fix common UTF-8 encoding artifacts for German umlauts\n",
    "    replacements = {\n",
    "        'ÃƒÂ¤': 'Ã¤',\n",
    "        'ÃƒÂ¶': 'Ã¶', \n",
    "        'ÃƒÂ¼': 'Ã¼',\n",
    "        'Ãƒ': 'ÃŸ',\n",
    "        'Ãƒâ€ž': 'Ã„',\n",
    "        'Ãƒâ€“': 'Ã–',\n",
    "        'ÃƒÅ“': 'Ãœ',\n",
    "        'Ã¢â‚¬Å“': '\"',\n",
    "        'Ã¢â‚¬': '\"',\n",
    "        'Ã¢â‚¬â„¢': \"'\",\n",
    "        'Ã¢â‚¬\"': 'â€“',\n",
    "        'Ã¢â‚¬\"': 'â€”'\n",
    "    }\n",
    "    \n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_csv_file(file_path: Path) -> str:\n",
    "    \"\"\"Process CSV files with intelligent content extraction and proper encoding, with special handling for German statistical data\"\"\"\n",
    "    try:\n",
    "        # Detect encoding for better German text handling\n",
    "        encoding = detect_encoding(file_path)\n",
    "        \n",
    "        # Special handling for German BKA statistics files\n",
    "        if \"statistic\" in file_path.name.lower() or \"bka\" in file_path.name.lower():\n",
    "            return process_german_statistics_csv_integrated(file_path, encoding)\n",
    "        \n",
    "        # Try to read with pandas using detected encoding\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        \n",
    "        # Check if this looks like a statistics table (like your example)\n",
    "        if df.columns[0].startswith('Unnamed'):\n",
    "            # This looks like a messy CSV with metadata, try to clean it\n",
    "            df = pd.read_csv(file_path, encoding=encoding, header=None)\n",
    "            \n",
    "            # Extract meaningful content\n",
    "            content_parts = []\n",
    "            \n",
    "            # Look for title or description rows\n",
    "            for idx, row in df.iterrows():\n",
    "                row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])\n",
    "                if len(row_text.strip()) > 10:  # Skip empty or very short rows\n",
    "                    content_parts.append(row_text.strip())\n",
    "            \n",
    "            # Join the meaningful parts\n",
    "            content = '\\n'.join(content_parts)\n",
    "            \n",
    "        else:\n",
    "            # Regular CSV - convert to readable format\n",
    "            content_parts = [f\"CSV Data from {file_path.name}\"]\n",
    "            content_parts.append(f\"Columns: {', '.join(df.columns)}\")\n",
    "            content_parts.append(\"\")\n",
    "            \n",
    "            # Add first few rows as examples\n",
    "            for idx, row in df.head(10).iterrows():\n",
    "                row_data = []\n",
    "                for col in df.columns:\n",
    "                    if pd.notna(row[col]):\n",
    "                        row_data.append(f\"{col}: {row[col]}\")\n",
    "                if row_data:\n",
    "                    content_parts.append(\" | \".join(row_data))\n",
    "            \n",
    "            if len(df) > 10:\n",
    "                content_parts.append(f\"... and {len(df) - 10} more rows\")\n",
    "            \n",
    "            content = '\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up any encoding artifacts\n",
    "        content = clean_german_text(content)\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing CSV file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_german_statistics_csv_integrated(file_path: Path, encoding: str = 'utf-8') -> str:\n",
    "    \"\"\"\n",
    "    Specialized processor for German statistics CSV files like BKA crime statistics\n",
    "    Integrated into the normal multi-file processing workflow\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   ðŸ“Š Using specialized German statistics processing\")\n",
    "        \n",
    "        # Read the raw file with proper encoding\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        content_parts = []\n",
    "        content_parts.append(f\"German Statistical Data from {file_path.name}\")\n",
    "        content_parts.append(\"\")\n",
    "        \n",
    "        # Extract meaningful rows with German text and numbers\n",
    "        meaningful_rows = []\n",
    "        data_rows = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines and pure comma lines\n",
    "            if not line or line.replace(',', '').strip() == '':\n",
    "                continue\n",
    "                \n",
    "            # Parse CSV row\n",
    "            try:\n",
    "                row_parts = [part.strip().strip('\"') for part in line.split(',')]\n",
    "                row_text = ' '.join([part for part in row_parts if part and part != 'nan'])\n",
    "                \n",
    "                # Check if this row contains meaningful German text (more than just column headers)\n",
    "                if len(row_text) > 20:\n",
    "                    # Key statistical content - look for German keywords and numbers\n",
    "                    if any(keyword in row_text.lower() for keyword in [\n",
    "                        'straftat', 'verdÃ¤chtig', 'kriminal', 'delikt', 'deutschland', \n",
    "                        'polizei', 'ermittelt', 'auslÃ¤nder', 'rohheit', 'diebstahl'\n",
    "                    ]):\n",
    "                        meaningful_rows.append(f\"Zeile {i+1}: {row_text}\")\n",
    "                        \n",
    "                        # If it contains large numbers, it's likely key statistical data\n",
    "                        if any(num in row_text for num in ['000', '.000', ',000']):\n",
    "                            content_parts.append(f\"ðŸ“Š WICHTIGE STATISTIK: {row_text}\")\n",
    "                    \n",
    "                    # Data rows with specific crime categories and numbers\n",
    "                    elif any(crime_type in row_text for crime_type in [\n",
    "                        'Strafrechtliche Nebengesetze', 'Rohheitsdelikte', 'Diebstahlsdelikte',\n",
    "                        'VermÃ¶gens- und FÃ¤lschungsdelikte', 'GewaltkriminalitÃ¤t', 'RauschgiftkriminalitÃ¤t'\n",
    "                    ]):\n",
    "                        data_rows.append(row_text)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                # If CSV parsing fails, just treat as text\n",
    "                if len(line) > 20 and any(keyword in line.lower() for keyword in [\n",
    "                    'straftat', 'verdÃ¤chtig', 'deutschland', 'polizei', 'auslÃ¤nder'\n",
    "                ]):\n",
    "                    meaningful_rows.append(f\"Text Zeile {i+1}: {line}\")\n",
    "        \n",
    "        # Add meaningful content to the result\n",
    "        if meaningful_rows:\n",
    "            content_parts.append(\"BESCHREIBUNG UND KONTEXT:\")\n",
    "            content_parts.extend(meaningful_rows[:10])  # Limit to avoid too much metadata\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        if data_rows:\n",
    "            content_parts.append(\"STATISTISCHE DATEN:\")\n",
    "            content_parts.extend(data_rows)\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        # Add source information\n",
    "        content_parts.append(\"QUELLE: Bundeskriminalamt (BKA)\")\n",
    "        content_parts.append(\"JAHR: 2024\")\n",
    "        content_parts.append(\"REGION: Deutschland\")\n",
    "        \n",
    "        result = '\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up any encoding issues\n",
    "        result = clean_german_text(result)\n",
    "        \n",
    "        print(f\"   âœ… German statistics: {len(meaningful_rows)} description rows, {len(data_rows)} data rows\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error in German statistics processing: {e}\")\n",
    "        return f\"Error processing German statistics file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_pdf_file(file_path: Path) -> str:\n",
    "    \"\"\"Process PDF files to extract text content with proper encoding\"\"\"\n",
    "    if not PDF_AVAILABLE:\n",
    "        return f\"PDF processing not available for {file_path.name}. Install PyPDF2.\"\n",
    "    \n",
    "    try:\n",
    "        content_parts = []\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                text = page.extract_text()\n",
    "                if text.strip():\n",
    "                    # Clean German encoding issues\n",
    "                    text = clean_german_text(text)\n",
    "                    content_parts.append(f\"Page {page_num}:\\n{text.strip()}\")\n",
    "        \n",
    "        return '\\n\\n'.join(content_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing PDF file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_excel_file(file_path: Path) -> str:\n",
    "    \"\"\"Process Excel files with multiple sheets and proper encoding\"\"\"\n",
    "    if not EXCEL_AVAILABLE:\n",
    "        return f\"Excel processing not available for {file_path.name}. Install openpyxl.\"\n",
    "    \n",
    "    try:\n",
    "        content_parts = [f\"Excel Data from {file_path.name}\"]\n",
    "        \n",
    "        # Read all sheets\n",
    "        xl_file = pd.ExcelFile(file_path)\n",
    "        \n",
    "        for sheet_name in xl_file.sheet_names:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            \n",
    "            content_parts.append(f\"\\nSheet: {sheet_name}\")\n",
    "            content_parts.append(f\"Columns: {', '.join(df.columns)}\")\n",
    "            \n",
    "            # Add first few rows\n",
    "            for idx, row in df.head(5).iterrows():\n",
    "                row_data = []\n",
    "                for col in df.columns:\n",
    "                    if pd.notna(row[col]):\n",
    "                        row_data.append(f\"{col}: {row[col]}\")\n",
    "                if row_data:\n",
    "                    content_parts.append(\" | \".join(row_data))\n",
    "            \n",
    "            if len(df) > 5:\n",
    "                content_parts.append(f\"... and {len(df) - 5} more rows\")\n",
    "        \n",
    "        content = '\\n'.join(content_parts)\n",
    "        # Clean German encoding issues\n",
    "        content = clean_german_text(content)\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing Excel file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_text_file(file_path: Path) -> str:\n",
    "    \"\"\"Process plain text files with proper encoding detection\"\"\"\n",
    "    try:\n",
    "        # Detect best encoding for the file\n",
    "        encoding = detect_encoding(file_path)\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Clean up any encoding artifacts\n",
    "        content = clean_german_text(content)\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback encodings\n",
    "        encodings_to_try = ['utf-8', 'iso-8859-1', 'cp1252', 'latin1']\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    content = f.read()\n",
    "                content = clean_german_text(content)\n",
    "                return content\n",
    "            except (UnicodeDecodeError, UnicodeError):\n",
    "                continue\n",
    "        \n",
    "        return f\"Error reading text file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def process_json_file(file_path: Path) -> str:\n",
    "    \"\"\"Process JSON files with proper encoding\"\"\"\n",
    "    try:\n",
    "        # Detect encoding first\n",
    "        encoding = detect_encoding(file_path)\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert JSON to readable format\n",
    "        content_parts = [f\"JSON Data from {file_path.name}\"]\n",
    "        json_content = json.dumps(data, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Clean up any encoding artifacts\n",
    "        json_content = clean_german_text(json_content)\n",
    "        content_parts.append(json_content)\n",
    "        \n",
    "        return '\\n'.join(content_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing JSON file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def get_file_processor(file_path: Path):\n",
    "    \"\"\"Get the appropriate processor function for a file type\"\"\"\n",
    "    suffix = file_path.suffix.lower()\n",
    "    \n",
    "    processors = {\n",
    "        '.md': process_text_file,\n",
    "        '.txt': process_text_file,\n",
    "        '.csv': process_csv_file,\n",
    "        '.pdf': process_pdf_file,\n",
    "        '.xlsx': process_excel_file,\n",
    "        '.xls': process_excel_file,\n",
    "        '.json': process_json_file,\n",
    "    }\n",
    "    \n",
    "    return processors.get(suffix, process_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_definition(name: str) -> SearchIndex:\n",
    "    \"\"\"\n",
    "    Returns an Azure Cognitive Search index with the given name.\n",
    "    \"\"\"\n",
    "    # The fields we want to index. The \"embedding\" field is a vector field that will\n",
    "    # be used for vector search.\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"filepath\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
    "        # Additional metadata fields for better referencing\n",
    "        SimpleField(name=\"originalFilename\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"originalTitle\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"documentStem\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"documentType\", type=SearchFieldDataType.String),  # New: file type\n",
    "        SimpleField(name=\"fileExtension\", type=SearchFieldDataType.String),  # New: file extension\n",
    "        SimpleField(name=\"chunkIndex\", type=SearchFieldDataType.Int32),\n",
    "        SimpleField(name=\"totalChunks\", type=SearchFieldDataType.Int32),\n",
    "        SimpleField(name=\"isChunked\", type=SearchFieldDataType.Boolean),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # Size of the vector created by the text-embedding-3-large model.\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"titleVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # Size of the vector created by the text-embedding-3-large model.\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "        ),    \n",
    "    ]\n",
    "\n",
    "    # The \"content\" field should be prioritized for semantic ranking.\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"default\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            keywords_fields=[],\n",
    "            content_fields=[SemanticField(field_name=\"content\")],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # For vector search, we want to use the HNSW (Hierarchical Navigable Small World)\n",
    "    # algorithm (a type of approximate nearest neighbor search algorithm) with cosine\n",
    "    # distance.\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            ),\n",
    "            ExhaustiveKnnAlgorithmConfiguration(\n",
    "                name=\"myExhaustiveKnn\",\n",
    "                kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "                parameters=ExhaustiveKnnParameters(\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "                vectorizer_name=\"myvectorizer\"\n",
    "            ),\n",
    "            VectorSearchProfile(\n",
    "                name=\"myExhaustiveKnnProfile\",\n",
    "                algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "            ),\n",
    "        ],\n",
    "        vectorizers=[  \n",
    "            AzureOpenAIVectorizer(  \n",
    "                vectorizer_name=\"myvectorizer\",  \n",
    "                kind=\"azureOpenAI\",  \n",
    "                parameters=AzureOpenAIVectorizerParameters(  \n",
    "                    resource_url=os.environ[\"AZURE_OPENAI_ENDPOINT\"],  \n",
    "                    deployment_name=os.environ[\"AZURE_EMBEDDING_NAME\"],\n",
    "                    model_name=os.environ[\"AZURE_EMBEDDING_NAME\"],\n",
    "                    # Todo: there is some issue with managed identity for AI search\n",
    "                    # authIdentity=\"/subscriptions/db4948d5-b90c-47d4-91b5-d8c4c43493d2/resourcegroups/rg-chainlit-agent/providers/Microsoft.ManagedIdentity/userAssignedIdentities/id-7r5g6is3dx73u\"\n",
    "                ),\n",
    "            ),  \n",
    "        ],  \n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index.\n",
    "    index = SearchIndex(\n",
    "        name=name,\n",
    "        fields=fields,\n",
    "        semantic_search=semantic_search,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_tokens: int = 6500) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks that fit within the token limit.\n",
    "    Using a more conservative estimate of ~3 characters per token.\n",
    "    \"\"\"\n",
    "    # More conservative estimate: 1 token â‰ˆ 3 characters (down from 4)\n",
    "    # And using 6000 tokens max instead of 7000 for extra safety\n",
    "    max_chars = max_tokens * 3\n",
    "    \n",
    "    # Split by paragraphs first (double newlines)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If adding this paragraph would exceed the limit, start a new chunk\n",
    "        if len(current_chunk) + len(paragraph) + 2 > max_chars and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + paragraph\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    # Handle edge case where a single paragraph is too long\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) <= max_chars:\n",
    "            final_chunks.append(chunk)\n",
    "        else:\n",
    "            # Split by sentences if paragraph is too long\n",
    "            sentences = chunk.split('. ')\n",
    "            temp_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                if len(temp_chunk) + len(sentence) + 2 > max_chars and temp_chunk:\n",
    "                    final_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = sentence\n",
    "                else:\n",
    "                    if temp_chunk:\n",
    "                        temp_chunk += \". \" + sentence\n",
    "                    else:\n",
    "                        temp_chunk = sentence\n",
    "            if temp_chunk:\n",
    "                final_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    # Final safety check - if any chunk is still too large, split it by words\n",
    "    safe_chunks = []\n",
    "    for chunk in final_chunks:\n",
    "        if len(chunk) <= max_chars:\n",
    "            safe_chunks.append(chunk)\n",
    "        else:\n",
    "            # Emergency word-level splitting\n",
    "            words = chunk.split()\n",
    "            temp_chunk = \"\"\n",
    "            for word in words:\n",
    "                if len(temp_chunk) + len(word) + 1 > max_chars and temp_chunk:\n",
    "                    safe_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = word\n",
    "                else:\n",
    "                    if temp_chunk:\n",
    "                        temp_chunk += \" \" + word\n",
    "                    else:\n",
    "                        temp_chunk = word\n",
    "            if temp_chunk:\n",
    "                safe_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    return safe_chunks\n",
    "\n",
    "def gen_multi_format_documents(\n",
    "    folder_path: str,\n",
    ") -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Process documents of multiple formats (MD, CSV, PDF, Excel, etc.)\n",
    "    \"\"\"\n",
    "    openai_service_endoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    openai_deployment = os.environ[\"AZURE_EMBEDDING_NAME\"]\n",
    "\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")    \n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2023-07-01-preview\",\n",
    "        azure_endpoint=openai_service_endoint,\n",
    "        azure_deployment=openai_deployment,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "    items = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Supported file extensions\n",
    "    supported_extensions = {'.md', '.txt', '.csv', '.pdf', '.xlsx', '.xls', '.json'}\n",
    "    \n",
    "    # Find all supported files in the folder and subfolders\n",
    "    all_files = []\n",
    "    for ext in supported_extensions:\n",
    "        all_files.extend(folder.glob(f\"**/*{ext}\"))\n",
    "    \n",
    "    print(f\"Found {len(all_files)} supported files to process\")\n",
    "    \n",
    "    doc_id = 1\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            print(f\"\\nProcessing: {file_path.name} ({file_path.suffix})\")\n",
    "            \n",
    "            # Get the appropriate processor for this file type\n",
    "            processor = get_file_processor(file_path)\n",
    "            content = processor(file_path)\n",
    "            \n",
    "            if not content or len(content.strip()) < 50:\n",
    "                print(f\"âš ï¸  Skipping {file_path.name} - insufficient content\")\n",
    "                continue\n",
    "            \n",
    "            # Determine document type and title\n",
    "            file_extension = file_path.suffix.lower()\n",
    "            document_type = {\n",
    "                '.md': 'Markdown Document',\n",
    "                '.txt': 'Text Document', \n",
    "                '.csv': 'CSV Data Table',\n",
    "                '.pdf': 'PDF Document',\n",
    "                '.xlsx': 'Excel Spreadsheet',\n",
    "                '.xls': 'Excel Spreadsheet',\n",
    "                '.json': 'JSON Data'\n",
    "            }.get(file_extension, 'Unknown Document')\n",
    "            \n",
    "            # Extract title based on file type\n",
    "            if file_extension == '.md':\n",
    "                # Extract title from first heading or use filename\n",
    "                title = file_path.stem\n",
    "                lines = content.split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.startswith('# '):\n",
    "                        title = line[2:].strip()\n",
    "                        break\n",
    "            elif file_extension == '.csv':\n",
    "                # For CSV, try to extract a meaningful title from content\n",
    "                title = file_path.stem\n",
    "                # Look for title in first few lines\n",
    "                lines = content.split('\\n')[:5]\n",
    "                for line in lines:\n",
    "                    if len(line.strip()) > 10 and not line.startswith('Unnamed'):\n",
    "                        title = line.strip()[:100]  # Limit title length\n",
    "                        break\n",
    "            else:\n",
    "                title = file_path.stem\n",
    "            \n",
    "            # Preserve document metadata for referencing\n",
    "            original_filename = file_path.name\n",
    "            document_path = str(file_path.relative_to(folder))\n",
    "            document_stem = file_path.stem\n",
    "            \n",
    "            # Split content into chunks\n",
    "            content_chunks = chunk_text(content)\n",
    "            \n",
    "            print(f\"Document: {title} - Split into {len(content_chunks)} chunks\")\n",
    "            \n",
    "            # Process each chunk as a separate document\n",
    "            for chunk_idx, chunk in enumerate(content_chunks):\n",
    "                # Create unique ID for each chunk\n",
    "                chunk_id = f\"{doc_id}_{chunk_idx + 1}\" if len(content_chunks) > 1 else str(doc_id)\n",
    "                \n",
    "                # Preserve original title for reference, add chunk info only for display\n",
    "                display_title = title\n",
    "                if len(content_chunks) > 1:\n",
    "                    display_title = f\"{title} (Part {chunk_idx + 1})\"\n",
    "                \n",
    "                # Enhanced content with document reference information\n",
    "                enhanced_content = f\"Source Document: {original_filename}\\n\"\n",
    "                enhanced_content += f\"Document Type: {document_type}\\n\"\n",
    "                if len(content_chunks) > 1:\n",
    "                    enhanced_content += f\"Document Section: Part {chunk_idx + 1} of {len(content_chunks)}\\n\"\n",
    "                enhanced_content += f\"Original Title: {title}\\n\\n{chunk}\"\n",
    "                \n",
    "                url = f\"/docs/{document_path.replace(file_extension, '').replace('/', '-').lower()}\"\n",
    "                if len(content_chunks) > 1:\n",
    "                    url += f\"-part-{chunk_idx + 1}\"\n",
    "                \n",
    "                # Generate embeddings for enhanced content and title\n",
    "                try:\n",
    "                    print(f\"Processing chunk {chunk_idx + 1}: {len(enhanced_content)} chars (estimated {len(enhanced_content)//3} tokens)\")\n",
    "                    \n",
    "                    content_emb = client.embeddings.create(input=enhanced_content, model=openai_deployment)\n",
    "                    title_emb = client.embeddings.create(input=display_title, model=openai_deployment)\n",
    "                    \n",
    "                    rec = {\n",
    "                        \"id\": chunk_id,\n",
    "                        \"content\": enhanced_content,  # Contains source reference\n",
    "                        \"filepath\": document_path,     # Original relative path\n",
    "                        \"title\": display_title,       # Display title with chunk info\n",
    "                        \"url\": url,\n",
    "                        \"contentVector\": content_emb.data[0].embedding,\n",
    "                        \"titleVector\": title_emb.data[0].embedding,\n",
    "                        # Additional metadata for better referencing\n",
    "                        \"originalFilename\": original_filename,\n",
    "                        \"originalTitle\": title,\n",
    "                        \"documentStem\": document_stem,\n",
    "                        \"documentType\": document_type,\n",
    "                        \"fileExtension\": file_extension,\n",
    "                        \"chunkIndex\": chunk_idx + 1,\n",
    "                        \"totalChunks\": len(content_chunks),\n",
    "                        \"isChunked\": len(content_chunks) > 1\n",
    "                    }\n",
    "                    items.append(rec)\n",
    "                    print(f\"âœ“ Successfully processed: {display_title}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— Error processing chunk {chunk_idx + 1} of {title}: {e}\")\n",
    "                    print(f\"  Enhanced content length: {len(enhanced_content)} characters (estimated {len(enhanced_content)//3} tokens)\")\n",
    "                    print(f\"  Original chunk length: {len(chunk)} characters\")\n",
    "                    continue\n",
    "            \n",
    "            doc_id += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error processing file {file_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return items\n",
    "\n",
    "# Keep the old function for backward compatibility\n",
    "def gen_markdown_documents(folder_path: str) -> List[Dict[str, any]]:\n",
    "    \"\"\"Legacy function - use gen_multi_format_documents instead\"\"\"\n",
    "    return gen_multi_format_documents(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Starting fresh index creation pipeline...\n",
      "Index: parssedindexer1\n",
      "Endpoint: https://srch-cqbrigubozxty.search.windows.net/\n",
      "ðŸ—‘ï¸ Deleting existing index 'parssedindexer1'...\n",
      "deleting index parssedindexer1\n",
      "ðŸ—‘ï¸ Deleting existing index 'parssedindexer1'...\n",
      "deleting index parssedindexer1\n",
      "âœ… Index deleted successfully\n",
      "ðŸ—ï¸ Creating new index 'parssedindexer1' with latest schema...\n",
      "âœ… Index deleted successfully\n",
      "ðŸ—ï¸ Creating new index 'parssedindexer1' with latest schema...\n",
      "âœ… Fresh index 'parssedindexer1' created successfully!\n",
      "ðŸ“ Schema includes all metadata fields: documentType, fileExtension, etc.\n",
      "âœ… Fresh index 'parssedindexer1' created successfully!\n",
      "ðŸ“ Schema includes all metadata fields: documentType, fileExtension, etc.\n"
     ]
    }
   ],
   "source": [
    "search_endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "\n",
    "print(f\"ðŸ”„ Starting fresh index creation pipeline...\")\n",
    "print(f\"Index: {index_name}\")\n",
    "print(f\"Endpoint: {search_endpoint}\")\n",
    "\n",
    "search_index_client = SearchIndexClient(\n",
    "    search_endpoint, DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Always delete existing index for fresh start (testing mode)\n",
    "try:\n",
    "    existing_index = search_index_client.get_index(index_name)\n",
    "    print(f\"ðŸ—‘ï¸ Deleting existing index '{index_name}'...\")\n",
    "    delete_index(search_index_client, index_name)\n",
    "    print(f\"âœ… Index deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ Index '{index_name}' did not exist (this is fine)\")\n",
    "\n",
    "# Create fresh index with latest schema\n",
    "print(f\"ðŸ—ï¸ Creating new index '{index_name}' with latest schema...\")\n",
    "index = create_index_definition(index_name)\n",
    "search_index_client.create_or_update_index(index)\n",
    "print(f\"âœ… Fresh index '{index_name}' created successfully!\")\n",
    "print(f\"ðŸ“ Schema includes all metadata fields: documentType, fileExtension, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents for indexing...\n",
      "ðŸ” Supported file types: .md, .txt, .csv, .pdf, .xlsx, .xls, .json\n",
      "ðŸ“‚ Scanning folder: ./data/\n",
      "Found 5 supported files to process\n",
      "\n",
      "Processing: merged_statistic_id558217.csv (.csv)\n",
      "Document: Statistik als Exceldatei - Split into 1 chunks\n",
      "Processing chunk 1: 2156 chars (estimated 718 tokens)\n",
      "âœ“ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (.pdf)\n",
      "âœ“ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (.pdf)\n",
      "Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "âœ“ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "\n",
      "ðŸ“¤ Uploading 11 document chunks to fresh index...\n",
      "âœ“ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "\n",
      "ðŸ“¤ Uploading 11 document chunks to fresh index...\n",
      "âœ… Uploaded batch 1: 11 documents\n",
      "\n",
      "ðŸŽ‰ Document indexing completed!\n",
      "   ðŸ“Š Total chunks processed: 11\n",
      "   âœ… Successful batches: 1\n",
      "\n",
      "ðŸ“‹ Documents processed by type:\n",
      "   ðŸ“„ CSV Data Table: 1 chunks\n",
      "   ðŸ“„ Markdown Document: 7 chunks\n",
      "   ðŸ“„ PDF Document: 3 chunks\n",
      "âœ… Uploaded batch 1: 11 documents\n",
      "\n",
      "ðŸŽ‰ Document indexing completed!\n",
      "   ðŸ“Š Total chunks processed: 11\n",
      "   âœ… Successful batches: 1\n",
      "\n",
      "ðŸ“‹ Documents processed by type:\n",
      "   ðŸ“„ CSV Data Table: 1 chunks\n",
      "   ðŸ“„ Markdown Document: 7 chunks\n",
      "   ðŸ“„ PDF Document: 3 chunks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing documents for indexing...\")\n",
    "\n",
    "# Configuration for document processing\n",
    "OVERWRITE_EXISTING = False  # Set to True to overwrite existing documents\n",
    "CHECK_FOR_DUPLICATES = True  # Set to True to avoid processing already indexed files\n",
    "\n",
    "# Change this path to point to your folder containing documents\n",
    "# Now supports: .md, .txt, .csv, .pdf, .xlsx, .xls, .json files\n",
    "document_folder = \"./data/\"  # Update this path as needed\n",
    "\n",
    "# Create search client for document operations\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "# Process all documents using the multi-format function\n",
    "print(f\"ðŸ” Supported file types: .md, .txt, .csv, .pdf, .xlsx, .xls, .json\")\n",
    "print(f\"ðŸ“‚ Scanning folder: {document_folder}\")\n",
    "\n",
    "docs = gen_multi_format_documents(document_folder)\n",
    "\n",
    "if len(docs) == 0:\n",
    "    print(\"âš ï¸ No documents found to process\")\n",
    "    print(f\"   Make sure your documents are in: {document_folder}\")\n",
    "    print(f\"   Supported extensions: .md, .txt, .csv, .pdf, .xlsx, .xls, .json\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“¤ Uploading {len(docs)} document chunks to fresh index...\")\n",
    "    \n",
    "    # Upload in batches to handle large document sets\n",
    "    batch_size = 50\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        try:\n",
    "            result = search_client.upload_documents(batch)\n",
    "            successful_batches += 1\n",
    "            print(f\"âœ… Uploaded batch {i//batch_size + 1}: {len(batch)} documents\")\n",
    "        except Exception as e:\n",
    "            failed_batches += 1\n",
    "            print(f\"âŒ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Document indexing completed!\")\n",
    "    print(f\"   ðŸ“Š Total chunks processed: {len(docs)}\")\n",
    "    print(f\"   âœ… Successful batches: {successful_batches}\")\n",
    "    if failed_batches > 0:\n",
    "        print(f\"   âŒ Failed batches: {failed_batches}\")\n",
    "    \n",
    "    # Show summary by document type\n",
    "    type_summary = {}\n",
    "    for doc in docs:\n",
    "        doc_type = doc.get(\"documentType\", \"Unknown\")\n",
    "        type_summary[doc_type] = type_summary.get(doc_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Documents processed by type:\")\n",
    "    for doc_type, count in sorted(type_summary.items()):\n",
    "        print(f\"   ðŸ“„ {doc_type}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Fresh Index Ready!\n",
      "ðŸ“Š Fresh Index Statistics:\n",
      "   ðŸ“„ Total documents/chunks: 0\n",
      "   ðŸ“ Unique source files: 0\n",
      "   ðŸ“ˆ Average chunks per file: 0.0\n",
      "\n",
      "ðŸ” Checking for encoding issues...\n",
      "ðŸ“Š Fresh Index Statistics:\n",
      "   ðŸ“„ Total documents/chunks: 0\n",
      "   ðŸ“ Unique source files: 0\n",
      "   ðŸ“ˆ Average chunks per file: 0.0\n",
      "\n",
      "ðŸ” Checking for encoding issues...\n",
      "ðŸš¨ Found 3 documents with encoding issues:\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: ÃƒÂ¤, Ãƒ\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: ÃƒÂ¤, Ãƒ\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: ÃƒÂ¤, Ãƒ\n",
      "\n",
      "ðŸ’¡ Recommendation: Run the fresh index pipeline to fix encoding issues\n",
      "\n",
      "ðŸ“ Files in index:\n",
      "ðŸš¨ Found 3 documents with encoding issues:\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: ÃƒÂ¤, Ãƒ\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: ÃƒÂ¤, Ãƒ\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: ÃƒÂ¤, Ãƒ\n",
      "\n",
      "ðŸ’¡ Recommendation: Run the fresh index pipeline to fix encoding issues\n",
      "\n",
      "ðŸ“ Files in index:\n",
      "ðŸ“ Indexed Files:\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (Markdown Document, 3 chunks)\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (Markdown Document, 1 chunks)\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (Markdown Document, 3 chunks)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (PDF Document, 3 chunks)\n",
      "   ðŸ“„ merged_statistic_id558217.csv (CSV Data Table, 1 chunks)\n",
      "\n",
      "ðŸ‡©ðŸ‡ª Testing German search:\n",
      "ðŸ‡©ðŸ‡ª Testing German search terms:\n",
      "\n",
      "ðŸ” Searching for: 'KriminalitÃ¤t'\n",
      "ðŸ“ Indexed Files:\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (Markdown Document, 3 chunks)\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (Markdown Document, 1 chunks)\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (Markdown Document, 3 chunks)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (PDF Document, 3 chunks)\n",
      "   ðŸ“„ merged_statistic_id558217.csv (CSV Data Table, 1 chunks)\n",
      "\n",
      "ðŸ‡©ðŸ‡ª Testing German search:\n",
      "ðŸ‡©ðŸ‡ª Testing German search terms:\n",
      "\n",
      "ðŸ” Searching for: 'KriminalitÃ¤t'\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1) (from Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md)\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 3) (from Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md)\n",
      "\n",
      "ðŸ” Searching for: 'AuslÃ¤nder'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "\n",
      "ðŸ” Searching for: 'StraftÃ¤ter'\n",
      "   âš ï¸ No results found - possible encoding issues\n",
      "\n",
      "ðŸ” Searching for: 'Deutschland'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 3) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1) (from Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md)\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 3) (from Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md)\n",
      "\n",
      "ðŸ” Searching for: 'AuslÃ¤nder'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted (from City of Munich_2017_Flugblatt_Kriminalitaet_converted.md)\n",
      "\n",
      "ðŸ” Searching for: 'StraftÃ¤ter'\n",
      "   âš ï¸ No results found - possible encoding issues\n",
      "\n",
      "ðŸ” Searching for: 'Deutschland'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 3) (from Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf)\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for index management\n",
    "\n",
    "def clean_german_text_simple(text: str) -> str:\n",
    "    \"\"\"Clean up common German encoding issues\"\"\"\n",
    "    # Fix common UTF-8 encoding artifacts for German umlauts\n",
    "    replacements = {\n",
    "        'ÃƒÂ¤': 'Ã¤',\n",
    "        'ÃƒÂ¶': 'Ã¶', \n",
    "        'ÃƒÂ¼': 'Ã¼',\n",
    "        'Ãƒ': 'ÃŸ',\n",
    "        'Ãƒâ€ž': 'Ã„',\n",
    "        'Ãƒâ€“': 'Ã–',\n",
    "        'ÃƒÅ“': 'Ãœ',\n",
    "        'Ã¢â‚¬Å“': '\"',\n",
    "        'Ã¢â‚¬': '\"',\n",
    "        'Ã¢â‚¬â„¢': \"'\",\n",
    "        'Ã¢â‚¬\"': 'â€“',\n",
    "        'Ã¢â‚¬\"': 'â€”'\n",
    "    }\n",
    "    \n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def check_encoding_issues():\n",
    "    \"\"\"Check if there are encoding issues in the current index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get a few documents to check for encoding issues\n",
    "        results = search_client.search(\"*\", select=[\"id\", \"title\", \"content\", \"originalFilename\"], top=10)\n",
    "        \n",
    "        encoding_issues = []\n",
    "        for result in results:\n",
    "            title = result.get(\"title\", \"\")\n",
    "            content = result.get(\"content\", \"\")\n",
    "            filename = result.get(\"originalFilename\", \"\")\n",
    "            \n",
    "            # Check for common encoding artifacts\n",
    "            if any(artifact in title + content for artifact in ['ÃƒÂ¤', 'ÃƒÂ¶', 'ÃƒÂ¼', 'Ãƒ', 'Ã¢â‚¬']):\n",
    "                encoding_issues.append({\n",
    "                    \"id\": result[\"id\"],\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": title,\n",
    "                    \"issues\": [artifact for artifact in ['ÃƒÂ¤', 'ÃƒÂ¶', 'ÃƒÂ¼', 'Ãƒ'] if artifact in title + content]\n",
    "                })\n",
    "        \n",
    "        if encoding_issues:\n",
    "            print(f\"ðŸš¨ Found {len(encoding_issues)} documents with encoding issues:\")\n",
    "            for issue in encoding_issues[:5]:  # Show first 5\n",
    "                print(f\"   ðŸ“„ {issue['filename']}: {', '.join(issue['issues'])}\")\n",
    "            if len(encoding_issues) > 5:\n",
    "                print(f\"   ... and {len(encoding_issues) - 5} more\")\n",
    "            print(f\"\\nðŸ’¡ Recommendation: Run the fresh index pipeline to fix encoding issues\")\n",
    "        else:\n",
    "            print(f\"âœ… No encoding issues detected in current index\")\n",
    "            \n",
    "        return len(encoding_issues)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking encoding: {e}\")\n",
    "        return -1\n",
    "\n",
    "def get_index_stats():\n",
    "    \"\"\"Get statistics about the current fresh index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get total document count\n",
    "        results = search_client.search(\"*\", select=[\"id\"], include_total_count=True)\n",
    "        total_docs = results.get_count()\n",
    "        \n",
    "        # Get document type breakdown (schema is fresh, so these fields exist)\n",
    "        file_results = search_client.search(\"*\", select=[\"originalFilename\", \"documentType\"])\n",
    "        \n",
    "        unique_files = set()\n",
    "        doc_types = {}\n",
    "        \n",
    "        for result in file_results:\n",
    "            if result.get(\"originalFilename\"):\n",
    "                unique_files.add(result[\"originalFilename\"])\n",
    "            if result.get(\"documentType\"):\n",
    "                doc_type = result[\"documentType\"]\n",
    "                doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "        \n",
    "        print(f\"ðŸ“Š Fresh Index Statistics:\")\n",
    "        print(f\"   ðŸ“„ Total documents/chunks: {total_docs}\")\n",
    "        print(f\"   ðŸ“ Unique source files: {len(unique_files)}\")\n",
    "        print(f\"   ðŸ“ˆ Average chunks per file: {total_docs / len(unique_files) if unique_files else 0:.1f}\")\n",
    "        \n",
    "        if doc_types:\n",
    "            print(f\"\\nðŸ“‹ Document Types:\")\n",
    "            for doc_type, count in sorted(doc_types.items()):\n",
    "                print(f\"   ðŸ“„ {doc_type}: {count} chunks\")\n",
    "        \n",
    "        # Check for encoding issues\n",
    "        print(f\"\\nðŸ” Checking for encoding issues...\")\n",
    "        check_encoding_issues()\n",
    "        \n",
    "        return total_docs, len(unique_files)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting index stats: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def delete_documents_by_filename(filename: str):\n",
    "    \"\"\"Delete all chunks from a specific source file\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Find all documents from this file\n",
    "        results = search_client.search(f'originalFilename:\"{filename}\"', select=[\"id\"])\n",
    "        doc_ids = [result[\"id\"] for result in results]\n",
    "        \n",
    "        if doc_ids:\n",
    "            # Delete the documents\n",
    "            delete_docs = [{\"@search.action\": \"delete\", \"id\": doc_id} for doc_id in doc_ids]\n",
    "            search_client.upload_documents(delete_docs)\n",
    "            print(f\"ðŸ—‘ï¸ Deleted {len(doc_ids)} chunks from file '{filename}'\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ No documents found for file '{filename}'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error deleting documents: {e}\")\n",
    "\n",
    "def delete_documents_by_type(document_type: str):\n",
    "    \"\"\"Delete all documents of a specific type (e.g., 'CSV Data Table')\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        results = search_client.search(f'documentType:\"{document_type}\"', select=[\"id\"])\n",
    "        doc_ids = [result[\"id\"] for result in results]\n",
    "        \n",
    "        if doc_ids:\n",
    "            # Delete the documents\n",
    "            delete_docs = [{\"@search.action\": \"delete\", \"id\": doc_id} for doc_id in doc_ids]\n",
    "            search_client.upload_documents(delete_docs)\n",
    "            print(f\"ðŸ—‘ï¸ Deleted {len(doc_ids)} chunks of type '{document_type}'\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ No documents found for type '{document_type}'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error deleting documents: {e}\")\n",
    "\n",
    "def list_indexed_files():\n",
    "    \"\"\"List all files that have been indexed with their types\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        results = search_client.search(\"*\", \n",
    "                                     select=[\"originalFilename\", \"documentType\", \"totalChunks\", \"fileExtension\"])\n",
    "        \n",
    "        files_info = {}\n",
    "        for result in results:\n",
    "            filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "            doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "            chunks = result.get(\"totalChunks\", 1)\n",
    "            extension = result.get(\"fileExtension\", \"\")\n",
    "            \n",
    "            if filename not in files_info:\n",
    "                files_info[filename] = {\n",
    "                    \"type\": doc_type,\n",
    "                    \"chunks\": chunks,\n",
    "                    \"extension\": extension\n",
    "                }\n",
    "        \n",
    "        print(\"ðŸ“ Indexed Files:\")\n",
    "        for filename, info in sorted(files_info.items()):\n",
    "            print(f\"   ðŸ“„ {filename} ({info['type']}, {info['chunks']} chunks)\")\n",
    "            \n",
    "        return files_info\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error listing files: {e}\")\n",
    "        return {}\n",
    "\n",
    "def search_by_file_type(file_type: str, query: str = \"*\", top: int = 5):\n",
    "    \"\"\"Search within specific file types\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Use documentType filter (schema is fresh, so this field exists)\n",
    "        filter_expr = f'documentType eq \"{file_type}\"'\n",
    "        \n",
    "        results = search_client.search(\n",
    "            query, \n",
    "            filter=filter_expr,\n",
    "            select=[\"title\", \"originalFilename\", \"documentType\", \"content\"],\n",
    "            top=top\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ” Search results for '{query}' in {file_type} files:\")\n",
    "        for result in results:\n",
    "            print(f\"   ðŸ“„ {result['title']} (from {result['originalFilename']})\")\n",
    "            print(f\"      Preview: {result['content'][:200]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error searching: {e}\")\n",
    "\n",
    "def test_vector_search(query: str = \"product data\", top: int = 3):\n",
    "    \"\"\"Test vector search functionality with the fresh index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ” Testing vector search for: '{query}'\")\n",
    "        \n",
    "        # Perform hybrid search (text + vector)\n",
    "        results = search_client.search(\n",
    "            query,\n",
    "            select=[\"title\", \"originalFilename\", \"documentType\", \"content\"],\n",
    "            top=top,\n",
    "            query_type=\"semantic\",\n",
    "            semantic_configuration_name=\"default\"\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ“‹ Top {top} results:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"   {i}. {result['title']} ({result['documentType']})\")\n",
    "            print(f\"      File: {result['originalFilename']}\")\n",
    "            print(f\"      Preview: {result['content'][:150]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in vector search: {e}\")\n",
    "\n",
    "def test_german_search():\n",
    "    \"\"\"Test search with German terms to check for encoding issues\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Test with German terms that might have encoding issues\n",
    "        test_queries = [\n",
    "            \"KriminalitÃ¤t\",\n",
    "            \"AuslÃ¤nder\", \n",
    "            \"StraftÃ¤ter\",\n",
    "            \"Deutschland\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"ðŸ‡©ðŸ‡ª Testing German search terms:\")\n",
    "        for query in test_queries:\n",
    "            print(f\"\\nðŸ” Searching for: '{query}'\")\n",
    "            results = search_client.search(\n",
    "                query,\n",
    "                select=[\"title\", \"originalFilename\", \"content\"],\n",
    "                top=3\n",
    "            )\n",
    "            \n",
    "            result_count = 0\n",
    "            for result in results:\n",
    "                result_count += 1\n",
    "                title = result['title']\n",
    "                filename = result['originalFilename']\n",
    "                print(f\"   ðŸ“„ {title} (from {filename})\")\n",
    "            \n",
    "            if result_count == 0:\n",
    "                print(f\"   âš ï¸ No results found - possible encoding issues\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in German search test: {e}\")\n",
    "\n",
    "# Show current statistics for the fresh index\n",
    "print(\"ðŸŽ¯ Fresh Index Ready!\")\n",
    "get_index_stats()\n",
    "print(\"\\nðŸ“ Files in index:\")\n",
    "list_indexed_files()\n",
    "print(\"\\nðŸ‡©ðŸ‡ª Testing German search:\")\n",
    "test_german_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing fresh index with sample searches...\n",
      "\n",
      "1ï¸âƒ£ Testing general content search:\n",
      "ðŸ” Testing vector search for: 'data analysis'\n",
      "ðŸ“‹ Top 3 results:\n",
      "2ï¸âƒ£ Only one document type found, skipping type-specific search\n",
      "\n",
      "3ï¸âƒ£ Testing semantic search:\n",
      "   ðŸ“Š Semantic search results:\n",
      "\n",
      "âœ… Fresh index testing completed!\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ FRESH INDEX PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "âœ… Index deleted and recreated with latest schema\n",
      "âœ… All documents processed and uploaded\n",
      "âœ… Utility functions ready for use\n",
      "âœ… Sample searches tested\n",
      "\n",
      "ðŸ’¡ Next steps:\n",
      "   â€¢ Use the utility functions above to manage your index\n",
      "   â€¢ Run search_by_file_type() to search specific document types\n",
      "   â€¢ Use delete_documents_by_filename() to remove specific files\n",
      "   â€¢ The index is now ready for your application!\n",
      "============================================================\n",
      "2ï¸âƒ£ Only one document type found, skipping type-specific search\n",
      "\n",
      "3ï¸âƒ£ Testing semantic search:\n",
      "   ðŸ“Š Semantic search results:\n",
      "\n",
      "âœ… Fresh index testing completed!\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ FRESH INDEX PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "âœ… Index deleted and recreated with latest schema\n",
      "âœ… All documents processed and uploaded\n",
      "âœ… Utility functions ready for use\n",
      "âœ… Sample searches tested\n",
      "\n",
      "ðŸ’¡ Next steps:\n",
      "   â€¢ Use the utility functions above to manage your index\n",
      "   â€¢ Run search_by_file_type() to search specific document types\n",
      "   â€¢ Use delete_documents_by_filename() to remove specific files\n",
      "   â€¢ The index is now ready for your application!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Index Schema Update Helper\n",
    "\n",
    "# Step 4: Test the Fresh Index with Sample Searches\n",
    "\n",
    "def perform_test_searches():\n",
    "    \"\"\"Run sample searches to test the fresh index functionality\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing fresh index with sample searches...\\n\")\n",
    "    \n",
    "    # Test 1: General content search\n",
    "    print(\"1ï¸âƒ£ Testing general content search:\")\n",
    "    test_vector_search(\"data analysis\", top=3)\n",
    "    \n",
    "    # Test 2: Search by document type if we have multiple types\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get available document types\n",
    "        type_results = search_client.search(\"*\", select=[\"documentType\"], top=100)\n",
    "        doc_types = set()\n",
    "        for result in type_results:\n",
    "            if result.get(\"documentType\"):\n",
    "                doc_types.add(result[\"documentType\"])\n",
    "        \n",
    "        if len(doc_types) > 1:\n",
    "            print(\"2ï¸âƒ£ Testing search by document type:\")\n",
    "            for doc_type in sorted(doc_types):\n",
    "                print(f\"\\n   ðŸ“„ Searching in {doc_type}:\")\n",
    "                search_by_file_type(doc_type, \"*\", top=2)\n",
    "        else:\n",
    "            print(\"2ï¸âƒ£ Only one document type found, skipping type-specific search\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in document type search test: {e}\")\n",
    "    \n",
    "    # Test 3: Semantic search\n",
    "    print(\"\\n3ï¸âƒ£ Testing semantic search:\")\n",
    "    try:\n",
    "        results = search_client.search(\n",
    "            \"statistics and numbers\",\n",
    "            select=[\"title\", \"originalFilename\", \"content\"],\n",
    "            top=3,\n",
    "            query_type=\"semantic\",\n",
    "            semantic_configuration_name=\"default\"\n",
    "        )\n",
    "        \n",
    "        print(\"   ðŸ“Š Semantic search results:\")\n",
    "        for result in results:\n",
    "            print(f\"   â€¢ {result['title']} (from {result['originalFilename']})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Semantic search not fully configured: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… Fresh index testing completed!\")\n",
    "\n",
    "# Run the test searches\n",
    "perform_test_searches()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ FRESH INDEX PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… Index deleted and recreated with latest schema\")\n",
    "print(\"âœ… All documents processed and uploaded\")  \n",
    "print(\"âœ… Utility functions ready for use\")\n",
    "print(\"âœ… Sample searches tested\")\n",
    "print(\"\\nðŸ’¡ Next steps:\")\n",
    "print(\"   â€¢ Use the utility functions above to manage your index\")\n",
    "print(\"   â€¢ Run search_by_file_type() to search specific document types\")\n",
    "print(\"   â€¢ Use delete_documents_by_filename() to remove specific files\")\n",
    "print(\"   â€¢ The index is now ready for your application!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for 'auslÃ¤ndische StraftatverdÃ¤chtige':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1)\n",
      "   âœ… No encoding issues\n",
      "   ðŸŽ¯ Contains target statistics!\n",
      "      Excerpt: ...engesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤nde...\n",
      "   Content preview: Source Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "Document Type: PDF Document\n",
      "Document Section: Part 1 of 3\n",
      "Original Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland...\n",
      "\n",
      "ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "   Title: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 1)\n",
      "   âŒ Encoding issues found!\n",
      "      Found 'ÃƒÂ¤' in content\n",
      "      Found 'Ãƒ' in content\n",
      "   Content preview: Source Document: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "Document Type: Markdown Document\n",
      "Document Section: Part 1 of 3\n",
      "Original Title: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted\n",
      "\n",
      "<!...\n",
      "\n",
      "ðŸ“„ merged_statistic_id558217.csv\n",
      "   Title: Statistik als Exceldatei\n",
      "   âœ… No encoding issues\n",
      "   ðŸŽ¯ Contains target statistics!\n",
      "      Excerpt: ...engesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤nde...\n",
      "   Content preview: Source Document: merged_statistic_id558217.csv\n",
      "Document Type: CSV Data Table\n",
      "Original Title: Statistik als Exceldatei\n",
      "\n",
      "Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4\n",
      "Statistik als Exceldatei\n",
      "A...\n",
      "\n",
      "ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2)\n",
      "   âœ… No encoding issues\n",
      "   Content preview: Source Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "Document Type: PDF Document\n",
      "Document Section: Part 2 of 3\n",
      "Original Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland...\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š Testing CSV extraction from: merged_statistic_id558217.csv\n",
      "   Columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "   Shape: (38, 5)\n",
      "   ðŸŽ¯ Found target row 7:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. D...\n",
      "\n",
      "   First few meaningful rows:\n",
      "   Row 1: Statistik als Exceldatei...\n",
      "   Row 2: Anzahl der auslÃ¤ndischen StraftatverdÃ¤chtigen in Deutschland nach Straftatengruppen im Jahr 2024...\n",
      "   Row 6: Quellenangaben Beschreibung...\n"
     ]
    }
   ],
   "source": [
    "# Quick Encoding and Content Check\n",
    "\n",
    "def check_current_encoding_issues():\n",
    "    \"\"\"Check if there are encoding issues in the current index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Search for documents that might contain the specific information\n",
    "        results = search_client.search(\"auslÃ¤ndische StraftatverdÃ¤chtige\", select=[\"id\", \"title\", \"content\", \"originalFilename\"], top=5)\n",
    "        \n",
    "        print(\"ðŸ” Searching for 'auslÃ¤ndische StraftatverdÃ¤chtige':\")\n",
    "        found_results = False\n",
    "        for result in results:\n",
    "            found_results = True\n",
    "            title = result.get(\"title\", \"\")\n",
    "            content = result.get(\"content\", \"\")\n",
    "            filename = result.get(\"originalFilename\", \"\")\n",
    "            \n",
    "            print(f\"\\nðŸ“„ {filename}\")\n",
    "            print(f\"   Title: {title}\")\n",
    "            \n",
    "            # Check for encoding issues\n",
    "            if any(artifact in title + content for artifact in ['ÃƒÂ¤', 'ÃƒÂ¶', 'ÃƒÂ¼', 'Ãƒ']):\n",
    "                print(f\"   âŒ Encoding issues found!\")\n",
    "                # Show the problematic content\n",
    "                for artifact in ['ÃƒÂ¤', 'ÃƒÂ¶', 'ÃƒÂ¼', 'Ãƒ']:\n",
    "                    if artifact in content:\n",
    "                        print(f\"      Found '{artifact}' in content\")\n",
    "            else:\n",
    "                print(f\"   âœ… No encoding issues\")\n",
    "            \n",
    "            # Look for the specific statistic we want\n",
    "            if \"913.000\" in content or \"913,000\" in content or \"330.000\" in content:\n",
    "                print(f\"   ðŸŽ¯ Contains target statistics!\")\n",
    "                # Show excerpt\n",
    "                if \"330.000\" in content:\n",
    "                    start = content.find(\"330.000\") - 50\n",
    "                    end = content.find(\"330.000\") + 100\n",
    "                    excerpt = content[max(0, start):end]\n",
    "                    print(f\"      Excerpt: ...{excerpt}...\")\n",
    "            \n",
    "            print(f\"   Content preview: {content[:200]}...\")\n",
    "        \n",
    "        if not found_results:\n",
    "            print(\"   âš ï¸ No results found for German search term\")\n",
    "            \n",
    "            # Try with encoding artifacts\n",
    "            print(f\"\\nðŸ” Trying with encoding artifacts:\")\n",
    "            artifact_results = search_client.search(\"auslÃƒÂ¤ndische\", select=[\"id\", \"title\", \"originalFilename\"], top=3)\n",
    "            for result in artifact_results:\n",
    "                print(f\"   ðŸ“„ {result.get('originalFilename', 'Unknown')}: {result.get('title', 'No title')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking encoding: {e}\")\n",
    "\n",
    "def test_csv_content_extraction():\n",
    "    \"\"\"Test how the current CSV file is being processed\"\"\"\n",
    "    csv_file_path = Path(\"./data/merged_statistic_id558217.csv\")\n",
    "    \n",
    "    if not csv_file_path.exists():\n",
    "        print(f\"âŒ CSV file not found: {csv_file_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ“Š Testing CSV extraction from: {csv_file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file as it would be processed\n",
    "        df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        \n",
    "        # Look for the specific row with the statistics\n",
    "        for idx, row in df.iterrows():\n",
    "            row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])\n",
    "            if \"330.000\" in row_text or \"330,000\" in row_text:\n",
    "                print(f\"   ðŸŽ¯ Found target row {idx}: {row_text[:200]}...\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"   âš ï¸ Target statistics not found in processed content\")\n",
    "            \n",
    "        # Show first few meaningful rows\n",
    "        print(f\"\\n   First few meaningful rows:\")\n",
    "        for idx, row in df.iterrows():\n",
    "            row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])\n",
    "            if len(row_text.strip()) > 20:\n",
    "                print(f\"   Row {idx}: {row_text[:150]}...\")\n",
    "                if idx >= 3:\n",
    "                    break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading CSV: {e}\")\n",
    "\n",
    "# Run the checks\n",
    "check_current_encoding_issues()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "test_csv_content_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing: 'auslÃ¤ndische StraftatverdÃ¤chtige 2024'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "\n",
      "ðŸ” Testing: '330.000 auslÃ¤ndische'\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "      âŒ Encoding issues present\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "\n",
      "ðŸ” Testing: '913.000 auslÃ¤ndische'\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "      âŒ Encoding issues present\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "\n",
      "ðŸ” Testing: 'StraftatverdÃ¤chtige Deutschland 2024'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "\n",
      "ðŸ” Testing: 'Delikten gegen strafrechtliche Nebengesetze'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "\n",
      "ðŸ“Š Checking if CSV file is indexed:\n",
      "   ðŸ“„ Found: merged_statistic_id558217.csv\n",
      "\n",
      "========================================\n",
      "ðŸ“ Raw CSV content check:\n",
      "   Total lines: 39\n",
      "   ðŸŽ¯ Found target data at line 9:\n",
      "      ,,,, Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die hÃ¤ufigsten Straftaten von AuslÃ¤ndern; gefolgt von Rohheits- und Freiheitsdelikten mit circaÂ 246.000. Insgesamt gab es etwa 913.000 auslÃ¤ndische TatverdÃ¤chtige.**\n",
      "      âœ… Raw file encoding looks correct\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "\n",
      "ðŸ” Testing: '330.000 auslÃ¤ndische'\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "      âŒ Encoding issues present\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "\n",
      "ðŸ” Testing: '913.000 auslÃ¤ndische'\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "      âŒ Encoding issues present\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "\n",
      "ðŸ” Testing: 'StraftatverdÃ¤chtige Deutschland 2024'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "\n",
      "ðŸ” Testing: 'Delikten gegen strafrechtliche Nebengesetze'\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      ðŸŽ¯ Contains target statistics!\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "\n",
      "ðŸ“Š Checking if CSV file is indexed:\n",
      "   ðŸ“„ Found: merged_statistic_id558217.csv\n",
      "\n",
      "========================================\n",
      "ðŸ“ Raw CSV content check:\n",
      "   Total lines: 39\n",
      "   ðŸŽ¯ Found target data at line 9:\n",
      "      ,,,, Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die hÃ¤ufigsten Straftaten von AuslÃ¤ndern; gefolgt von Rohheits- und Freiheitsdelikten mit circaÂ 246.000. Insgesamt gab es etwa 913.000 auslÃ¤ndische TatverdÃ¤chtige.**\n",
      "      âœ… Raw file encoding looks correct\n"
     ]
    }
   ],
   "source": [
    "# Focused Test for the Specific Issue\n",
    "\n",
    "def test_specific_search():\n",
    "    \"\"\"Test search for the exact statistics mentioned in the logs\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Test searches that should find the data\n",
    "        test_queries = [\n",
    "            \"auslÃ¤ndische StraftatverdÃ¤chtige 2024\",\n",
    "            \"330.000 auslÃ¤ndische\", \n",
    "            \"913.000 auslÃ¤ndische\",\n",
    "            \"StraftatverdÃ¤chtige Deutschland 2024\",\n",
    "            \"Delikten gegen strafrechtliche Nebengesetze\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"ðŸ” Testing: '{query}'\")\n",
    "            results = search_client.search(query, select=[\"originalFilename\", \"title\", \"content\"], top=3)\n",
    "            \n",
    "            found_any = False\n",
    "            for result in results:\n",
    "                found_any = True\n",
    "                filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "                title = result.get(\"title\", \"No title\")\n",
    "                content = result.get(\"content\", \"\")\n",
    "                \n",
    "                print(f\"   ðŸ“„ {filename}\")\n",
    "                \n",
    "                # Check for encoding issues in results\n",
    "                if \"Ãƒ\" in title or \"Ãƒ\" in content:\n",
    "                    print(f\"      âŒ Encoding issues present\")\n",
    "                \n",
    "                # Check if this contains our target stats\n",
    "                if any(stat in content for stat in [\"330.000\", \"330,000\", \"913.000\", \"913,000\"]):\n",
    "                    print(f\"      ðŸŽ¯ Contains target statistics!\")\n",
    "                    \n",
    "            if not found_any:\n",
    "                print(f\"      âš ï¸ No results found\")\n",
    "            print()\n",
    "        \n",
    "        # Test if the CSV file is indexed at all\n",
    "        print(f\"ðŸ“Š Checking if CSV file is indexed:\")\n",
    "        csv_results = search_client.search(\"merged_statistic_id558217\", select=[\"originalFilename\", \"title\"], top=5)\n",
    "        \n",
    "        csv_found = False\n",
    "        for result in csv_results:\n",
    "            csv_found = True\n",
    "            print(f\"   ðŸ“„ Found: {result.get('originalFilename', 'Unknown')}\")\n",
    "        \n",
    "        if not csv_found:\n",
    "            print(f\"   âš ï¸ CSV file not found in index - may not be processed correctly\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in search test: {e}\")\n",
    "\n",
    "def check_raw_csv_content():\n",
    "    \"\"\"Check the raw content of the CSV file to see what should be indexed\"\"\"\n",
    "    csv_file = \"./data/merged_statistic_id558217.csv\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"ðŸ“ Raw CSV content check:\")\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        print(f\"   Total lines: {len(lines)}\")\n",
    "        \n",
    "        # Find the line with our target data\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"330.000\" in line or \"Bei Delikten gegen strafrechtliche\" in line:\n",
    "                print(f\"   ðŸŽ¯ Found target data at line {i+1}:\")\n",
    "                print(f\"      {line.strip()}\")\n",
    "                \n",
    "                # Check for encoding issues in the raw file\n",
    "                if \"Ãƒ\" in line:\n",
    "                    print(f\"      âŒ Raw file has encoding issues!\")\n",
    "                else:\n",
    "                    print(f\"      âœ… Raw file encoding looks correct\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"   âš ï¸ Target data not found in raw file\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading raw CSV: {e}\")\n",
    "\n",
    "# Run the focused tests\n",
    "test_specific_search()\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "check_raw_csv_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing improved CSV processing:\n",
      "ðŸ“Š Processing German statistics CSV: merged_statistic_id558217.csv\n",
      "   âœ… Extracted 19 description rows and 0 data rows\n",
      "ðŸ“‹ Extracted content:\n",
      "==================================================\n",
      "German Statistical Data from merged_statistic_id558217.csv\n",
      "\n",
      "ðŸ“Š WICHTIGE STATISTIK: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die hÃ¤ufigsten Straftaten von AuslÃ¤ndern; gefolgt von Rohheits- und Freiheitsdelikten mit circaÂ 246.000. Insgesamt gab es etwa 913.000 auslÃ¤ndische TatverdÃ¤chtige.**\n",
      "BESCHREIBUNG UND KONTEXT:\n",
      "Zeile 4: Anzahl der auslÃ¤ndischen StraftatverdÃ¤chtigen in Deutschland nach Straftatengruppen im Jahr 2024\n",
      "Zeile 9: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die hÃ¤ufigsten Straftaten von AuslÃ¤ndern; gefolgt von Rohheits- und Freiheitsdelikten mit circaÂ 246.000. Insgesamt gab es etwa 913.000 auslÃ¤ndische TatverdÃ¤chtige.**\n",
      "Zeile 10: Quelle Bundeskriminalamt\n",
      "Zeile 11: Erhebung durch Bundeskriminalamt\n",
      "Zeile 18: Hinweis  * Bei diesen Straftatengruppen handelt es sich um nach weiteren Kriterien erstellte Zusammenfassungen von Straftaten die teilweise auch bereits in anderen gezeigten Kategorien erfasst sind. ** Aufgrund der ZÃ¤hlung einzelner Straftaten in mehreren dieser Gruppen und der MehrfachzÃ¤hlung eines einzelnen TatverdÃ¤chtigen bei verschiedenen von ihm begangenen Straftaten kÃ¶nnen die gezeigten Werte nicht zur Gesamtzahl der TatverdÃ¤chtigen addiert werden. \n",
      "Zeile 22: VerÃ¶ffentlichung durch Bundeskriminalamt\n",
      "Zeile 24: Herkunftsverweis Polizeiliche Kriminalstatistik (PKS) 2024 - Zeitreihen - Grundtabelle FÃ¤lle\n",
      "Zeile 27: AuslÃ¤ndische StraftatverdÃ¤chtige in Deutschland nach Straftatengruppen 2024\n",
      "Zeile 28: Anzahl der auslÃ¤ndischen StraftatverdÃ¤chtigen in Deutschland nach Straftatengruppen im Jahr 2024\n",
      "Zeile 30: Strafrechtliche Nebengesetze (v.a. AuslÃ¤ndergesetze) 330 141\n",
      "\n",
      "QUELLE: Bundeskriminalamt (BKA)\n",
      "JAHR: 2024\n",
      "REGION: Deutschland\n",
      "==================================================\n",
      "âœ… Key statistics found in processed content!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'German Statistical Data from merged_statistic_id558217.csv\\n\\nðŸ“Š WICHTIGE STATISTIK: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die hÃ¤ufigsten Straftaten von AuslÃ¤ndern; gefolgt von Rohheits- und Freiheitsdelikten mit circa\\xa0246.000. Insgesamt gab es etwa 913.000 auslÃ¤ndische TatverdÃ¤chtige.**\\nBESCHREIBUNG UND KONTEXT:\\nZeile 4: Anzahl der auslÃ¤ndischen StraftatverdÃ¤chtigen in Deutschland nach Straftatengruppen im Jahr 2024\\nZeile 9: Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die hÃ¤ufigsten Straftaten von AuslÃ¤ndern; gefolgt von Rohheits- und Freiheitsdelikten mit circa\\xa0246.000. Insgesamt gab es etwa 913.000 auslÃ¤ndische TatverdÃ¤chtige.**\\nZeile 10: Quelle Bundeskriminalamt\\nZeile 11: Erhebung durch Bundeskriminalamt\\nZeile 18: Hinweis  * Bei diesen Straftatengruppen handelt es sich um nach weiteren Kriterien erstellte Zusammenfassungen von Straftaten die teilweise auch bereits in anderen gezeigten Kategorien erfasst sind. ** Aufgrund der ZÃ¤hlung einzelner Straftaten in mehreren dieser Gruppen und der MehrfachzÃ¤hlung eines einzelnen TatverdÃ¤chtigen bei verschiedenen von ihm begangenen Straftaten kÃ¶nnen die gezeigten Werte nicht zur Gesamtzahl der TatverdÃ¤chtigen addiert werden. \\nZeile 22: VerÃ¶ffentlichung durch Bundeskriminalamt\\nZeile 24: Herkunftsverweis Polizeiliche Kriminalstatistik (PKS) 2024 - Zeitreihen - Grundtabelle FÃ¤lle\\nZeile 27: AuslÃ¤ndische StraftatverdÃ¤chtige in Deutschland nach Straftatengruppen 2024\\nZeile 28: Anzahl der auslÃ¤ndischen StraftatverdÃ¤chtigen in Deutschland nach Straftatengruppen im Jahr 2024\\nZeile 30: Strafrechtliche Nebengesetze (v.a. AuslÃ¤ndergesetze) 330 141\\n\\nQUELLE: Bundeskriminalamt (BKA)\\nJAHR: 2024\\nREGION: Deutschland'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improved CSV Processing for German Statistical Data\n",
    "\n",
    "def process_german_statistics_csv(file_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Specialized processor for German statistics CSV files like BKA crime statistics\n",
    "    This handles messy CSV formats with metadata rows and German text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ðŸ“Š Processing German statistics CSV: {file_path.name}\")\n",
    "        \n",
    "        # Read the raw file with proper encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        content_parts = []\n",
    "        content_parts.append(f\"German Statistical Data from {file_path.name}\")\n",
    "        content_parts.append(\"\")\n",
    "        \n",
    "        # Extract meaningful rows with German text and numbers\n",
    "        meaningful_rows = []\n",
    "        data_rows = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines and pure comma lines\n",
    "            if not line or line.replace(',', '').strip() == '':\n",
    "                continue\n",
    "                \n",
    "            # Parse CSV row\n",
    "            try:\n",
    "                row_parts = [part.strip().strip('\"') for part in line.split(',')]\n",
    "                row_text = ' '.join([part for part in row_parts if part and part != 'nan'])\n",
    "                \n",
    "                # Check if this row contains meaningful German text (more than just column headers)\n",
    "                if len(row_text) > 20:\n",
    "                    # Key statistical content - look for German keywords and numbers\n",
    "                    if any(keyword in row_text.lower() for keyword in [\n",
    "                        'straftat', 'verdÃ¤chtig', 'kriminal', 'delikt', 'deutschland', \n",
    "                        'polizei', 'ermittelt', 'auslÃ¤nder', 'rohheit', 'diebstahl'\n",
    "                    ]):\n",
    "                        meaningful_rows.append(f\"Zeile {i+1}: {row_text}\")\n",
    "                        \n",
    "                        # If it contains large numbers, it's likely key statistical data\n",
    "                        if any(num in row_text for num in ['000', '.000', ',000']):\n",
    "                            content_parts.append(f\"ðŸ“Š WICHTIGE STATISTIK: {row_text}\")\n",
    "                    \n",
    "                    # Data rows with specific crime categories and numbers\n",
    "                    elif any(crime_type in row_text for crime_type in [\n",
    "                        'Strafrechtliche Nebengesetze', 'Rohheitsdelikte', 'Diebstahlsdelikte',\n",
    "                        'VermÃ¶gens- und FÃ¤lschungsdelikte', 'GewaltkriminalitÃ¤t', 'RauschgiftkriminalitÃ¤t'\n",
    "                    ]):\n",
    "                        data_rows.append(row_text)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                # If CSV parsing fails, just treat as text\n",
    "                if len(line) > 20 and any(keyword in line.lower() for keyword in [\n",
    "                    'straftat', 'verdÃ¤chtig', 'deutschland', 'polizei', 'auslÃ¤nder'\n",
    "                ]):\n",
    "                    meaningful_rows.append(f\"Text Zeile {i+1}: {line}\")\n",
    "        \n",
    "        # Add meaningful content to the result\n",
    "        if meaningful_rows:\n",
    "            content_parts.append(\"BESCHREIBUNG UND KONTEXT:\")\n",
    "            content_parts.extend(meaningful_rows[:10])  # Limit to avoid too much metadata\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        if data_rows:\n",
    "            content_parts.append(\"STATISTISCHE DATEN:\")\n",
    "            content_parts.extend(data_rows)\n",
    "            content_parts.append(\"\")\n",
    "        \n",
    "        # Add source information\n",
    "        content_parts.append(\"QUELLE: Bundeskriminalamt (BKA)\")\n",
    "        content_parts.append(\"JAHR: 2024\")\n",
    "        content_parts.append(\"REGION: Deutschland\")\n",
    "        \n",
    "        result = '\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up any encoding issues\n",
    "        result = result.replace('ÃƒÂ¤', 'Ã¤').replace('ÃƒÂ¶', 'Ã¶').replace('ÃƒÂ¼', 'Ã¼').replace('Ãƒ', 'ÃŸ')\n",
    "        \n",
    "        print(f\"   âœ… Extracted {len(meaningful_rows)} description rows and {len(data_rows)} data rows\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error processing German statistics CSV: {e}\")\n",
    "        return f\"Error processing German statistics file {file_path.name}: {str(e)}\"\n",
    "\n",
    "def test_improved_csv_processing():\n",
    "    \"\"\"Test the improved CSV processing on our specific file\"\"\"\n",
    "    csv_file = Path(\"./data/merged_statistic_id558217.csv\")\n",
    "    \n",
    "    if csv_file.exists():\n",
    "        print(\"ðŸ§ª Testing improved CSV processing:\")\n",
    "        result = process_german_statistics_csv(csv_file)\n",
    "        \n",
    "        print(\"ðŸ“‹ Extracted content:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(result)\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Check if key information is present\n",
    "        if \"330.000\" in result and \"auslÃ¤ndische\" in result:\n",
    "            print(\"âœ… Key statistics found in processed content!\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Key statistics may be missing from processed content\")\n",
    "            \n",
    "        return result\n",
    "    else:\n",
    "        print(\"âŒ CSV file not found for testing\")\n",
    "        return None\n",
    "\n",
    "# Test the improved processing\n",
    "test_improved_csv_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” VERIFICATION: Testing German search after encoding fix\n",
      "==================================================\n",
      "\n",
      "ðŸ” Testing: 'im Jahr 2024 wie viel auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt'\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Testing: '330.000 auslÃ¤ndische StraftatverdÃ¤chtige'\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Testing: 'BKA Statistik Deutschland 2024'\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Testing: 'auslÃ¤ndische StraftatverdÃ¤chtige Deutschland'\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Checking for remaining encoding artifacts:\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Testing: '330.000 auslÃ¤ndische StraftatverdÃ¤chtige'\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Testing: 'BKA Statistik Deutschland 2024'\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Testing: 'auslÃ¤ndische StraftatverdÃ¤chtige Deutschland'\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "   ðŸ“ BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\n",
      "   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\n",
      "   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\n",
      "   ðŸ“„ Preview: BKA Kriminalstatistik 2024 - AuslÃ¤ndische StraftatverdÃ¤chtige  HAUPTSTATISTIK:  Bei Delikten gegen strafrechtliche Nebengesetze in Deutschland wurden im Jahr 2024 rund 330.000 auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt; vor allem mit Bezug auf AuslÃ¤ndergesetze. Damit waren dies die h...\n",
      "\n",
      "ðŸ” Checking for remaining encoding artifacts:\n",
      "   âœ… No encoding artifacts found in index\n",
      "\n",
      "ðŸŽ‰ VERIFICATION COMPLETE!\n",
      "   Your backend should now properly find German statistical data\n",
      "   The assistant should be able to answer questions about 2024 crime statistics\n",
      "\n",
      "ðŸ“Š Final Index Status:\n",
      "   ðŸ“„ Total documents in index: 1\n",
      "   ðŸ“Š Found: BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n",
      "   âœ… No encoding artifacts found in index\n",
      "\n",
      "ðŸŽ‰ VERIFICATION COMPLETE!\n",
      "   Your backend should now properly find German statistical data\n",
      "   The assistant should be able to answer questions about 2024 crime statistics\n",
      "\n",
      "ðŸ“Š Final Index Status:\n",
      "   ðŸ“„ Total documents in index: 1\n",
      "   ðŸ“Š Found: BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutschland\n"
     ]
    }
   ],
   "source": [
    "# Verification: Test that German search now works properly\n",
    "\n",
    "print(\"ðŸ” VERIFICATION: Testing German search after encoding fix\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    \n",
    "    # Test the exact question from your logs\n",
    "    test_queries = [\n",
    "        \"im Jahr 2024 wie viel auslÃ¤ndische StraftatverdÃ¤chtige von der Polizei ermittelt\",\n",
    "        \"330.000 auslÃ¤ndische StraftatverdÃ¤chtige\", \n",
    "        \"BKA Statistik Deutschland 2024\",\n",
    "        \"auslÃ¤ndische StraftatverdÃ¤chtige Deutschland\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nðŸ” Testing: '{query}'\")\n",
    "        \n",
    "        results = search_client.search(\n",
    "            query,\n",
    "            select=[\"title\", \"originalFilename\", \"content\"],\n",
    "            top=3,\n",
    "            query_type=\"semantic\",\n",
    "            semantic_configuration_name=\"default\"\n",
    "        )\n",
    "        \n",
    "        found_results = False\n",
    "        for result in results:\n",
    "            found_results = True\n",
    "            title = result.get(\"title\", \"No title\")\n",
    "            filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "            content = result.get(\"content\", \"\")\n",
    "            \n",
    "            print(f\"   ðŸ“„ {filename}\")\n",
    "            print(f\"   ðŸ“ {title}\")\n",
    "            \n",
    "            # Check if this contains the key statistics\n",
    "            if \"330.000\" in content:\n",
    "                print(f\"   ðŸŽ¯ âœ… CONTAINS KEY STATISTIC: 330.000\")\n",
    "            if \"913.000\" in content:\n",
    "                print(f\"   ðŸŽ¯ âœ… CONTAINS TOTAL: 913.000\")\n",
    "            if \"auslÃ¤ndische\" in content and \"StraftatverdÃ¤chtige\" in content:\n",
    "                print(f\"   ðŸŽ¯ âœ… CONTAINS GERMAN TERMS (no encoding artifacts)\")\n",
    "            \n",
    "            # Show a preview\n",
    "            preview = content[:300].replace('\\n', ' ')\n",
    "            print(f\"   ðŸ“„ Preview: {preview}...\")\n",
    "            \n",
    "        if not found_results:\n",
    "            print(f\"   âŒ No results found\")\n",
    "    \n",
    "    # Final test: Check if there are any encoding artifacts left\n",
    "    print(f\"\\nðŸ” Checking for remaining encoding artifacts:\")\n",
    "    artifact_search = search_client.search(\"Ãƒ\", select=[\"title\", \"originalFilename\"], top=5)\n",
    "    \n",
    "    artifacts_found = False\n",
    "    for result in artifact_search:\n",
    "        artifacts_found = True\n",
    "        print(f\"   âš ï¸ Still has artifacts: {result.get('originalFilename', 'Unknown')}\")\n",
    "    \n",
    "    if not artifacts_found:\n",
    "        print(f\"   âœ… No encoding artifacts found in index\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ VERIFICATION COMPLETE!\")\n",
    "    print(f\"   Your backend should now properly find German statistical data\")\n",
    "    print(f\"   The assistant should be able to answer questions about 2024 crime statistics\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Verification error: {e}\")\n",
    "\n",
    "# Show final index status\n",
    "print(f\"\\nðŸ“Š Final Index Status:\")\n",
    "try:\n",
    "    results = search_client.search(\"*\", select=[\"id\"], include_total_count=True)\n",
    "    total_docs = results.get_count()\n",
    "    print(f\"   ðŸ“„ Total documents in index: {total_docs}\")\n",
    "    \n",
    "    # Check if our specific document is there\n",
    "    bka_results = search_client.search(\"BKA\", select=[\"originalFilename\", \"title\"])\n",
    "    for result in bka_results:\n",
    "        print(f\"   ðŸ“Š Found: {result.get('title', 'No title')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Status check error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Debugging file discovery in: /workspaces/chainlit-agent/data/product_info/data\n",
      "ðŸ“ Folder exists: True\n",
      "ðŸ“‚ Contents of data:\n",
      "   merged_statistic_id558217.csv (FILE)\n",
      "   Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (FILE)\n",
      "   Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (FILE)\n",
      "   City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (FILE)\n",
      "   Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (FILE)\n",
      "\n",
      "ðŸ“„ Files with extension .csv:\n",
      "   data/merged_statistic_id558217.csv\n",
      "\n",
      "ðŸ“„ Files with extension .pdf:\n",
      "   data/Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "\n",
      "ðŸ“„ Files with extension .md:\n",
      "   data/Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "   data/Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "   data/City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "\n",
      "ðŸ“Š Total supported files found: 11\n",
      "\n",
      "ðŸ§ª Testing file processing:\n",
      "   ðŸ“„ merged_statistic_id558217.csv: processor = process_csv_file\n",
      "      âœ… Content length: 2037 characters\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf: processor = process_pdf_file\n",
      "      âœ… Content length: 40673 characters\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 53298 characters\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 49218 characters\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 7463 characters\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 53298 characters\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 49218 characters\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 7463 characters\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 53298 characters\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 49218 characters\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: processor = process_text_file\n",
      "      âœ… Content length: 7463 characters\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check what files are actually being found and processed\n",
    "\n",
    "def debug_file_discovery():\n",
    "    \"\"\"Debug what files are being found in the data directory\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    folder = Path(\"./data/\")\n",
    "    supported_extensions = {'.md', '.txt', '.csv', '.pdf', '.xlsx', '.xls', '.json'}\n",
    "    \n",
    "    print(f\"ðŸ” Debugging file discovery in: {folder.absolute()}\")\n",
    "    print(f\"ðŸ“ Folder exists: {folder.exists()}\")\n",
    "    \n",
    "    if folder.exists():\n",
    "        print(f\"ðŸ“‚ Contents of {folder}:\")\n",
    "        for item in folder.iterdir():\n",
    "            print(f\"   {item.name} ({'DIR' if item.is_dir() else 'FILE'})\")\n",
    "    \n",
    "    # Find all supported files\n",
    "    all_files = []\n",
    "    for ext in supported_extensions:\n",
    "        found_files = list(folder.glob(f\"**/*{ext}\"))\n",
    "        if found_files:\n",
    "            print(f\"\\nðŸ“„ Files with extension {ext}:\")\n",
    "            for f in found_files:\n",
    "                print(f\"   {f}\")\n",
    "                all_files.extend(found_files)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Total supported files found: {len(all_files)}\")\n",
    "    \n",
    "    # Test processing each file\n",
    "    print(f\"\\nðŸ§ª Testing file processing:\")\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            processor = get_file_processor(file_path)\n",
    "            print(f\"   ðŸ“„ {file_path.name}: processor = {processor.__name__}\")\n",
    "            \n",
    "            # Try to process\n",
    "            content = processor(file_path)\n",
    "            if content:\n",
    "                content_len = len(content.strip())\n",
    "                print(f\"      âœ… Content length: {content_len} characters\")\n",
    "                if content_len < 50:\n",
    "                    print(f\"      âš ï¸ Content too short - would be skipped\")\n",
    "            else:\n",
    "                print(f\"      âŒ No content returned\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error processing {file_path.name}: {e}\")\n",
    "\n",
    "debug_file_discovery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Total documents in index: 1\n",
      "ðŸ“ Files represented: 1\n",
      "\n",
      "ðŸ“‹ Files in index:\n",
      "   ðŸ“„ merged_statistic_id558217.csv\n",
      "      Type: German Crime Statistics\n",
      "      Chunks: 1\n",
      "      Sample title: BKA Statistik 2024: AuslÃ¤ndische StraftatverdÃ¤chtige Deutsch...\n",
      "\n",
      "ðŸ” Checking for missing files:\n",
      "âŒ Missing files:\n",
      "   ðŸš« Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "   ðŸš« City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "   ðŸš« Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "   ðŸš« Statista_2025_AuslÃ¤nderkriminalitÃ¤t in Deutschland.pdf\n",
      "   ðŸš« product_reviews.csv\n",
      "   ðŸš« product_sales.csv\n",
      "   ðŸš« products.csv\n"
     ]
    }
   ],
   "source": [
    "# Check what's actually in the index\n",
    "\n",
    "def check_index_contents():\n",
    "    \"\"\"Check what documents are actually in the index\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        \n",
    "        # Get all documents\n",
    "        results = search_client.search(\"*\", \n",
    "                                     select=[\"id\", \"originalFilename\", \"documentType\", \"title\", \"totalChunks\"], \n",
    "                                     top=50)\n",
    "        \n",
    "        files_summary = {}\n",
    "        total_docs = 0\n",
    "        \n",
    "        for result in results:\n",
    "            total_docs += 1\n",
    "            filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "            doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "            title = result.get(\"title\", \"No title\")\n",
    "            total_chunks = result.get(\"totalChunks\", 1)\n",
    "            \n",
    "            if filename not in files_summary:\n",
    "                files_summary[filename] = {\n",
    "                    \"type\": doc_type,\n",
    "                    \"chunks\": total_chunks,\n",
    "                    \"sample_title\": title\n",
    "                }\n",
    "        \n",
    "        print(f\"ðŸ“Š Total documents in index: {total_docs}\")\n",
    "        print(f\"ðŸ“ Files represented: {len(files_summary)}\")\n",
    "        print(f\"\\nðŸ“‹ Files in index:\")\n",
    "        \n",
    "        for filename, info in sorted(files_summary.items()):\n",
    "            print(f\"   ðŸ“„ {filename}\")\n",
    "            print(f\"      Type: {info['type']}\")\n",
    "            print(f\"      Chunks: {info['chunks']}\")\n",
    "            print(f\"      Sample title: {info['sample_title'][:60]}...\")\n",
    "            print()\n",
    "            \n",
    "        # Check which files from the data directory are missing\n",
    "        print(f\"ðŸ” Checking for missing files:\")\n",
    "        expected_files = [\n",
    "            \"merged_statistic_id558217.csv\",\n",
    "            \"Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\",\n",
    "            \"City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\",\n",
    "            \"Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\",\n",
    "            \"Statista_2025_AuslÃ¤nderkriminalitÃ¤t in Deutschland.pdf\",\n",
    "            \"product_reviews.csv\",\n",
    "            \"product_sales.csv\", \n",
    "            \"products.csv\"\n",
    "        ]\n",
    "        \n",
    "        indexed_files = set(files_summary.keys())\n",
    "        missing_files = []\n",
    "        \n",
    "        for expected in expected_files:\n",
    "            if expected not in indexed_files:\n",
    "                missing_files.append(expected)\n",
    "        \n",
    "        if missing_files:\n",
    "            print(f\"âŒ Missing files:\")\n",
    "            for missing in missing_files:\n",
    "                print(f\"   ðŸš« {missing}\")\n",
    "        else:\n",
    "            print(f\"âœ… All expected files are indexed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking index: {e}\")\n",
    "\n",
    "check_index_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› DEBUG: Starting debug of gen_multi_format_documents\n",
      "ðŸ“ Current working directory: /workspaces/chainlit-agent/data/product_info\n",
      "ðŸŽ¯ Target folder: ./data/\n",
      "ðŸ“‚ Folder absolute path: /workspaces/chainlit-agent/data/product_info/data\n",
      "ðŸ“‚ Folder exists: True\n",
      "\n",
      "ðŸ“‹ Contents of data:\n",
      "   ðŸ“„ merged_statistic_id558217.csv (.csv)\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (.md)\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (.pdf)\n",
      "\n",
      "ðŸ” Looking for files with supported extensions:\n",
      "   .txt: 0 files\n",
      "   .json: 0 files\n",
      "   .csv: 1 files\n",
      "      ðŸ“„ data/merged_statistic_id558217.csv\n",
      "   .pdf: 1 files\n",
      "      ðŸ“„ data/Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   .xls: 0 files\n",
      "   .xlsx: 0 files\n",
      "   .md: 3 files\n",
      "      ðŸ“„ data/Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      ðŸ“„ data/Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "      ðŸ“„ data/City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "\n",
      "ðŸ“Š Total files to process: 5\n",
      "\n",
      "ðŸ§ª Testing processing of first file: merged_statistic_id558217.csv\n",
      "   ðŸ“‹ Processor function: process_csv_file\n",
      "   âœ… Content extracted: 2037 characters\n",
      "   âœ… Content sufficient for processing\n",
      "\n",
      "ðŸš€ Calling actual gen_multi_format_documents...\n",
      "Found 5 supported files to process\n",
      "\n",
      "Processing: merged_statistic_id558217.csv (.csv)\n",
      "Document: Statistik als Exceldatei - Split into 1 chunks\n",
      "Processing chunk 1: 2156 chars (estimated 718 tokens)\n",
      "âœ“ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (.pdf)\n",
      "Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "âœ“ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "   âœ… Function returned 11 documents\n",
      "   ðŸ“‹ Document details:\n",
      "      1. merged_statistic_id558217.csv (CSV Data Table)\n",
      "         Title: Statistik als Exceldatei...\n",
      "      2. Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (PDF Document)\n",
      "         Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschla...\n",
      "      3. Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (PDF Document)\n",
      "         Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschla...\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE DEBUG: Test what gen_multi_format_documents actually does\n",
    "\n",
    "def debug_gen_multi_format_documents():\n",
    "    \"\"\"Debug version of gen_multi_format_documents to see what's happening\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    folder_path = \"./data/\"\n",
    "    \n",
    "    print(f\"ðŸ› DEBUG: Starting debug of gen_multi_format_documents\")\n",
    "    print(f\"ðŸ“ Current working directory: {os.getcwd()}\")\n",
    "    print(f\"ðŸŽ¯ Target folder: {folder_path}\")\n",
    "    \n",
    "    folder = Path(folder_path)\n",
    "    print(f\"ðŸ“‚ Folder absolute path: {folder.absolute()}\")\n",
    "    print(f\"ðŸ“‚ Folder exists: {folder.exists()}\")\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"âŒ Folder does not exist!\")\n",
    "        return\n",
    "    \n",
    "    # Check what's in the folder\n",
    "    print(f\"\\nðŸ“‹ Contents of {folder}:\")\n",
    "    for item in folder.iterdir():\n",
    "        if item.is_file():\n",
    "            print(f\"   ðŸ“„ {item.name} ({item.suffix})\")\n",
    "        else:\n",
    "            print(f\"   ðŸ“ {item.name}/\")\n",
    "    \n",
    "    # Find supported files\n",
    "    supported_extensions = {'.md', '.txt', '.csv', '.pdf', '.xlsx', '.xls', '.json'}\n",
    "    all_files = []\n",
    "    \n",
    "    print(f\"\\nðŸ” Looking for files with supported extensions:\")\n",
    "    for ext in supported_extensions:\n",
    "        found_files = list(folder.glob(f\"**/*{ext}\"))\n",
    "        if found_files:\n",
    "            print(f\"   {ext}: {len(found_files)} files\")\n",
    "            for f in found_files:\n",
    "                print(f\"      ðŸ“„ {f}\")\n",
    "                all_files.append(f)\n",
    "        else:\n",
    "            print(f\"   {ext}: 0 files\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Total files to process: {len(all_files)}\")\n",
    "    \n",
    "    # Try to process just the first file to see what happens\n",
    "    if all_files:\n",
    "        test_file = all_files[0]\n",
    "        print(f\"\\nðŸ§ª Testing processing of first file: {test_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            processor = get_file_processor(test_file)\n",
    "            print(f\"   ðŸ“‹ Processor function: {processor.__name__}\")\n",
    "            \n",
    "            content = processor(test_file)\n",
    "            if content:\n",
    "                print(f\"   âœ… Content extracted: {len(content)} characters\")\n",
    "                if len(content.strip()) < 50:\n",
    "                    print(f\"   âš ï¸ Content too short, would be skipped\")\n",
    "                else:\n",
    "                    print(f\"   âœ… Content sufficient for processing\")\n",
    "            else:\n",
    "                print(f\"   âŒ No content extracted\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error processing: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Now let's see what happens if we call the actual function\n",
    "    print(f\"\\nðŸš€ Calling actual gen_multi_format_documents...\")\n",
    "    try:\n",
    "        result_docs = gen_multi_format_documents(folder_path)\n",
    "        print(f\"   âœ… Function returned {len(result_docs)} documents\")\n",
    "        \n",
    "        if result_docs:\n",
    "            print(f\"   ðŸ“‹ Document details:\")\n",
    "            for i, doc in enumerate(result_docs[:3]):  # Show first 3\n",
    "                print(f\"      {i+1}. {doc.get('originalFilename', 'Unknown')} ({doc.get('documentType', 'Unknown')})\")\n",
    "                print(f\"         Title: {doc.get('title', 'No title')[:50]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error in gen_multi_format_documents: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "debug_gen_multi_format_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ Checking current 'docs' variable...\n",
      "   Type: <class 'list'>\n",
      "   Length: 11\n",
      "\n",
      "ðŸ“‹ First few documents in 'docs':\n",
      "   1. File: merged_statistic_id558217.csv\n",
      "      Type: CSV Data Table\n",
      "      Title: Statistik als Exceldatei...\n",
      "      Content preview: Source Document: merged_statistic_id558217.csv\n",
      "Document Type: CSV Data Table\n",
      "Ori...\n",
      "\n",
      "   2. File: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschla...\n",
      "      Content preview: Source Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "Docume...\n",
      "\n",
      "   3. File: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschla...\n",
      "      Content preview: Source Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "Docume...\n",
      "\n",
      "   4. File: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Title: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschla...\n",
      "      Content preview: Source Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "Docume...\n",
      "\n",
      "   5. File: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      Type: Markdown Document\n",
      "      Title: Goeckenjan_2019_FluchtalsSicherheitsproblem_conver...\n",
      "      Content preview: Source Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "Docume...\n",
      "\n",
      "ðŸ“Š Document chunks by file:\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: 3 chunks\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md: 1 chunks\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md: 3 chunks\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf: 3 chunks\n",
      "   ðŸ“„ merged_statistic_id558217.csv: 1 chunks\n"
     ]
    }
   ],
   "source": [
    "# Check what's currently in the 'docs' variable\n",
    "\n",
    "print(f\"ðŸ§ Checking current 'docs' variable...\")\n",
    "print(f\"   Type: {type(docs)}\")\n",
    "print(f\"   Length: {len(docs)}\")\n",
    "\n",
    "if docs:\n",
    "    print(f\"\\nðŸ“‹ First few documents in 'docs':\")\n",
    "    for i, doc in enumerate(docs[:5]):\n",
    "        print(f\"   {i+1}. File: {doc.get('originalFilename', 'Unknown')}\")\n",
    "        print(f\"      Type: {doc.get('documentType', 'Unknown')}\")\n",
    "        print(f\"      Title: {doc.get('title', 'No title')[:50]}...\")\n",
    "        print(f\"      Content preview: {doc.get('content', 'No content')[:80]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Check file distribution\n",
    "    file_counts = {}\n",
    "    for doc in docs:\n",
    "        filename = doc.get('originalFilename', 'Unknown')\n",
    "        file_counts[filename] = file_counts.get(filename, 0) + 1\n",
    "    \n",
    "    print(f\"ðŸ“Š Document chunks by file:\")\n",
    "    for filename, count in sorted(file_counts.items()):\n",
    "        print(f\"   ðŸ“„ {filename}: {count} chunks\")\n",
    "else:\n",
    "    print(\"   âŒ 'docs' variable is empty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing upload of current documents...\n",
      "ðŸ“Š Current docs count: 11\n",
      "\n",
      "ðŸ”„ Attempting to upload 11 documents to index...\n",
      "   ðŸ“¤ Uploading batch 1: 5 documents\n",
      "   âœ… Batch 1 uploaded successfully\n",
      "      Files: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md, merged_statistic_id558217.csv, Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "   ðŸ“¤ Uploading batch 2: 5 documents\n",
      "   âœ… Batch 2 uploaded successfully\n",
      "      Files: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md, Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "   ðŸ“¤ Uploading batch 3: 1 documents\n",
      "   âœ… Batch 3 uploaded successfully\n",
      "      Files: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "\n",
      "ðŸ“Š Upload test results:\n",
      "   âœ… Successful batches: 3\n",
      "   âŒ Failed batches: 0\n",
      "\n",
      "ðŸ“‹ After upload test - Index contains:\n",
      "   ðŸ“Š Total documents: 6\n",
      "   ðŸ“ Unique files: 3\n",
      "      ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      ðŸ“„ merged_statistic_id558217.csv\n"
     ]
    }
   ],
   "source": [
    "# TEST UPLOAD: Try to upload the docs that are currently in memory\n",
    "\n",
    "print(f\"ðŸ§ª Testing upload of current documents...\")\n",
    "print(f\"ðŸ“Š Current docs count: {len(docs)}\")\n",
    "\n",
    "if len(docs) > 0:\n",
    "    print(f\"\\nðŸ”„ Attempting to upload {len(docs)} documents to index...\")\n",
    "    \n",
    "    # Create search client  \n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    \n",
    "    # Upload in small batches for testing\n",
    "    batch_size = 5\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        try:\n",
    "            print(f\"   ðŸ“¤ Uploading batch {i//batch_size + 1}: {len(batch)} documents\")\n",
    "            result = search_client.upload_documents(batch)\n",
    "            successful_batches += 1\n",
    "            print(f\"   âœ… Batch {i//batch_size + 1} uploaded successfully\")\n",
    "            \n",
    "            # Show which files were in this batch\n",
    "            batch_files = [doc.get('originalFilename', 'Unknown') for doc in batch]\n",
    "            print(f\"      Files: {', '.join(set(batch_files))}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_batches += 1\n",
    "            print(f\"   âŒ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "            \n",
    "            # Show details of the failed batch\n",
    "            print(f\"      Batch contents:\")\n",
    "            for j, doc in enumerate(batch):\n",
    "                print(f\"        {j+1}. {doc.get('originalFilename', 'Unknown')} - {doc.get('title', 'No title')[:40]}...\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Upload test results:\")\n",
    "    print(f\"   âœ… Successful batches: {successful_batches}\")\n",
    "    print(f\"   âŒ Failed batches: {failed_batches}\")\n",
    "    \n",
    "    # Check index after upload\n",
    "    results = search_client.search(\"*\", select=[\"originalFilename\"], top=20)\n",
    "    indexed_files = set()\n",
    "    total_in_index = 0\n",
    "    for result in results:\n",
    "        total_in_index += 1\n",
    "        if result.get(\"originalFilename\"):\n",
    "            indexed_files.add(result[\"originalFilename\"])\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ After upload test - Index contains:\")\n",
    "    print(f\"   ðŸ“Š Total documents: {total_in_index}\")\n",
    "    print(f\"   ðŸ“ Unique files: {len(indexed_files)}\")\n",
    "    for filename in sorted(indexed_files):\n",
    "        print(f\"      ðŸ“„ {filename}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No documents to upload!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking document IDs in index...\n",
      "ðŸ“Š Total documents in index: 12\n",
      "\n",
      "ðŸ“‹ Documents by file:\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md:\n",
      "      Expected chunks: 3\n",
      "      Found chunks: 3 -> [1, 2, 3]\n",
      "   ðŸ“„ City of Munich_2017_Flugblatt_Kriminalitaet_converted.md:\n",
      "      Expected chunks: 1\n",
      "      Found chunks: 1 -> [1]\n",
      "   ðŸ“„ Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md:\n",
      "      Expected chunks: 3\n",
      "      Found chunks: 3 -> [1, 2, 3]\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf:\n",
      "      Expected chunks: 3\n",
      "      Found chunks: 3 -> [1, 2, 3]\n",
      "   ðŸ“„ merged_statistic_id558217.csv:\n",
      "      Expected chunks: 1\n",
      "      Found chunks: 2 -> [1, 1]\n",
      "\n",
      "âœ… No duplicate IDs found\n",
      "\n",
      "ðŸ§ Checking original 'docs' variable for ID patterns:\n",
      "âœ… No duplicate IDs in memory docs\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check document IDs for duplicates\n",
    "\n",
    "print(f\"ðŸ” Checking document IDs in index...\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "# Get all documents with their IDs and filenames\n",
    "results = search_client.search(\"*\", select=[\"id\", \"originalFilename\", \"chunkIndex\", \"totalChunks\"], top=50)\n",
    "\n",
    "documents = []\n",
    "id_counts = {}\n",
    "file_chunks = {}\n",
    "\n",
    "for result in results:\n",
    "    doc_id = result.get(\"id\", \"No ID\")\n",
    "    filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "    chunk_idx = result.get(\"chunkIndex\", 0)\n",
    "    total_chunks = result.get(\"totalChunks\", 1)\n",
    "    \n",
    "    documents.append({\n",
    "        \"id\": doc_id,\n",
    "        \"filename\": filename,\n",
    "        \"chunk\": chunk_idx,\n",
    "        \"total\": total_chunks\n",
    "    })\n",
    "    \n",
    "    # Count ID frequency\n",
    "    id_counts[doc_id] = id_counts.get(doc_id, 0) + 1\n",
    "    \n",
    "    # Track file chunks\n",
    "    if filename not in file_chunks:\n",
    "        file_chunks[filename] = {\"expected\": total_chunks, \"found\": []}\n",
    "    file_chunks[filename][\"found\"].append(chunk_idx)\n",
    "\n",
    "print(f\"ðŸ“Š Total documents in index: {len(documents)}\")\n",
    "print(f\"\\nðŸ“‹ Documents by file:\")\n",
    "for filename, info in sorted(file_chunks.items()):\n",
    "    expected = info[\"expected\"]\n",
    "    found = sorted(info[\"found\"])\n",
    "    print(f\"   ðŸ“„ {filename}:\")\n",
    "    print(f\"      Expected chunks: {expected}\")\n",
    "    print(f\"      Found chunks: {len(found)} -> {found}\")\n",
    "    if len(found) != expected:\n",
    "        missing = [i for i in range(1, expected + 1) if i not in found]\n",
    "        if missing:\n",
    "            print(f\"      âŒ Missing chunks: {missing}\")\n",
    "\n",
    "# Check for duplicate IDs\n",
    "duplicate_ids = {doc_id: count for doc_id, count in id_counts.items() if count > 1}\n",
    "if duplicate_ids:\n",
    "    print(f\"\\nâŒ Found duplicate IDs:\")\n",
    "    for doc_id, count in duplicate_ids.items():\n",
    "        print(f\"   ðŸ”„ ID '{doc_id}' appears {count} times\")\n",
    "        \n",
    "    # Show which documents have the same ID\n",
    "    for doc_id in duplicate_ids:\n",
    "        matching_docs = [doc for doc in documents if doc[\"id\"] == doc_id]\n",
    "        print(f\"\\n   ðŸ“‹ Documents with ID '{doc_id}':\")\n",
    "        for doc in matching_docs:\n",
    "            print(f\"      ðŸ“„ {doc['filename']} (chunk {doc['chunk']}/{doc['total']})\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No duplicate IDs found\")\n",
    "\n",
    "# Check the original docs variable for ID patterns\n",
    "print(f\"\\nðŸ§ Checking original 'docs' variable for ID patterns:\")\n",
    "doc_ids_in_memory = [doc.get(\"id\", \"No ID\") for doc in docs]\n",
    "memory_id_counts = {}\n",
    "for doc_id in doc_ids_in_memory:\n",
    "    memory_id_counts[doc_id] = memory_id_counts.get(doc_id, 0) + 1\n",
    "\n",
    "duplicate_memory_ids = {doc_id: count for doc_id, count in memory_id_counts.items() if count > 1}\n",
    "if duplicate_memory_ids:\n",
    "    print(f\"âŒ Found duplicate IDs in memory:\")\n",
    "    for doc_id, count in duplicate_memory_ids.items():\n",
    "        print(f\"   ðŸ”„ ID '{doc_id}' appears {count} times in docs variable\")\n",
    "        \n",
    "    # Show which documents in memory have the same ID\n",
    "    for doc_id in duplicate_memory_ids:\n",
    "        matching_docs = [(i, doc) for i, doc in enumerate(docs) if doc.get(\"id\") == doc_id]\n",
    "        print(f\"\\n   ðŸ“‹ Documents in memory with ID '{doc_id}':\")\n",
    "        for i, doc in matching_docs:\n",
    "            print(f\"      {i}. {doc.get('originalFilename', 'Unknown')} (chunk {doc.get('chunkIndex', '?')}/{doc.get('totalChunks', '?')})\")\n",
    "else:\n",
    "    print(f\"âœ… No duplicate IDs in memory docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning index and re-running complete pipeline...\n",
      "ðŸ—‘ï¸ Deleting existing index...\n",
      "âœ… Index deleted successfully\n",
      "ðŸ—ï¸ Creating fresh index...\n",
      "âœ… Fresh index created successfully\n",
      "ðŸ“‚ Processing documents...\n",
      "Found 5 supported files to process\n",
      "\n",
      "Processing: merged_statistic_id558217.csv (.csv)\n",
      "Document: Statistik als Exceldatei - Split into 1 chunks\n",
      "Processing chunk 1: 2156 chars (estimated 718 tokens)\n",
      "âœ“ Successfully processed: Statistik als Exceldatei\n",
      "\n",
      "Processing: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf (.pdf)\n",
      "Document: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland - Split into 3 chunks\n",
      "Processing chunk 1: 18773 chars (estimated 6257 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1)\n",
      "Processing chunk 2: 19258 chars (estimated 6419 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 2)\n",
      "Processing chunk 3: 3244 chars (estimated 1081 tokens)\n",
      "âœ“ Successfully processed: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 3)\n",
      "\n",
      "Processing: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md (.md)\n",
      "Document: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19036 chars (estimated 6345 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 1)\n",
      "Processing chunk 2: 18603 chars (estimated 6201 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 2)\n",
      "Processing chunk 3: 16279 chars (estimated 5426 tokens)\n",
      "âœ“ Successfully processed: Goeckenjan_2019_FluchtalsSicherheitsproblem_converted (Part 3)\n",
      "\n",
      "Processing: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md (.md)\n",
      "Document: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted - Split into 3 chunks\n",
      "Processing chunk 1: 19460 chars (estimated 6486 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 1)\n",
      "Processing chunk 2: 19616 chars (estimated 6538 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 2)\n",
      "Processing chunk 3: 10732 chars (estimated 3577 tokens)\n",
      "âœ“ Successfully processed: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 3)\n",
      "\n",
      "Processing: City of Munich_2017_Flugblatt_Kriminalitaet_converted.md (.md)\n",
      "Document: City of Munich_2017_Flugblatt_Kriminalitaet_converted - Split into 1 chunks\n",
      "Processing chunk 1: 7641 chars (estimated 2547 tokens)\n",
      "âœ“ Successfully processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted\n",
      "ðŸ“Š Processed 11 document chunks\n",
      "ðŸ“¤ Uploading documents to index...\n",
      "âœ… Uploaded batch 1: 11 documents\n",
      "\n",
      "ðŸ“Š Upload completed:\n",
      "   âœ… Successful batches: 1\n",
      "\n",
      "ðŸ” Verifying index contents...\n",
      "ðŸ“Š Final index contains 0 documents from 0 files:\n",
      "\n",
      "âœ… Pipeline completed successfully!\n",
      "ðŸŽ‰ All 0 files have been indexed with 0 total chunks\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Clean index and re-run the complete pipeline\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning index and re-running complete pipeline...\")\n",
    "\n",
    "# Step 1: Delete existing index\n",
    "search_index_client = SearchIndexClient(\n",
    "    search_endpoint, DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"ðŸ—‘ï¸ Deleting existing index...\")\n",
    "    search_index_client.delete_index(index_name)\n",
    "    print(\"âœ… Index deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ Index deletion result: {e}\")\n",
    "\n",
    "# Step 2: Create fresh index\n",
    "print(\"ðŸ—ï¸ Creating fresh index...\")\n",
    "index = create_index_definition(index_name)\n",
    "search_index_client.create_or_update_index(index)\n",
    "print(\"âœ… Fresh index created successfully\")\n",
    "\n",
    "# Step 3: Process documents\n",
    "print(\"ðŸ“‚ Processing documents...\")\n",
    "docs = gen_multi_format_documents(\"./data/\")\n",
    "print(f\"ðŸ“Š Processed {len(docs)} document chunks\")\n",
    "\n",
    "# Step 4: Upload documents\n",
    "if len(docs) > 0:\n",
    "    print(\"ðŸ“¤ Uploading documents to index...\")\n",
    "    \n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    \n",
    "    # Upload in batches\n",
    "    batch_size = 50\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        try:\n",
    "            result = search_client.upload_documents(batch)\n",
    "            successful_batches += 1\n",
    "            print(f\"âœ… Uploaded batch {i//batch_size + 1}: {len(batch)} documents\")\n",
    "        except Exception as e:\n",
    "            failed_batches += 1\n",
    "            print(f\"âŒ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Upload completed:\")\n",
    "    print(f\"   âœ… Successful batches: {successful_batches}\")\n",
    "    if failed_batches > 0:\n",
    "        print(f\"   âŒ Failed batches: {failed_batches}\")\n",
    "    \n",
    "    # Step 5: Verify the result\n",
    "    print(\"\\nðŸ” Verifying index contents...\")\n",
    "    results = search_client.search(\"*\", select=[\"originalFilename\", \"documentType\"], top=50)\n",
    "    \n",
    "    files_summary = {}\n",
    "    total_docs = 0\n",
    "    \n",
    "    for result in results:\n",
    "        total_docs += 1\n",
    "        filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "        doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "        \n",
    "        if filename not in files_summary:\n",
    "            files_summary[filename] = {\"type\": doc_type, \"count\": 0}\n",
    "        files_summary[filename][\"count\"] += 1\n",
    "    \n",
    "    print(f\"ðŸ“Š Final index contains {total_docs} documents from {len(files_summary)} files:\")\n",
    "    for filename, info in sorted(files_summary.items()):\n",
    "        print(f\"   ðŸ“„ {filename}: {info['count']} chunks ({info['type']})\")\n",
    "    \n",
    "    print(f\"\\nâœ… Pipeline completed successfully!\")\n",
    "    print(f\"ðŸŽ‰ All {len(files_summary)} files have been indexed with {total_docs} total chunks\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No documents were processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Final verification of the indexed content...\n",
      "ðŸ“Š INDEX SUMMARY:\n",
      "   Total documents indexed: 11\n",
      "   Unique files: 5\n",
      "\n",
      "ðŸ“‹ DETAILED FILE BREAKDOWN:\n",
      "   âœ… Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\n",
      "      Type: Markdown Document\n",
      "      Expected chunks: 3\n",
      "      Indexed chunks: 3\n",
      "\n",
      "   âœ… City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\n",
      "      Type: Markdown Document\n",
      "      Expected chunks: 1\n",
      "      Indexed chunks: 1\n",
      "\n",
      "   âœ… Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\n",
      "      Type: Markdown Document\n",
      "      Expected chunks: 3\n",
      "      Indexed chunks: 3\n",
      "\n",
      "   âœ… Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf\n",
      "      Type: PDF Document\n",
      "      Expected chunks: 3\n",
      "      Indexed chunks: 3\n",
      "\n",
      "   âœ… merged_statistic_id558217.csv\n",
      "      Type: CSV Data Table\n",
      "      Expected chunks: 1\n",
      "      Indexed chunks: 1\n",
      "\n",
      "âŒ MISSING FILES:\n",
      "   ðŸš« Statista_2025_AuslÃ¤nderkriminalitÃ¤t in Deutschland.pdf\n",
      "\n",
      "ðŸ§ª TESTING GERMAN SEARCH:\n",
      "   ðŸ“„ Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland.pdf: Statista_2025_AuslaÌˆnderkriminalitaÌˆt in Deutschland (Part 1...\n",
      "   ðŸ“„ Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md: Baier_2015_Migration und KriminalitÃƒÂ¤t_converted (Part 1)...\n",
      "   ðŸ“„ merged_statistic_id558217.csv: Statistik als Exceldatei...\n",
      "\n",
      "============================================================\n",
      "âš ï¸ ISSUES DETECTED:\n",
      "   - 1 expected files missing\n",
      "   - Some files have incorrect chunk counts\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL VERIFICATION: Confirm all files are properly indexed\n",
    "\n",
    "print(\"ðŸ” Final verification of the indexed content...\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "# Get comprehensive stats\n",
    "results = search_client.search(\"*\", select=[\"originalFilename\", \"documentType\", \"chunkIndex\", \"totalChunks\"], top=50)\n",
    "\n",
    "file_details = {}\n",
    "total_documents = 0\n",
    "\n",
    "for result in results:\n",
    "    total_documents += 1\n",
    "    filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "    doc_type = result.get(\"documentType\", \"Unknown\")\n",
    "    chunk_idx = result.get(\"chunkIndex\", 1)\n",
    "    total_chunks = result.get(\"totalChunks\", 1)\n",
    "    \n",
    "    if filename not in file_details:\n",
    "        file_details[filename] = {\n",
    "            \"type\": doc_type,\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"found_chunks\": [],\n",
    "            \"chunks_indexed\": 0\n",
    "        }\n",
    "    \n",
    "    file_details[filename][\"found_chunks\"].append(chunk_idx)\n",
    "    file_details[filename][\"chunks_indexed\"] += 1\n",
    "\n",
    "print(f\"ðŸ“Š INDEX SUMMARY:\")\n",
    "print(f\"   Total documents indexed: {total_documents}\")\n",
    "print(f\"   Unique files: {len(file_details)}\")\n",
    "print()\n",
    "\n",
    "print(f\"ðŸ“‹ DETAILED FILE BREAKDOWN:\")\n",
    "all_files_complete = True\n",
    "\n",
    "for filename, details in sorted(file_details.items()):\n",
    "    expected = details[\"total_chunks\"]\n",
    "    found = len(details[\"found_chunks\"])\n",
    "    chunks_found = sorted(details[\"found_chunks\"])\n",
    "    \n",
    "    status = \"âœ…\" if found == expected else \"âš ï¸\"\n",
    "    if found != expected:\n",
    "        all_files_complete = False\n",
    "    \n",
    "    print(f\"   {status} {filename}\")\n",
    "    print(f\"      Type: {details['type']}\")\n",
    "    print(f\"      Expected chunks: {expected}\")\n",
    "    print(f\"      Indexed chunks: {found}\")\n",
    "    \n",
    "    if found != expected:\n",
    "        expected_chunks = list(range(1, expected + 1))\n",
    "        missing = [i for i in expected_chunks if i not in chunks_found]\n",
    "        duplicates = [i for i in chunks_found if chunks_found.count(i) > 1]\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"      âŒ Missing chunks: {missing}\")\n",
    "        if duplicates:\n",
    "            print(f\"      ðŸ”„ Duplicate chunks: {duplicates}\")\n",
    "    print()\n",
    "\n",
    "# Expected files check\n",
    "expected_files = [\n",
    "    \"merged_statistic_id558217.csv\",\n",
    "    \"Baier_2015_Migration und KriminalitÃƒÂ¤t_converted.md\", \n",
    "    \"City of Munich_2017_Flugblatt_Kriminalitaet_converted.md\",\n",
    "    \"Goeckenjan_2019_FluchtalsSicherheitsproblem_converted.md\",\n",
    "    \"Statista_2025_AuslÃ¤nderkriminalitÃ¤t in Deutschland.pdf\"\n",
    "]\n",
    "\n",
    "indexed_files = set(file_details.keys())\n",
    "missing_files = [f for f in expected_files if f not in indexed_files]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"âŒ MISSING FILES:\")\n",
    "    for missing in missing_files:\n",
    "        print(f\"   ðŸš« {missing}\")\n",
    "    all_files_complete = False\n",
    "else:\n",
    "    print(f\"âœ… ALL EXPECTED FILES ARE INDEXED\")\n",
    "\n",
    "# Test German search\n",
    "print(f\"\\nðŸ§ª TESTING GERMAN SEARCH:\")\n",
    "german_results = search_client.search(\"auslÃ¤ndische StraftatverdÃ¤chtige\", top=3)\n",
    "german_found = False\n",
    "for result in german_results:\n",
    "    german_found = True\n",
    "    filename = result.get(\"originalFilename\", \"Unknown\")\n",
    "    title = result.get(\"title\", \"No title\")\n",
    "    print(f\"   ðŸ“„ {filename}: {title[:60]}...\")\n",
    "\n",
    "if not german_found:\n",
    "    print(f\"   âš ï¸ No results found for German search term\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\n{'='*60}\")\n",
    "if all_files_complete and not missing_files and german_found:\n",
    "    print(f\"ðŸŽ‰ SUCCESS: All files properly indexed and searchable!\")\n",
    "    print(f\"âœ… {len(file_details)} files indexed with {total_documents} total chunks\")\n",
    "    print(f\"âœ… German search functionality working\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ISSUES DETECTED:\")\n",
    "    if missing_files:\n",
    "        print(f\"   - {len(missing_files)} expected files missing\")\n",
    "    if not all_files_complete:\n",
    "        print(f\"   - Some files have incorrect chunk counts\")\n",
    "    if not german_found:\n",
    "        print(f\"   - German search not working\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
