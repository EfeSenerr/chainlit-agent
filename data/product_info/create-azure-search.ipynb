{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating your document search index\n",
    "This notebook is designed to automatically create a document search index for you. It uses markdown files from a local folder to create the index. In order to do so it needs names and keys for the following services:\n",
    "\n",
    "- Azure Search Service\n",
    "- Azure OpenAI Service\n",
    "\n",
    "You can find the names and keys in the Azure Portal. These need to be entered in a `.env` file in the root of this repository. The `.env` file is not checked in to source control. You can use the [`.env.sample`](../../.env.sample) file as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SemanticSearch,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters\n",
    ")\n",
    "from typing import List, Dict\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index(search_index_client: SearchIndexClient, search_index: str):\n",
    "    print(f\"deleting index {search_index}\")\n",
    "    search_index_client.delete_index(search_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_definition(name: str) -> SearchIndex:\n",
    "    \"\"\"\n",
    "    Returns an Azure Cognitive Search index with the given name.\n",
    "    \"\"\"\n",
    "    # The fields we want to index. The \"embedding\" field is a vector field that will\n",
    "    # be used for vector search.\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"filepath\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
    "        # Additional metadata fields for better referencing\n",
    "        SimpleField(name=\"originalFilename\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"originalTitle\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"documentStem\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"chunkIndex\", type=SearchFieldDataType.Int32),\n",
    "        SimpleField(name=\"totalChunks\", type=SearchFieldDataType.Int32),\n",
    "        SimpleField(name=\"isChunked\", type=SearchFieldDataType.Boolean),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # Size of the vector created by the text-embedding-3-large model.\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"titleVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # Size of the vector created by the text-embedding-3-large model.\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "        ),    \n",
    "    ]\n",
    "\n",
    "    # The \"content\" field should be prioritized for semantic ranking.\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"default\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            keywords_fields=[],\n",
    "            content_fields=[SemanticField(field_name=\"content\")],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # For vector search, we want to use the HNSW (Hierarchical Navigable Small World)\n",
    "    # algorithm (a type of approximate nearest neighbor search algorithm) with cosine\n",
    "    # distance.\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            ),\n",
    "            ExhaustiveKnnAlgorithmConfiguration(\n",
    "                name=\"myExhaustiveKnn\",\n",
    "                kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "                parameters=ExhaustiveKnnParameters(\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "                vectorizer_name=\"myvectorizer\"\n",
    "            ),\n",
    "            VectorSearchProfile(\n",
    "                name=\"myExhaustiveKnnProfile\",\n",
    "                algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "            ),\n",
    "        ],\n",
    "        vectorizers=[  \n",
    "            AzureOpenAIVectorizer(  \n",
    "                vectorizer_name=\"myvectorizer\",  \n",
    "                kind=\"azureOpenAI\",  \n",
    "                parameters=AzureOpenAIVectorizerParameters(  \n",
    "                    resource_url=os.environ[\"AZURE_OPENAI_ENDPOINT\"],  \n",
    "                    deployment_name=os.environ[\"AZURE_EMBEDDING_NAME\"],\n",
    "                    model_name=os.environ[\"AZURE_EMBEDDING_NAME\"],\n",
    "                    # Todo: there is some issue with managed identity for AI search\n",
    "                    # authIdentity=\"/subscriptions/db4948d5-b90c-47d4-91b5-d8c4c43493d2/resourcegroups/rg-chainlit-agent/providers/Microsoft.ManagedIdentity/userAssignedIdentities/id-7r5g6is3dx73u\"\n",
    "                ),\n",
    "            ),  \n",
    "        ],  \n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index.\n",
    "    index = SearchIndex(\n",
    "        name=name,\n",
    "        fields=fields,\n",
    "        semantic_search=semantic_search,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_tokens: int = 6500) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks that fit within the token limit.\n",
    "    Using a more conservative estimate of ~3 characters per token.\n",
    "    \"\"\"\n",
    "    # More conservative estimate: 1 token ≈ 3 characters (down from 4)\n",
    "    # And using 6000 tokens max instead of 7000 for extra safety\n",
    "    max_chars = max_tokens * 3\n",
    "    \n",
    "    # Split by paragraphs first (double newlines)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If adding this paragraph would exceed the limit, start a new chunk\n",
    "        if len(current_chunk) + len(paragraph) + 2 > max_chars and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + paragraph\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    # Handle edge case where a single paragraph is too long\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) <= max_chars:\n",
    "            final_chunks.append(chunk)\n",
    "        else:\n",
    "            # Split by sentences if paragraph is too long\n",
    "            sentences = chunk.split('. ')\n",
    "            temp_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                if len(temp_chunk) + len(sentence) + 2 > max_chars and temp_chunk:\n",
    "                    final_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = sentence\n",
    "                else:\n",
    "                    if temp_chunk:\n",
    "                        temp_chunk += \". \" + sentence\n",
    "                    else:\n",
    "                        temp_chunk = sentence\n",
    "            if temp_chunk:\n",
    "                final_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    # Final safety check - if any chunk is still too large, split it by words\n",
    "    safe_chunks = []\n",
    "    for chunk in final_chunks:\n",
    "        if len(chunk) <= max_chars:\n",
    "            safe_chunks.append(chunk)\n",
    "        else:\n",
    "            # Emergency word-level splitting\n",
    "            words = chunk.split()\n",
    "            temp_chunk = \"\"\n",
    "            for word in words:\n",
    "                if len(temp_chunk) + len(word) + 1 > max_chars and temp_chunk:\n",
    "                    safe_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = word\n",
    "                else:\n",
    "                    if temp_chunk:\n",
    "                        temp_chunk += \" \" + word\n",
    "                    else:\n",
    "                        temp_chunk = word\n",
    "            if temp_chunk:\n",
    "                safe_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    return safe_chunks\n",
    "\n",
    "def gen_markdown_documents(\n",
    "    folder_path: str,\n",
    ") -> List[Dict[str, any]]:\n",
    "    openai_service_endoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    openai_deployment = os.environ[\"AZURE_EMBEDDING_NAME\"]\n",
    "\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")    \n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2023-07-01-preview\",\n",
    "        azure_endpoint=openai_service_endoint,\n",
    "        azure_deployment=openai_deployment,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "    items = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Find all markdown files in the folder and subfolders\n",
    "    md_files = list(folder.glob(\"**/*.md\"))\n",
    "    \n",
    "    doc_id = 1\n",
    "    for md_file in md_files:\n",
    "        # Read the markdown file content\n",
    "        with open(md_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract title from the first heading or use filename\n",
    "        title = md_file.stem  # Default to filename without extension\n",
    "        lines = content.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('# '):\n",
    "                title = line[2:].strip()\n",
    "                break\n",
    "        \n",
    "        # Preserve document metadata for referencing\n",
    "        original_filename = md_file.name\n",
    "        document_path = str(md_file.relative_to(folder))\n",
    "        document_stem = md_file.stem\n",
    "        \n",
    "        # Split content into chunks\n",
    "        content_chunks = chunk_text(content)\n",
    "        \n",
    "        print(f\"Document: {title} - Split into {len(content_chunks)} chunks\")\n",
    "        \n",
    "        # Process each chunk as a separate document\n",
    "        for chunk_idx, chunk in enumerate(content_chunks):\n",
    "            # Create unique ID for each chunk\n",
    "            chunk_id = f\"{doc_id}_{chunk_idx + 1}\" if len(content_chunks) > 1 else str(doc_id)\n",
    "            \n",
    "            # Preserve original title for reference, add chunk info only for display\n",
    "            chunk_title = title\n",
    "            display_title = title\n",
    "            if len(content_chunks) > 1:\n",
    "                display_title = f\"{title} (Part {chunk_idx + 1})\"\n",
    "            \n",
    "            # Enhanced content with document reference information\n",
    "            enhanced_content = f\"Source Document: {original_filename}\\n\"\n",
    "            if len(content_chunks) > 1:\n",
    "                enhanced_content += f\"Document Section: Part {chunk_idx + 1} of {len(content_chunks)}\\n\"\n",
    "            enhanced_content += f\"Original Title: {title}\\n\\n{chunk}\"\n",
    "            \n",
    "            url = f\"/docs/{document_path.replace('.md', '').replace('/', '-').lower()}\"\n",
    "            if len(content_chunks) > 1:\n",
    "                url += f\"-part-{chunk_idx + 1}\"\n",
    "            \n",
    "            # Generate embeddings for enhanced content and title\n",
    "            try:\n",
    "                print(f\"Processing chunk {chunk_idx + 1}: {len(enhanced_content)} chars (estimated {len(enhanced_content)//3} tokens)\")\n",
    "                \n",
    "                content_emb = client.embeddings.create(input=enhanced_content, model=openai_deployment)\n",
    "                title_emb = client.embeddings.create(input=display_title, model=openai_deployment)\n",
    "                \n",
    "                rec = {\n",
    "                    \"id\": chunk_id,\n",
    "                    \"content\": enhanced_content,  # Contains source reference\n",
    "                    \"filepath\": document_path,     # Original relative path\n",
    "                    \"title\": display_title,       # Display title with chunk info\n",
    "                    \"url\": url,\n",
    "                    \"contentVector\": content_emb.data[0].embedding,\n",
    "                    \"titleVector\": title_emb.data[0].embedding,\n",
    "                    # Additional metadata for better referencing\n",
    "                    \"originalFilename\": original_filename,\n",
    "                    \"originalTitle\": title,\n",
    "                    \"documentStem\": document_stem,\n",
    "                    \"chunkIndex\": chunk_idx + 1,\n",
    "                    \"totalChunks\": len(content_chunks),\n",
    "                    \"isChunked\": len(content_chunks) > 1\n",
    "                }\n",
    "                items.append(rec)\n",
    "                print(f\"✓ Successfully processed: {display_title}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing chunk {chunk_idx + 1} of {title}: {e}\")\n",
    "                print(f\"  Enhanced content length: {len(enhanced_content)} characters (estimated {len(enhanced_content)//3} tokens)\")\n",
    "                print(f\"  Original chunk length: {len(chunk)} characters\")\n",
    "                continue\n",
    "        \n",
    "        doc_id += 1\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting index parssedindexer1\n",
      "creating index parssedindexer1\n",
      "creating index parssedindexer1\n",
      "index parssedindexer1 created\n",
      "index parssedindexer1 created\n"
     ]
    }
   ],
   "source": [
    "search_endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "\n",
    "search_index_client = SearchIndexClient(\n",
    "    search_endpoint, DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "delete_index(search_index_client, index_name)\n",
    "index = create_index_definition(index_name)\n",
    "print(f\"creating index {index_name}\")\n",
    "search_index_client.create_or_update_index(index)\n",
    "print(f\"index {index_name} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing documents\n",
      "Error processing chunk 1 of Goeckenjan_2019_FluchtalsSicherheitsproblem_converted: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8725 tokens (8725 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Chunk length: 27513 characters\n",
      "Error processing chunk 2 of Goeckenjan_2019_FluchtalsSicherheitsproblem_converted: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8408 tokens (8408 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Chunk length: 25783 characters\n",
      "Error processing chunk 1 of Goeckenjan_2019_FluchtalsSicherheitsproblem_converted: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8725 tokens (8725 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Chunk length: 27513 characters\n",
      "Error processing chunk 2 of Goeckenjan_2019_FluchtalsSicherheitsproblem_converted: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8408 tokens (8408 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Chunk length: 25783 characters\n",
      "Processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1) (27118 characters)\n",
      "Processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 1) (27118 characters)\n",
      "Processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 2) (22098 characters)\n",
      "Processed: Baier_2015_Migration und KriminalitÃ¤t_converted (Part 2) (22098 characters)\n",
      "Processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted (7463 characters)\n",
      "uploading 3 documents to index parssedindexer1\n",
      "Processed: City of Munich_2017_Flugblatt_Kriminalitaet_converted (7463 characters)\n",
      "uploading 3 documents to index parssedindexer1\n"
     ]
    }
   ],
   "source": [
    "print(f\"indexing documents\")\n",
    "# Change this path to point to your folder containing markdown files\n",
    "markdown_folder = \"./data/\"  # Update this path as needed\n",
    "docs = gen_markdown_documents(markdown_folder)\n",
    "# Upload our data to the index.\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "print(f\"uploading {len(docs)} documents to index {index_name}\")\n",
    "ds = search_client.upload_documents(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
